{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n1. I used two transformer layer in the encoder instead of four.\n2. I used four attention heads instead of two.\n3. I used new tokens for SOS, EOS, and padding (very minor since Rohith used rare tokens for these purposes, but still- more 'correct').\n2. I fixed a bug (probably?) in the decoder's dropout layers, which did not have the training flag, resulting in dropout during inference. This change gave a nice bump in the score.\n3. I made the passing of the training flag explicit. I know it can be implicit since it is a kwarg, but explicit passing makes the whole thing more straightforward and maybe fix another one or two training-flag-related bugs along the way.\n4. I changed the positional encoding in the decoder from tf.keras.layers.Embedding to proper positional embeddings (i.e., the usual sines and cosines usually used for this purpose). This had a significant impact.\n5. I added positional embedding to the encoder. This, too, had a significant impact.\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.metrics import Accuracy\nfrom skimage.transform import resize\nfrom sklearn.model_selection import train_test_split\nimport json\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport Levenshtein as lev\nimport os\nimport gc","metadata":{"execution":{"iopub.status.busy":"2023-07-10T01:44:52.637541Z","iopub.execute_input":"2023-07-10T01:44:52.637894Z","iopub.status.idle":"2023-07-10T01:44:52.644518Z","shell.execute_reply.started":"2023-07-10T01:44:52.637865Z","shell.execute_reply":"2023-07-10T01:44:52.643596Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"inpdir = \"/kaggle/input/asl-fingerspelling\"\ndf = pd.read_csv(f'{inpdir}/train.csv')\ndf[\"phrase_bytes\"] = df[\"phrase\"].map(lambda x: x.encode(\"utf-8\"))\ndisplay(df.head())","metadata":{}},{"cell_type":"markdown","source":"train_landmarks = pd.read_parquet('/kaggle/input/asl-fingerspelling/train_landmarks/1019715464.parquet')\nkeys = train_landmarks.keys()[1:]\ntrain_landmarks.head()","metadata":{}},{"cell_type":"markdown","source":"# TFRecord","metadata":{}},{"cell_type":"markdown","source":"LPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\nPOSE = LPOSE + RPOSE\n\nRHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\nLHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\nPOSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n\nX = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\nY = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\nZ = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n\nSEL_COLS = X + Y + Z\nFRAME_LEN = 128\n\nX_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\nY_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\nZ_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n\nRHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\nLHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\nRPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\nLPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n\nprint('SEL_COLS size:' + str(len(SEL_COLS)))","metadata":{}},{"cell_type":"markdown","source":"def load_relevant_data_subset(pq_path):\n    return pd.read_parquet(pq_path, columns=SEL_COLS)\n\ncounter = 0\nfor file_id in tqdm(df.file_id.unique()):\n    \n    print(counter)\n    counter+=1\n    \n    pqfile = f\"{inpdir}/train_landmarks/{file_id}.parquet\"\n    if not os.path.isdir(\"tfds\"): os.mkdir(\"tfds\")\n    tffile = f\"tfds/{file_id}.tfrecord\"\n    seq_refs = df.loc[df.file_id == file_id]\n    seqs = load_relevant_data_subset(pqfile)\n    seqs_numpy = seqs.to_numpy()\n    with tf.io.TFRecordWriter(tffile) as file_writer:\n        for seq_id, phrase in zip(seq_refs.sequence_id, seq_refs.phrase_bytes):\n            frames = seqs_numpy[seqs.index == seq_id]\n            \n            r_nonan = np.sum(np.sum(np.isnan(frames[:, RHAND_IDX]), axis = 1) == 0)\n            l_nonan = np.sum(np.sum(np.isnan(frames[:, LHAND_IDX]), axis = 1) == 0)\n            no_nan = max(r_nonan, l_nonan)\n            \n            if 2*len(phrase)<no_nan:\n                features = {SEL_COLS[i]: tf.train.Feature(\n                    float_list=tf.train.FloatList(value=frames[:, i])) for i in range(len(SEL_COLS))}\n                features[\"phrase\"] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[phrase]))\n                record_bytes = tf.train.Example(features=tf.train.Features(feature=features)).SerializeToString()\n                file_writer.write(record_bytes)","metadata":{}},{"cell_type":"markdown","source":"# Data loading","metadata":{}},{"cell_type":"markdown","source":"#### Here I use new tokens for padding, start and end of sentences. (Capitals are good since the original phrases have only lower case letters, besides numbers and various signs).","metadata":{}},{"cell_type":"code","source":"pad_token = 'P'\nstart_token = 'S'\nend_token = 'E'\npad_token_idx = 59\nstart_token_idx = 60\nend_token_idx = 61","metadata":{"execution":{"iopub.status.busy":"2023-07-10T01:44:52.649173Z","iopub.execute_input":"2023-07-10T01:44:52.650055Z","iopub.status.idle":"2023-07-10T01:44:52.656600Z","shell.execute_reply.started":"2023-07-10T01:44:52.650023Z","shell.execute_reply":"2023-07-10T01:44:52.655597Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n    char_to_num = json.load(f)\n\n\nchar_to_num[pad_token] = pad_token_idx\nchar_to_num[start_token] = start_token_idx\nchar_to_num[end_token] = end_token_idx\n\nnum_to_char = {j:i for i,j in char_to_num.items()}\n\n\ninpdir = \"/kaggle/input/asl-fingerspelling\"\ndf = pd.read_csv(f'{inpdir}/train.csv')\n\nLPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\nPOSE = LPOSE + RPOSE\n\nRHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\nLHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\nPOSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n\nX = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\nY = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\nZ = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n\nSEL_COLS = X + Y + Z\nFRAME_LEN = 128\n\nX_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\nY_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\nZ_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n\nRHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\nLHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\nRPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\nLPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n\nprint(RPOSE_IDX)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T01:44:52.660082Z","iopub.execute_input":"2023-07-10T01:44:52.660492Z","iopub.status.idle":"2023-07-10T01:44:52.811129Z","shell.execute_reply.started":"2023-07-10T01:44:52.660463Z","shell.execute_reply":"2023-07-10T01:44:52.810055Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"[47, 48, 49, 50, 51, 99, 100, 101, 102, 103, 151, 152, 153, 154, 155]\n","output_type":"stream"}]},{"cell_type":"code","source":"def resize_pad(x):\n    if tf.shape(x)[0] < FRAME_LEN:\n        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0], [0, 0]]))\n        print(x)\n    else:\n        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n    return x\n\ndef translate_landmarks(landmarks, max_translation):\n    translation = tf.random.uniform(shape=tf.shape(landmarks), minval=-max_translation, maxval=max_translation)\n    translated_landmarks = landmarks + translation\n    return translated_landmarks\n\n# def scale_landmarks(landmarks, min_scale, max_scale):\n#     scale_factor = tf.random.uniform(shape=tf.shape(landmarks), minval=min_scale, maxval=max_scale)\n#     scaled_landmarks = landmarks * scale_factor\n#     return scaled_landmarks\n\ndef pre_process(x):\n\n    rhand = tf.gather(x, RHAND_IDX, axis=1)\n    lhand = tf.gather(x, LHAND_IDX, axis=1)\n    rpose = tf.gather(x, RPOSE_IDX, axis=1)\n    lpose = tf.gather(x, LPOSE_IDX, axis=1)\n\n    rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)\n    lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)\n\n    rnans = tf.math.count_nonzero(rnan_idx)\n    lnans = tf.math.count_nonzero(lnan_idx)\n\n    # For dominant hand\n    if rnans > lnans:\n        hand = lhand\n        pose = lpose\n\n        hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n        hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n        hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n        hand = tf.concat([1-hand_x, hand_y, hand_z], axis=1)\n\n        pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n        pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n        pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n        pose = tf.concat([1-pose_x, pose_y, pose_z], axis=1)\n    else:\n        hand = rhand\n        pose = rpose\n\n    hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n    hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n    hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n    hand = tf.concat([hand_x[..., tf.newaxis], hand_y[..., tf.newaxis], hand_z[..., tf.newaxis]], axis=-1)\n\n    mean = tf.math.reduce_mean(hand, axis=1)[:, tf.newaxis, :]\n    std = tf.math.reduce_std(hand, axis=1)[:, tf.newaxis, :]\n    hand = (hand - mean) / std\n\n    pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n    pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n    pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n    pose = tf.concat([pose_x[..., tf.newaxis], pose_y[..., tf.newaxis], pose_z[..., tf.newaxis]], axis=-1)\n\n    x = tf.concat([hand, pose], axis=1)\n    x = resize_pad(x)\n\n    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n    x = tf.reshape(x, (FRAME_LEN, len(LHAND_IDX) + len(LPOSE_IDX)))\n    return x","metadata":{"execution":{"iopub.status.busy":"2023-07-10T01:44:52.812808Z","iopub.execute_input":"2023-07-10T01:44:52.813170Z","iopub.status.idle":"2023-07-10T01:44:52.834728Z","shell.execute_reply.started":"2023-07-10T01:44:52.813138Z","shell.execute_reply":"2023-07-10T01:44:52.833492Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"table = tf.lookup.StaticHashTable(\n    initializer=tf.lookup.KeyValueTensorInitializer(\n        keys=list(char_to_num.keys()),\n        values=list(char_to_num.values()),\n    ),\n    default_value=tf.constant(-1),\n    name=\"class_weight\"\n)\n\ndef preprocess_fn(landmarks, phrase):\n    phrase = start_token + phrase + end_token\n    phrase = tf.strings.bytes_split(phrase)\n    phrase = table.lookup(phrase)\n    phrase = tf.pad(phrase, paddings=[[0, 64 - tf.shape(phrase)[0]]], mode = 'CONSTANT',\n                    constant_values = pad_token_idx)\n\n    # landmarksを前処理する\n    translated_landmarks = translate_landmarks(landmarks, max_translation=10)\n    #scaled_landmarks = scale_landmarks(landmarks, min_scale=0.8, max_scale=1.2)\n\n    # 前処理済みのlandmarksを結合する\n    #combined_landmarks = tf.concat([landmarks, translated_landmarks, scaled_landmarks], axis=1)\n    combined_landmarks = tf.concat([landmarks, translated_landmarks], axis=1)\n    return pre_process(combined_landmarks), phrase\n\ndef decode_fn(record_bytes):\n    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in SEL_COLS}\n    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n    features = tf.io.parse_single_example(record_bytes, schema)\n    phrase = features[\"phrase\"]\n    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in SEL_COLS])\n    landmarks = tf.transpose(landmarks)\n\n    return landmarks, phrase\n\ninpdir = \"/kaggle/input/aslfr-parquets-to-tfrecords-cleaned\"\ntffiles = df.file_id.map(lambda x: f'{inpdir}/tfds/{x}.tfrecord').unique()\n\nbatch_size = 32\nval_len = int(0.05 * len(tffiles))\n\ntrain_dataset = tf.data.TFRecordDataset(tffiles[val_len:]).map(decode_fn).map(preprocess_fn).shuffle(30000, reshuffle_each_iteration=True).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\nval_dataset = tf.data.TFRecordDataset(tffiles[:val_len]).map(decode_fn).map(preprocess_fn).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T01:44:52.836461Z","iopub.execute_input":"2023-07-10T01:44:52.837068Z","iopub.status.idle":"2023-07-10T01:44:54.406740Z","shell.execute_reply.started":"2023-07-10T01:44:52.837035Z","shell.execute_reply":"2023-07-10T01:44:54.405448Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Tensor(\"cond_1/Pad:0\", shape=(None, 26, 3), dtype=float32)\nTensor(\"cond_1/Pad:0\", shape=(None, 26, 3), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# The model","metadata":{}},{"cell_type":"code","source":"LPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\n\nPOSE = LPOSE + RPOSE\n\nRHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\nLHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\nPOSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n\nX = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\nY = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\nZ = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n\nSEL_COLS = X + Y + Z\nFRAME_LEN = 128\n\nX_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\nY_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\nZ_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n\nRHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\nLHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\nRPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\nLPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n\nprint('SEL_COLS size:' + str(len(SEL_COLS)))","metadata":{"execution":{"iopub.status.busy":"2023-07-10T01:44:54.410101Z","iopub.execute_input":"2023-07-10T01:44:54.410493Z","iopub.status.idle":"2023-07-10T01:44:54.424556Z","shell.execute_reply.started":"2023-07-10T01:44:54.410461Z","shell.execute_reply":"2023-07-10T01:44:54.423514Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"SEL_COLS size:156\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Here I implemented proper positional embeddings for both the encoder and the decoder.","metadata":{}},{"cell_type":"code","source":"class MLPBlock(keras.layers.Layer):\n    def __init__(self, num_hid=256, num_layers=5):\n        super().__init__()\n        self.mlp = tf.keras.Sequential()\n        for _ in range(num_layers):\n            self.mlp.add(tf.keras.layers.Dense(num_hid, activation=tf.nn.gelu))\n        self.mlp.add(tf.keras.layers.Dense(num_hid))\n\n    def call(self, inputs):\n        return self.mlp(inputs)\n\n\nclass TokenEmbedding(keras.layers.Layer):\n    def __init__(self, num_vocab=61, maxlen=50, num_hid=256, mlp_num_layers=5):\n        super().__init__()\n        self.num_hid = num_hid\n        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n        self.pos_emb = self.positional_encoding(maxlen - 1, num_hid)\n        self.mlp_block = MLPBlock(num_hid, num_layers=mlp_num_layers)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        x = self.emb(x) * tf.math.sqrt(tf.cast(self.num_hid, tf.float32))\n        x = x + self.pos_emb[:maxlen, :]\n        x = self.mlp_block(x)\n        return x\n\n    def positional_encoding(self, maxlen, num_hid):\n        positions = tf.range(maxlen, dtype=tf.float32)[..., tf.newaxis]\n        depth = num_hid // 2\n        angles = positions / tf.pow(10000, tf.range(0, depth, 1, dtype=tf.float32) / num_hid)  # depthのインクリメントを修正\n        pos_encoding = tf.concat([tf.sin(angles), tf.cos(angles)], axis=-1)\n        return pos_encoding\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T01:44:54.426076Z","iopub.execute_input":"2023-07-10T01:44:54.426769Z","iopub.status.idle":"2023-07-10T01:44:54.440724Z","shell.execute_reply.started":"2023-07-10T01:44:54.426736Z","shell.execute_reply":"2023-07-10T01:44:54.439849Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"class TokenEmbedding(layers.Layer):\n    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n        super().__init__()\n        self.num_hid = num_hid\n        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n        #self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n        '''\n        self.pos_emb = tf.math.divide(\n            self.positional_encoding(maxlen-1, num_hid),\n            tf.math.sqrt(tf.cast(num_hid, tf.float32)))\n        '''\n        self.pos_emb = self.positional_encoding(maxlen-1, num_hid)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        x = self.emb(x)\n        x = tf.math.multiply(x, tf.math.sqrt(tf.cast(self.num_hid, tf.float32)))\n        '''\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        return x + positions\n        '''\n        return x + self.pos_emb[:maxlen, :]\n    \n    def positional_encoding(self, maxlen, num_hid):\n        depth = num_hid/2\n        positions = tf.range(maxlen, dtype = tf.float32)[..., tf.newaxis]\n        depths = tf.range(depth, dtype = tf.float32)[np.newaxis, :]/depth\n        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n        angle_rads = tf.linalg.matmul(positions, angle_rates)\n        pos_encoding = tf.concat(\n          [tf.math.sin(angle_rads), tf.math.cos(angle_rads)],\n          axis=-1)\n        return pos_encoding\n\n\n","metadata":{}},{"cell_type":"code","source":"class LandmarkEmbedding(tf.keras.Model):\n    def __init__(self, num_hid=256, maxlen=100):\n        super(LandmarkEmbedding, self).__init__()\n        self.conv1 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn1 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu1 = tf.keras.layers.LeakyReLU()\n\n        self.conv2 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn2 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu2 = tf.keras.layers.LeakyReLU()\n        self.dropout2 = tf.keras.layers.Dropout(0.2)\n\n        self.conv3 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn3 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu3 = tf.keras.layers.LeakyReLU()\n        self.dropout3 = tf.keras.layers.Dropout(0.2)\n\n        self.conv4 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn4 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu4 = tf.keras.layers.LeakyReLU()\n        self.dropout4 = tf.keras.layers.Dropout(0.2)\n\n        self.sigmoid = tf.keras.layers.Activation('sigmoid')\n        self.pos_emb = self.positional_encoding(maxlen, num_hid)\n        self.maxlen = maxlen\n        self.num_hid = num_hid\n\n    def call(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.leaky_relu1(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.leaky_relu2(x)\n        x = self.dropout2(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.leaky_relu3(x)\n        x = self.dropout3(x)\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.leaky_relu4(x)\n        x = self.dropout4(x)\n        x = tf.math.multiply(x, tf.math.sqrt(tf.cast(self.num_hid, tf.float32)))\n        x = x + self.pos_emb\n\n        return self.sigmoid(x)\n\n    def positional_encoding(self, maxlen, num_hid):\n        depth = num_hid/2\n        positions = tf.range(maxlen, dtype=tf.float32)[..., tf.newaxis]\n        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth\n        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n        angle_rads = tf.linalg.matmul(positions, angle_rates)\n        pos_encoding = tf.concat([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], axis=-1)\n        return pos_encoding\n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T01:44:54.441996Z","iopub.execute_input":"2023-07-10T01:44:54.442482Z","iopub.status.idle":"2023-07-10T01:44:54.462169Z","shell.execute_reply.started":"2023-07-10T01:44:54.442446Z","shell.execute_reply":"2023-07-10T01:44:54.461234Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"class TransformerEncoder(keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T01:44:54.463626Z","iopub.execute_input":"2023-07-10T01:44:54.464058Z","iopub.status.idle":"2023-07-10T01:44:54.476878Z","shell.execute_reply.started":"2023-07-10T01:44:54.464028Z","shell.execute_reply":"2023-07-10T01:44:54.475872Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"#### Here I added the training flag to the TransformerDecoder's Dropout layers.","metadata":{}},{"cell_type":"code","source":"class TransformerDecoder(keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n        super().__init__()\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n        self.self_att = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.self_dropout = layers.Dropout(0.5)\n        self.enc_dropout = layers.Dropout(0.1)\n        self.ffn_dropout = layers.Dropout(0.1)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n\n    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n        \"\"\"Masks the upper half of the dot product matrix in self attention.\n\n        This prevents flow of information from future tokens to current token.\n        1's in the lower triangle, counting from the lower right corner.\n        \"\"\"\n        i = tf.range(n_dest)[:, None]\n        j = tf.range(n_src)\n        m = i >= j - n_src + n_dest\n        mask = tf.cast(m, dtype)\n        mask = tf.reshape(mask, [1, n_dest, n_src])\n        mult = tf.concat(\n            [batch_size[..., tf.newaxis], tf.constant([1, 1], dtype=tf.int32)], 0\n        )\n        return tf.tile(mask, mult)\n\n    def call(self, enc_out, target, training):\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n        target_att = self.self_att(target, target, attention_mask=causal_mask)\n        target_norm = self.layernorm1(target + self.self_dropout(target_att, training = training))\n        enc_out = self.enc_att(target_norm, enc_out)\n        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out, training = training) + target_norm)\n        ffn_out = self.ffn(enc_out_norm)\n        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out, training = training))\n        return ffn_out_norm","metadata":{"execution":{"iopub.status.busy":"2023-07-10T01:44:54.478217Z","iopub.execute_input":"2023-07-10T01:44:54.478753Z","iopub.status.idle":"2023-07-10T01:44:54.493186Z","shell.execute_reply.started":"2023-07-10T01:44:54.478721Z","shell.execute_reply":"2023-07-10T01:44:54.492487Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"#### Here I made the passing of the training flag explicit.","metadata":{}},{"cell_type":"code","source":"class Transformer(tf.keras.Model):\n    def __init__(\n        self,\n        num_hid=64,\n        num_head=2,\n        num_feed_forward=128,\n        source_maxlen=100,\n        target_maxlen=100,\n        num_layers_enc=4,\n        num_layers_dec=1,\n        num_classes=60,\n    ):\n        super().__init__()\n        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n        self.num_layers_enc = num_layers_enc\n        self.num_layers_dec = num_layers_dec\n        self.target_maxlen = target_maxlen\n        self.num_classes = num_classes\n\n        self.enc_input = LandmarkEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n        self.dec_input = TokenEmbedding(\n            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n        )\n\n        self.encoder = keras.Sequential(\n            [self.enc_input]\n            + [\n                TransformerEncoder(num_hid, num_head, num_feed_forward)\n                for _ in range(num_layers_enc)\n            ]\n        )\n\n        for i in range(num_layers_dec):\n            setattr(\n                self,\n                f\"dec_layer_{i}\",\n                TransformerDecoder(num_hid, num_head, num_feed_forward),\n            )\n\n        self.classifier = layers.Dense(num_classes)\n\n    def decode(self, enc_out, target, training):\n        y = self.dec_input(target)\n        for i in range(self.num_layers_dec):\n            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y, training)\n        return y\n\n    def call(self, inputs, training):\n        source = inputs[0]\n        target = inputs[1]\n        x = self.encoder(source, training)\n        y = self.decode(x, target, training)\n        return self.classifier(y)\n\n    @property\n    def metrics(self):\n        return [self.loss_metric]\n\n    def train_step(self, batch):\n        \"\"\"Processes one batch inside model.fit().\"\"\"\n        source = batch[0]\n        target = batch[1]\n\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        \n        dec_input = target[:, :-1]\n        dec_target = target[:, 1:]\n        with tf.GradientTape() as tape:\n            preds = self([source, dec_input])\n            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n            mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        self.loss_metric.update_state(loss)\n        return {\"loss\": self.loss_metric.result()}\n\n    def test_step(self, batch):        \n        source = batch[0]\n        target = batch[1]\n\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        \n        dec_input = target[:, :-1]\n        dec_target = target[:, 1:]\n        preds = self([source, dec_input])\n        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n        mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n        self.loss_metric.update_state(loss)\n        return {\"loss\": self.loss_metric.result()}\n\n    def generate(self, source, target_start_token_idx):\n        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n        bs = tf.shape(source)[0]\n        enc = self.encoder(source, training = False)\n        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n        dec_logits = []\n        for i in range(self.target_maxlen - 1):\n            dec_out = self.decode(enc, dec_input, training = False)\n            logits = self.classifier(dec_out)\n            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n            last_logit = logits[:, -1][..., tf.newaxis]\n            dec_logits.append(last_logit)\n            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n        return dec_input","metadata":{"execution":{"iopub.status.busy":"2023-07-10T01:44:54.494555Z","iopub.execute_input":"2023-07-10T01:44:54.495155Z","iopub.status.idle":"2023-07-10T01:44:54.517322Z","shell.execute_reply.started":"2023-07-10T01:44:54.495124Z","shell.execute_reply":"2023-07-10T01:44:54.516360Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# 正解率を計算するためのメトリクスを作成\ntrain_accuracy = tf.keras.metrics.CategoricalAccuracy()\nval_accuracy = tf.keras.metrics.CategoricalAccuracy()\n\n# 学習ループ内で正解率を更新するコールバックを定義\nclass AccuracyCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        train_acc = train_accuracy.result()\n        val_acc = val_accuracy.result()\n        print(f\"Epoch {epoch+1}: Train Accuracy = {train_acc}, Validation Accuracy = {val_acc}\")\n        # 正解率をリセット\n        train_accuracy.reset_states()\n        val_accuracy.reset_states()\n# val_lossが3回マイナスになった場合に学習を停止するコールバック\nclass EarlyStoppingCallback(tf.keras.callbacks.Callback):\n    def __init__(self, patience=7):\n        super(EarlyStoppingCallback, self).__init__()\n        self.patience = patience\n        self.min_val_loss = float('inf')\n        self.wait = 0\n\n    def on_epoch_end(self, epoch, logs=None):\n        val_loss = logs.get('val_loss')\n        if val_loss < self.min_val_loss:\n            self.min_val_loss = val_loss\n            self.wait = 0\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                self.model.stop_training = True\n                print(\"Training stopped due to early stopping.\")\n\nbatch = next(iter(val_dataset))\nidx_to_char = list(char_to_num.keys())\n\nmodel = Transformer(\n    num_hid=200,\n    num_head=4,\n    num_feed_forward=400,\n    source_maxlen = FRAME_LEN,\n    target_maxlen=64,\n    num_layers_enc=2,\n    num_layers_dec=1,\n    num_classes=62,\n)\n\n\n\nloss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1)\naccuracy_callback = AccuracyCallback()\noptimizer = keras.optimizers.Adam(0.0001)\n\n\n# モデルのコンパイル\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=[train_accuracy])","metadata":{"execution":{"iopub.status.busy":"2023-07-10T01:44:54.520597Z","iopub.execute_input":"2023-07-10T01:44:54.520870Z","iopub.status.idle":"2023-07-10T01:44:54.976277Z","shell.execute_reply.started":"2023-07-10T01:44:54.520849Z","shell.execute_reply":"2023-07-10T01:44:54.975316Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"#modelアーキテクト\n#tf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True, show_layer_names=True, expand_nested=True, show_layer_activations=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T01:44:54.977822Z","iopub.execute_input":"2023-07-10T01:44:54.978165Z","iopub.status.idle":"2023-07-10T01:44:54.983185Z","shell.execute_reply.started":"2023-07-10T01:44:54.978134Z","shell.execute_reply":"2023-07-10T01:44:54.982269Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"%%time\n# EarlyStoppingCallbackをコールバックリストに追加して学習を行う\nhistory = model.fit(train_dataset, verbose=2, validation_data=val_dataset, epochs=100,\n                    callbacks=[AccuracyCallback(), EarlyStoppingCallback()])","metadata":{"execution":{"iopub.status.busy":"2023-07-10T01:44:54.987689Z","iopub.execute_input":"2023-07-10T01:44:54.988729Z","iopub.status.idle":"2023-07-10T03:21:33.923013Z","shell.execute_reply.started":"2023-07-10T01:44:54.988697Z","shell.execute_reply":"2023-07-10T03:21:33.921864Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Epoch 1/100\nEpoch 1: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 186s - loss: 0.7929 - val_loss: 0.6534 - 186s/epoch - 122ms/step\nEpoch 2/100\nEpoch 2: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 157s - loss: 0.5824 - val_loss: 0.5545 - 157s/epoch - 103ms/step\nEpoch 3/100\nEpoch 3: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 152s - loss: 0.5257 - val_loss: 0.5175 - 152s/epoch - 100ms/step\nEpoch 4/100\nEpoch 4: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 171s - loss: 0.4973 - val_loss: 0.4946 - 171s/epoch - 113ms/step\nEpoch 5/100\nEpoch 5: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 177s - loss: 0.4767 - val_loss: 0.4819 - 177s/epoch - 117ms/step\nEpoch 6/100\nEpoch 6: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 176s - loss: 0.4610 - val_loss: 0.4698 - 176s/epoch - 115ms/step\nEpoch 7/100\nEpoch 7: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 167s - loss: 0.4491 - val_loss: 0.4627 - 167s/epoch - 110ms/step\nEpoch 8/100\nEpoch 8: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 162s - loss: 0.4393 - val_loss: 0.4577 - 162s/epoch - 106ms/step\nEpoch 9/100\nEpoch 9: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 164s - loss: 0.4312 - val_loss: 0.4496 - 164s/epoch - 108ms/step\nEpoch 10/100\nEpoch 10: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 181s - loss: 0.4238 - val_loss: 0.4449 - 181s/epoch - 119ms/step\nEpoch 11/100\nEpoch 11: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 171s - loss: 0.4170 - val_loss: 0.4447 - 171s/epoch - 112ms/step\nEpoch 12/100\nEpoch 12: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 175s - loss: 0.4112 - val_loss: 0.4405 - 175s/epoch - 115ms/step\nEpoch 13/100\nEpoch 13: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 175s - loss: 0.4059 - val_loss: 0.4359 - 175s/epoch - 115ms/step\nEpoch 14/100\nEpoch 14: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 176s - loss: 0.4008 - val_loss: 0.4337 - 176s/epoch - 116ms/step\nEpoch 15/100\nEpoch 15: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 178s - loss: 0.3961 - val_loss: 0.4362 - 178s/epoch - 117ms/step\nEpoch 16/100\nEpoch 16: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 182s - loss: 0.3919 - val_loss: 0.4318 - 182s/epoch - 119ms/step\nEpoch 17/100\nEpoch 17: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 177s - loss: 0.3874 - val_loss: 0.4315 - 177s/epoch - 116ms/step\nEpoch 18/100\nEpoch 18: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 184s - loss: 0.3835 - val_loss: 0.4345 - 184s/epoch - 121ms/step\nEpoch 19/100\nEpoch 19: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 179s - loss: 0.3798 - val_loss: 0.4321 - 179s/epoch - 118ms/step\nEpoch 20/100\nEpoch 20: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 179s - loss: 0.3759 - val_loss: 0.4336 - 179s/epoch - 118ms/step\nEpoch 21/100\nEpoch 21: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 182s - loss: 0.3726 - val_loss: 0.4320 - 182s/epoch - 119ms/step\nEpoch 22/100\nEpoch 22: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 179s - loss: 0.3692 - val_loss: 0.4334 - 179s/epoch - 117ms/step\nEpoch 23/100\nEpoch 23: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 176s - loss: 0.3659 - val_loss: 0.4312 - 176s/epoch - 116ms/step\nEpoch 24/100\nEpoch 24: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 178s - loss: 0.3627 - val_loss: 0.4318 - 178s/epoch - 117ms/step\nEpoch 25/100\nEpoch 25: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 178s - loss: 0.3595 - val_loss: 0.4304 - 178s/epoch - 117ms/step\nEpoch 26/100\nEpoch 26: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 185s - loss: 0.3565 - val_loss: 0.4314 - 185s/epoch - 122ms/step\nEpoch 27/100\nEpoch 27: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 179s - loss: 0.3534 - val_loss: 0.4351 - 179s/epoch - 118ms/step\nEpoch 28/100\nEpoch 28: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 180s - loss: 0.3509 - val_loss: 0.4368 - 180s/epoch - 118ms/step\nEpoch 29/100\nEpoch 29: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 177s - loss: 0.3478 - val_loss: 0.4371 - 177s/epoch - 117ms/step\nEpoch 30/100\nEpoch 30: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 177s - loss: 0.3449 - val_loss: 0.4361 - 177s/epoch - 117ms/step\nEpoch 31/100\nEpoch 31: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 183s - loss: 0.3423 - val_loss: 0.4398 - 183s/epoch - 120ms/step\nEpoch 32/100\nEpoch 32: Train Accuracy = 0.0, Validation Accuracy = 0.0\nTraining stopped due to early stopping.\n1520/1520 - 180s - loss: 0.3396 - val_loss: 0.4412 - 180s/epoch - 118ms/step\nCPU times: user 2h 4min 23s, sys: 7min 20s, total: 2h 11min 44s\nWall time: 1h 36min 38s\n","output_type":"stream"}]},{"cell_type":"code","source":"print(model.summary())","metadata":{"execution":{"iopub.status.busy":"2023-07-10T03:25:28.122654Z","iopub.execute_input":"2023-07-10T03:25:28.123709Z","iopub.status.idle":"2023-07-10T03:25:28.167205Z","shell.execute_reply.started":"2023-07-10T03:25:28.123671Z","shell.execute_reply":"2023-07-10T03:25:28.166237Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Model: \"transformer_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n landmark_embedding_1 (Landm  (None, 128, 200)         1495600   \n arkEmbedding)                                                   \n                                                                 \n token_embedding_1 (TokenEmb  multiple                 253600    \n edding)                                                         \n                                                                 \n sequential_8 (Sequential)   (None, 128, 200)          3103600   \n                                                                 \n transformer_decoder_1 (Tran  multiple                 1447000   \n sformerDecoder)                                                 \n                                                                 \n dense_25 (Dense)            multiple                  12462     \n                                                                 \n=================================================================\nTotal params: 4,816,664\nTrainable params: 4,815,062\nNon-trainable params: 1,602\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])","metadata":{"execution":{"iopub.status.busy":"2023-07-10T03:25:44.411503Z","iopub.execute_input":"2023-07-10T03:25:44.411869Z","iopub.status.idle":"2023-07-10T03:25:44.673335Z","shell.execute_reply.started":"2023-07-10T03:25:44.411840Z","shell.execute_reply":"2023-07-10T03:25:44.672414Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"[<matplotlib.lines.Line2D at 0x78cadc8949d0>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"code","source":"batches = [batch for batch in val_dataset]\n\npreds_list = []\nground_truth_list = []\n\nfor batch in batches[:1]:\n    source = batch[0]\n    target = batch[1].numpy()\n    bs = tf.shape(source)[0]\n    preds = model.generate(source, start_token_idx)\n    preds = preds.numpy()\n\n    for i in range(bs):\n        target_text = \"\".join([idx_to_char[_] for _ in target[i, :]])\n        ground_truth_list.append(target_text.replace('P', ''))\n        prediction = \"\"\n        for idx in preds[i, :]:\n            prediction += idx_to_char[idx]\n            if idx == end_token_idx:\n                break\n        preds_list.append(prediction)\n\nfor i in range(10):\n    print(ground_truth_list[i])\n    print(preds_list[i])\n    print('\\n~~~\\n')","metadata":{"execution":{"iopub.status.busy":"2023-07-10T03:25:48.136852Z","iopub.execute_input":"2023-07-10T03:25:48.137228Z","iopub.status.idle":"2023-07-10T03:25:54.923539Z","shell.execute_reply.started":"2023-07-10T03:25:48.137197Z","shell.execute_reply":"2023-07-10T03:25:54.922528Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"S3 creekhouseE\nS3 creek houseE\n\n~~~\n\nSscales/kuhaylahE\nSscales chayltouhayE\n\n~~~\n\nS1383 william lanierE\nS1383 william lanierE\n\n~~~\n\nS988 franklin laneE\nS888 funlin lakeE\n\n~~~\n\nS6920 northeast 661st roadE\nS6920 northeast wast roadE\n\n~~~\n\nSwww.freem.ne.jpE\nSwww.freem.me.jpE\n\n~~~\n\nShttps://jsi.is/hukuokaE\nShttps://jsssi.is/hkuurokaE\n\n~~~\n\nS239613 stolze streetE\nS2961 ar stole streas reastE\n\n~~~\n\nS271097 bayshore boulevardE\nS271097 bay core boulevardE\n\n~~~\n\nSfederico pearsonE\nS9 ederico pearonE\n\n~~~\n\n","output_type":"stream"}]},{"cell_type":"code","source":"ground_truth_processed = [ground_truth_list[i][1:-1] for i in range(len(ground_truth_list))]\npreds_list_processed = [preds_list[i][1:-1] for i in range(len(preds_list))]\nlev_dist = [lev.distance(ground_truth_processed[i], preds_list_processed[i])\n            for i in range(len(preds_list_processed))]\nN = [len(phrase) for phrase in ground_truth_processed]\n\nprint('Validation score: '+str((np.sum(N) - np.sum(lev_dist))/np.sum(N)))","metadata":{"execution":{"iopub.status.busy":"2023-07-10T03:26:03.892925Z","iopub.execute_input":"2023-07-10T03:26:03.893293Z","iopub.status.idle":"2023-07-10T03:26:03.902297Z","shell.execute_reply.started":"2023-07-10T03:26:03.893262Z","shell.execute_reply":"2023-07-10T03:26:03.901363Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"Validation score: 0.7735849056603774\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Levenstein Distance Train","metadata":{}},{"cell_type":"code","source":"# Compute Levenstein Distances\ndef get_ld_train():\n    N = 100 if IS_INTERACTIVE else 1000\n    LD_TRAIN = []\n    for idx, (frames, phrase_true) in enumerate(zip(tqdm(X_train, total=N), y_train)):\n        # Predict Phrase and Convert to String\n        phrase_pred = predict_phrase(frames).numpy()\n        phrase_pred = outputs2phrase(phrase_pred)\n        # True Phrase Ordinal to String\n        phrase_true = outputs2phrase(phrase_true)\n        # Add Levenstein Distance\n        LD_TRAIN.append({\n            'phrase_true': phrase_true,\n            'phrase_pred': phrase_pred,\n            'levenshtein_distance': levenshtein(phrase_pred, phrase_true),\n        })\n        # Take subset in interactive mode\n        if idx == N:\n            break\n            \n    # Convert to DataFrame\n    LD_TRAIN_DF = pd.DataFrame(LD_TRAIN)\n    \n    return LD_TRAIN_DF","metadata":{"execution":{"iopub.status.busy":"2023-07-10T03:26:08.348476Z","iopub.execute_input":"2023-07-10T03:26:08.348845Z","iopub.status.idle":"2023-07-10T03:26:08.356665Z","shell.execute_reply.started":"2023-07-10T03:26:08.348816Z","shell.execute_reply":"2023-07-10T03:26:08.355513Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"# TFLiteModel","metadata":{}},{"cell_type":"code","source":" class TFLiteModel(tf.Module):\n    def __init__(self, model):\n        super(TFLiteModel, self).__init__()\n        self.target_start_token_idx = start_token_idx\n        self.target_end_token_idx = end_token_idx\n        # Load the feature generation and main models\n        self.model = model\n\n    @tf.function(input_signature=[tf.TensorSpec(shape=[None, len(SEL_COLS)], dtype=tf.float32, name='inputs')])\n    def __call__(self, inputs, training=False):\n        # Preprocess Data\n        x = tf.cast(inputs, tf.float32)\n        x = x[None]\n        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, len(SEL_COLS))), lambda: tf.identity(x))\n        x = x[0]\n        x = pre_process(x)\n        x = x[None]\n        x = self.model.generate(x, self.target_start_token_idx)\n        x = x[0]\n        idx = tf.argmax(tf.cast(tf.equal(x, self.target_end_token_idx), tf.int32))\n        idx = tf.where(tf.math.less(idx, 1), tf.constant(2, dtype=tf.int64), idx)\n        x = x[1:idx]\n        x = tf.one_hot(x, 59)\n        return {'outputs': x}\n\ntflitemodel_base = TFLiteModel(model)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T03:26:12.064377Z","iopub.execute_input":"2023-07-10T03:26:12.064758Z","iopub.status.idle":"2023-07-10T03:26:12.077001Z","shell.execute_reply.started":"2023-07-10T03:26:12.064730Z","shell.execute_reply":"2023-07-10T03:26:12.075788Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"model.save_weights(\"model.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-07-10T03:26:15.742612Z","iopub.execute_input":"2023-07-10T03:26:15.742975Z","iopub.status.idle":"2023-07-10T03:26:15.903364Z","shell.execute_reply.started":"2023-07-10T03:26:15.742946Z","shell.execute_reply":"2023-07-10T03:26:15.902081Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\nkeras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]#, tf.lite.OpsSet.SELECT_TF_OPS]\ntflite_model = keras_model_converter.convert()\nwith open('/kaggle/working/model.tflite', 'wb') as f:\n    f.write(tflite_model)\n\ninfargs = {\"selected_columns\" : SEL_COLS}\n\nwith open('inference_args.json', \"w\") as json_file:\n    json.dump(infargs, json_file)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T03:26:18.771519Z","iopub.execute_input":"2023-07-10T03:26:18.772194Z","iopub.status.idle":"2023-07-10T03:27:41.968787Z","shell.execute_reply.started":"2023-07-10T03:26:18.772154Z","shell.execute_reply":"2023-07-10T03:27:41.967712Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"Tensor(\"cond_2/Pad:0\", shape=(None, 26, 3), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"code","source":"!zip submission.zip  './model.tflite' './inference_args.json'","metadata":{"execution":{"iopub.status.busy":"2023-07-10T03:27:46.900802Z","iopub.execute_input":"2023-07-10T03:27:46.901167Z","iopub.status.idle":"2023-07-10T03:27:49.370003Z","shell.execute_reply.started":"2023-07-10T03:27:46.901137Z","shell.execute_reply":"2023-07-10T03:27:49.368799Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"updating: model.tflite (deflated 19%)\nupdating: inference_args.json (deflated 85%)\n","output_type":"stream"}]},{"cell_type":"code","source":"interpreter = tf.lite.Interpreter(\"model.tflite\")\n\nREQUIRED_SIGNATURE = \"serving_default\"\nREQUIRED_OUTPUT = \"outputs\"\n\nwith open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n    character_map = json.load(f)\nrev_character_map = {j:i for i,j in character_map.items()}\n\nfound_signatures = list(interpreter.get_signature_list().keys())\n\nif REQUIRED_SIGNATURE not in found_signatures:\n    raise KernelEvalException('Required input signature not found.')\n\nprediction_fn = interpreter.get_signature_runner(\"serving_default\")\noutput = prediction_fn(inputs=batch[0][0])\nprediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\nprint(prediction_str)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T03:27:59.617129Z","iopub.execute_input":"2023-07-10T03:27:59.617513Z","iopub.status.idle":"2023-07-10T03:28:00.016011Z","shell.execute_reply.started":"2023-07-10T03:27:59.617481Z","shell.execute_reply":"2023-07-10T03:28:00.014255Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"885 south\n","output_type":"stream"}]}]}