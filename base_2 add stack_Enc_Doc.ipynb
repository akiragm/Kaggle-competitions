{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ColabPreprocess  --START\n置換<br>\n\n/kaggle/input/  <br>\n/content/","metadata":{"id":"uIY3gyjVDW16"}},{"cell_type":"code","source":"from google.colab import auth\nauth.authenticate_user()","metadata":{"id":"9D2OX6U0CaH6","execution":{"iopub.status.busy":"2023-07-20T01:05:36.869625Z","iopub.execute_input":"2023-07-20T01:05:36.869976Z","iopub.status.idle":"2023-07-20T01:05:36.912833Z","shell.execute_reply.started":"2023-07-20T01:05:36.869947Z","shell.execute_reply":"2023-07-20T01:05:36.910659Z"},"trusted":true},"execution_count":27,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auth\n\u001b[1;32m      2\u001b[0m auth\u001b[38;5;241m.\u001b[39mauthenticate_user()\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"],"ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","output_type":"error"}]},{"cell_type":"code","source":"! echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n! curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n! apt update\n! apt install gcsfuse","metadata":{"outputId":"18407b6a-1ad8-4a99-dc58-810147bc8eb5","id":"A6qpeTkECaH7","execution":{"iopub.status.busy":"2023-07-20T01:05:36.913715Z","iopub.status.idle":"2023-07-20T01:05:36.914062Z","shell.execute_reply.started":"2023-07-20T01:05:36.913896Z","shell.execute_reply":"2023-07-20T01:05:36.913913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#kaggle get_gcspath\nfrom kaggle_datasets import KaggleDatasets\nprint(KaggleDatasets().get_gcs_path())","metadata":{"id":"1GmDR8Lh1fku"}},{"cell_type":"code","source":"!pip install kaggle","metadata":{"id":"Pj3Q0v8LEtnE","outputId":"a860b38b-94f2-490a-f1fc-eea66d643685","execution":{"iopub.status.busy":"2023-07-20T01:05:36.916473Z","iopub.status.idle":"2023-07-20T01:05:36.916931Z","shell.execute_reply.started":"2023-07-20T01:05:36.916699Z","shell.execute_reply":"2023-07-20T01:05:36.916720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! mkdir -p asl-fingerspelling\n! gcsfuse  --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 kds-7a1de6f6fb09cdad50ebab8364ce5c9e2937606d5437de561aab1448 asl-fingerspelling","metadata":{"id":"olczO1_pC2TX","outputId":"f098309c-d088-475c-869c-c45fc06b6250","execution":{"iopub.status.busy":"2023-07-20T01:05:36.918788Z","iopub.status.idle":"2023-07-20T01:05:36.919282Z","shell.execute_reply.started":"2023-07-20T01:05:36.919018Z","shell.execute_reply":"2023-07-20T01:05:36.919039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! mkdir -p aslfr-parquets-to-tfrecords-cleaned\n! gcsfuse  --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 kds-6720764106d040caa24754175e3c58fcbf8d829fba1b2c2d59149b30 aslfr-parquets-to-tfrecords-cleaned","metadata":{"outputId":"c078897e-5d2b-436f-d6a9-4efab3a2ccf6","id":"K2KWdEtVCaH9","execution":{"iopub.status.busy":"2023-07-20T01:05:36.923558Z","iopub.status.idle":"2023-07-20T01:05:36.924104Z","shell.execute_reply.started":"2023-07-20T01:05:36.923764Z","shell.execute_reply":"2023-07-20T01:05:36.923789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install Levenshtein","metadata":{"id":"Esy-KMuR8ivw","outputId":"a54b7684-d243-4771-ea4e-e465ee9ddcfd","execution":{"iopub.status.busy":"2023-07-20T01:05:36.926139Z","iopub.status.idle":"2023-07-20T01:05:36.926933Z","shell.execute_reply.started":"2023-07-20T01:05:36.926692Z","shell.execute_reply":"2023-07-20T01:05:36.926714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"8L77lmNJEQYQ"}},{"cell_type":"markdown","source":"# --END  ColabPreprocess","metadata":{"id":"ZDYAPQiVEATw"}},{"cell_type":"markdown","source":"\n\n1. I used two transformer layer in the encoder instead of four.\n2. I used four attention heads instead of two.\n3. I used new tokens for SOS, EOS, and padding (very minor since Rohith used rare tokens for these purposes, but still- more 'correct').\n2. I fixed a bug (probably?) in the decoder's dropout layers, which did not have the training flag, resulting in dropout during inference. This change gave a nice bump in the score.\n3. I made the passing of the training flag explicit. I know it can be implicit since it is a kwarg, but explicit passing makes the whole thing more straightforward and maybe fix another one or two training-flag-related bugs along the way.\n4. I changed the positional encoding in the decoder from tf.keras.layers.Embedding to proper positional embeddings (i.e., the usual sines and cosines usually used for this purpose). This had a significant impact.\n5. I added positional embedding to the encoder. This, too, had a significant impact.\n","metadata":{"id":"ZDWdHG-n3D4U"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.metrics import Accuracy\nfrom skimage.transform import resize\nfrom sklearn.model_selection import train_test_split\nimport json\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport Levenshtein as lev\nimport os\nimport gc","metadata":{"id":"7RmQuWvj3D4f","execution":{"iopub.status.busy":"2023-07-20T04:35:08.724765Z","iopub.execute_input":"2023-07-20T04:35:08.725170Z","iopub.status.idle":"2023-07-20T04:35:17.413461Z","shell.execute_reply.started":"2023-07-20T04:35:08.725131Z","shell.execute_reply":"2023-07-20T04:35:17.412482Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# EDA","metadata":{"id":"4ZCG_2Ml3D4k"}},{"cell_type":"markdown","source":"inpdir = \"/kaggle/input/asl-fingerspelling\"\ndf = pd.read_csv(f'{inpdir}/train.csv')\ndf[\"phrase_bytes\"] = df[\"phrase\"].map(lambda x: x.encode(\"utf-8\"))\ndisplay(df.head())","metadata":{"id":"0973wX8O3D4m"}},{"cell_type":"markdown","source":"train_landmarks = pd.read_parquet('/kaggle/input/asl-fingerspelling/train_landmarks/1019715464.parquet')\nkeys = train_landmarks.keys()[1:]\ntrain_landmarks.head()","metadata":{"id":"mV5fnqEi3D4n"}},{"cell_type":"markdown","source":"# TFRecord","metadata":{"id":"wTZS8Qxe3D4p"}},{"cell_type":"markdown","source":"LPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\nPOSE = LPOSE + RPOSE\n\nRHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\nLHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\nPOSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n\nX = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\nY = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\nZ = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n\nSEL_COLS = X + Y + Z\nFRAME_LEN = 128\n\nX_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\nY_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\nZ_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n\nRHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\nLHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\nRPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\nLPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n\nprint('SEL_COLS size:' + str(len(SEL_COLS)))","metadata":{"id":"n-vOTOB73D4r"}},{"cell_type":"markdown","source":"def load_relevant_data_subset(pq_path):\n    return pd.read_parquet(pq_path, columns=SEL_COLS)\n\ncounter = 0\nfor file_id in tqdm(df.file_id.unique()):\n    \n    print(counter)\n    counter+=1\n    \n    pqfile = f\"{inpdir}/train_landmarks/{file_id}.parquet\"\n    if not os.path.isdir(\"tfds\"): os.mkdir(\"tfds\")\n    tffile = f\"tfds/{file_id}.tfrecord\"\n    seq_refs = df.loc[df.file_id == file_id]\n    seqs = load_relevant_data_subset(pqfile)\n    seqs_numpy = seqs.to_numpy()\n    with tf.io.TFRecordWriter(tffile) as file_writer:\n        for seq_id, phrase in zip(seq_refs.sequence_id, seq_refs.phrase_bytes):\n            frames = seqs_numpy[seqs.index == seq_id]\n            \n            r_nonan = np.sum(np.sum(np.isnan(frames[:, RHAND_IDX]), axis = 1) == 0)\n            l_nonan = np.sum(np.sum(np.isnan(frames[:, LHAND_IDX]), axis = 1) == 0)\n            no_nan = max(r_nonan, l_nonan)\n            \n            if 2*len(phrase)<no_nan:\n                features = {SEL_COLS[i]: tf.train.Feature(\n                    float_list=tf.train.FloatList(value=frames[:, i])) for i in range(len(SEL_COLS))}\n                features[\"phrase\"] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[phrase]))\n                record_bytes = tf.train.Example(features=tf.train.Features(feature=features)).SerializeToString()\n                file_writer.write(record_bytes)","metadata":{"id":"ShX-eGAV3D4s"}},{"cell_type":"markdown","source":"# Data loading","metadata":{"id":"6aC8op943D4u"}},{"cell_type":"markdown","source":"#### Here I use new tokens for padding, start and end of sentences. (Capitals are good since the original phrases have only lower case letters, besides numbers and various signs).","metadata":{"id":"V1yIVyUZ3D4v"}},{"cell_type":"code","source":"pad_token = 'P'\nstart_token = 'S'\nend_token = 'E'\npad_token_idx = 59\nstart_token_idx = 60\nend_token_idx = 61","metadata":{"id":"Z2NX_XA43D4w","execution":{"iopub.status.busy":"2023-07-20T04:35:17.415289Z","iopub.execute_input":"2023-07-20T04:35:17.417117Z","iopub.status.idle":"2023-07-20T04:35:17.422444Z","shell.execute_reply.started":"2023-07-20T04:35:17.417083Z","shell.execute_reply":"2023-07-20T04:35:17.421364Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n    char_to_num = json.load(f)\n\n\nchar_to_num[pad_token] = pad_token_idx\nchar_to_num[start_token] = start_token_idx\nchar_to_num[end_token] = end_token_idx\n\nnum_to_char = {j:i for i,j in char_to_num.items()}\n\n\ninpdir = \"/kaggle/input/asl-fingerspelling\"\ndf = pd.read_csv(f'{inpdir}/train.csv')\n\nLPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\nPOSE = LPOSE + RPOSE\n\nRHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\nLHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\nPOSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n\nX = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\nY = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\nZ = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n\nSEL_COLS = X + Y + Z\nFRAME_LEN = 128\n\nX_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\nY_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\nZ_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n\nRHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\nLHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\nRPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\nLPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n\nprint(RPOSE_IDX)","metadata":{"id":"vIrIgs3S3D4x","outputId":"0a14f688-1324-4d24-933b-0cd3393c5ad3","execution":{"iopub.status.busy":"2023-07-20T04:35:17.423984Z","iopub.execute_input":"2023-07-20T04:35:17.424363Z","iopub.status.idle":"2023-07-20T04:35:17.616482Z","shell.execute_reply.started":"2023-07-20T04:35:17.424331Z","shell.execute_reply":"2023-07-20T04:35:17.615220Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[47, 48, 49, 50, 51, 99, 100, 101, 102, 103, 151, 152, 153, 154, 155]\n","output_type":"stream"}]},{"cell_type":"code","source":"def resize_pad(x):\n    if tf.shape(x)[0] < FRAME_LEN:\n        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0], [0, 0]]))\n        print(x)\n    else:\n        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n    return x\n\ndef translate_landmarks(landmarks, max_translation):\n    translation = tf.random.uniform(shape=(1,), minval=-max_translation, maxval=max_translation)\n    translated_landmarks = landmarks + translation\n    return translated_landmarks\n\ndef scale_landmarks(landmarks, min_scale, max_scale):\n    scale_factor = tf.random.uniform(shape=(1,), minval=min_scale, maxval=max_scale)\n    scaled_landmarks = landmarks * scale_factor\n    return scaled_landmarks\n\ndef add_noise(landmarks, noise_level):\n    noise = tf.random.normal(shape=tf.shape(landmarks), mean=0., stddev=noise_level)\n    landmarks = landmarks + noise\n    return landmarks\n\ndef pre_process(x):\n\n    rhand = tf.gather(x, RHAND_IDX, axis=1)\n    lhand = tf.gather(x, LHAND_IDX, axis=1)\n    rpose = tf.gather(x, RPOSE_IDX, axis=1)\n    lpose = tf.gather(x, LPOSE_IDX, axis=1)\n\n    rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)\n    lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)\n\n    rnans = tf.math.count_nonzero(rnan_idx)\n    lnans = tf.math.count_nonzero(lnan_idx)\n\n    # For dominant hand\n    if rnans > lnans:\n        hand = lhand\n        pose = lpose\n\n        hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n        hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n        hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n        hand = tf.concat([1-hand_x, hand_y, hand_z], axis=1)\n\n        pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n        pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n        pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n        pose = tf.concat([1-pose_x, pose_y, pose_z], axis=1)\n    else:\n        hand = rhand\n        pose = rpose\n\n    hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n    hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n    hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n    hand = tf.concat([hand_x[..., tf.newaxis], hand_y[..., tf.newaxis], hand_z[..., tf.newaxis]], axis=-1)\n\n    mean = tf.math.reduce_mean(hand, axis=1)[:, tf.newaxis, :]\n    std = tf.math.reduce_std(hand, axis=1)[:, tf.newaxis, :]\n    hand = (hand - mean) / std\n\n    pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n    pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n    pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n    pose = tf.concat([pose_x[..., tf.newaxis], pose_y[..., tf.newaxis], pose_z[..., tf.newaxis]], axis=-1)\n\n    x = tf.concat([hand, pose], axis=1)\n    x = resize_pad(x)\n\n    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n    x = tf.reshape(x, (FRAME_LEN, len(LHAND_IDX) + len(LPOSE_IDX)))\n    return x","metadata":{"id":"CwsIEjSx3D4y","execution":{"iopub.status.busy":"2023-07-20T04:35:17.619391Z","iopub.execute_input":"2023-07-20T04:35:17.619776Z","iopub.status.idle":"2023-07-20T04:35:17.645308Z","shell.execute_reply.started":"2023-07-20T04:35:17.619742Z","shell.execute_reply":"2023-07-20T04:35:17.644213Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"table = tf.lookup.StaticHashTable(\n    initializer=tf.lookup.KeyValueTensorInitializer(\n        keys=list(char_to_num.keys()),\n        values=list(char_to_num.values()),\n    ),\n    default_value=tf.constant(-1),\n    name=\"class_weight\"\n)\n\ndef preprocess_fn(landmarks, phrase):\n    phrase = start_token + phrase + end_token\n    phrase = tf.strings.bytes_split(phrase)\n    phrase = table.lookup(phrase)\n    phrase = tf.pad(phrase, paddings=[[0, 64 - tf.shape(phrase)[0]]], mode = 'CONSTANT',\n                    constant_values = pad_token_idx)\n\n    # landmarksを前処理する\n    if tf.random.uniform(()) < 0.5:  # Random chance to perform translation\n        landmarks = translate_landmarks(landmarks, max_translation=10)\n    if tf.random.uniform(()) < 0.5:  # Random chance to perform scaling\n        landmarks = scale_landmarks(landmarks, min_scale=0.8, max_scale=1.2)\n    if tf.random.uniform(()) < 0.5:  # Random chance to add noise\n        landmarks = add_noise(landmarks, noise_level=0.05)\n\n    return pre_process(landmarks), phrase\n\n\ndef decode_fn(record_bytes):\n    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in SEL_COLS}\n    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n    features = tf.io.parse_single_example(record_bytes, schema)\n    phrase = features[\"phrase\"]\n    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in SEL_COLS])\n    landmarks = tf.transpose(landmarks)\n\n    return landmarks, phrase\n","metadata":{"id":"4qLrqdoW3D4z","execution":{"iopub.status.busy":"2023-07-20T04:35:17.647182Z","iopub.execute_input":"2023-07-20T04:35:17.648110Z","iopub.status.idle":"2023-07-20T04:35:20.539369Z","shell.execute_reply.started":"2023-07-20T04:35:17.648075Z","shell.execute_reply":"2023-07-20T04:35:20.538209Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"inpdir = \"/kaggle/input/aslfr-parquets-to-tfrecords-cleaned\"\ntffiles = df.file_id.map(lambda x: f'{inpdir}/tfds/{x}.tfrecord').unique()\n\nbatch_size = 32\nval_len = int(0.05 * len(tffiles))\n\ntrain_dataset = tf.data.TFRecordDataset(tffiles[val_len:]).map(decode_fn).map(preprocess_fn).shuffle(30000, reshuffle_each_iteration=True).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\nval_dataset = tf.data.TFRecordDataset(tffiles[:val_len]).map(decode_fn).map(preprocess_fn).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n","metadata":{"id":"Tf0xDk_O8zfc","outputId":"885596e8-155d-47d9-ae36-9b1de76bd1f8","execution":{"iopub.status.busy":"2023-07-20T04:35:20.541211Z","iopub.execute_input":"2023-07-20T04:35:20.541584Z","iopub.status.idle":"2023-07-20T04:35:22.979777Z","shell.execute_reply.started":"2023-07-20T04:35:20.541551Z","shell.execute_reply":"2023-07-20T04:35:22.978441Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Tensor(\"cond_4/Pad:0\", shape=(None, 26, 3), dtype=float32)\nTensor(\"cond_4/Pad:0\", shape=(None, 26, 3), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# The model","metadata":{"id":"XrwFS5h03D41"}},{"cell_type":"code","source":"LPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\n\nPOSE = LPOSE + RPOSE\n\nRHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\nLHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\nPOSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n\nX = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\nY = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\nZ = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n\nSEL_COLS = X + Y + Z\nFRAME_LEN = 128\n\nX_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\nY_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\nZ_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n\nRHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\nLHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\nRPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\nLPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n\nprint('SEL_COLS size:' + str(len(SEL_COLS)))","metadata":{"id":"RYHzcYzp3D41","outputId":"a06565e3-6f48-4691-fc99-76f69a889df6","execution":{"iopub.status.busy":"2023-07-20T04:35:22.982470Z","iopub.execute_input":"2023-07-20T04:35:22.986945Z","iopub.status.idle":"2023-07-20T04:35:23.001296Z","shell.execute_reply.started":"2023-07-20T04:35:22.986917Z","shell.execute_reply":"2023-07-20T04:35:23.000042Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"SEL_COLS size:156\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Here I implemented proper positional embeddings for both the encoder and the decoder.","metadata":{"id":"u9Erb4iq3D42"}},{"cell_type":"code","source":"class MLPBlock(tf.keras.layers.Layer):\n    def __init__(self, num_hid=64, num_layers=5):\n        super().__init__()\n        self.mlp = tf.keras.Sequential()\n        for _ in range(num_layers):\n            self.mlp.add(tf.keras.layers.Dense(num_hid, activation=tf.nn.gelu))\n        self.mlp.add(tf.keras.layers.Dense(num_hid))\n\n    def call(self, inputs):\n        return self.mlp(inputs)\n\n\nclass TokenEmbedding(keras.layers.Layer):\n    def __init__(self, num_vocab=61, maxlen=50, num_hid=256, mlp_num_layers=5):\n        super().__init__()\n        self.num_hid = num_hid\n        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n        self.pos_emb = self.positional_encoding(maxlen - 1, num_hid)\n        self.mlp_block = MLPBlock(num_hid, num_layers=mlp_num_layers)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        x = self.emb(x) * tf.math.sqrt(tf.cast(self.num_hid, tf.float32))\n        x = x + self.pos_emb[:maxlen, :]\n        x = self.mlp_block(x)\n        return x\n\n    def positional_encoding(self, maxlen, num_hid):\n        positions = tf.range(maxlen, dtype=tf.float32)[..., tf.newaxis]\n        depth = num_hid // 2\n        angles = positions / tf.pow(10000, tf.range(0, depth, 1, dtype=tf.float32) / num_hid)  # depthのインクリメントを修正\n        pos_encoding = tf.concat([tf.sin(angles), tf.cos(angles)], axis=-1)\n        return pos_encoding\n\n","metadata":{"id":"0HslbkR83D43","execution":{"iopub.status.busy":"2023-07-20T04:35:23.004185Z","iopub.execute_input":"2023-07-20T04:35:23.004831Z","iopub.status.idle":"2023-07-20T04:35:23.017406Z","shell.execute_reply.started":"2023-07-20T04:35:23.004806Z","shell.execute_reply":"2023-07-20T04:35:23.016390Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"class TokenEmbedding(layers.Layer):\n    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n        super().__init__()\n        self.num_hid = num_hid\n        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n        #self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n        '''\n        self.pos_emb = tf.math.divide(\n            self.positional_encoding(maxlen-1, num_hid),\n            tf.math.sqrt(tf.cast(num_hid, tf.float32)))\n        '''\n        self.pos_emb = self.positional_encoding(maxlen-1, num_hid)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        x = self.emb(x)\n        x = tf.math.multiply(x, tf.math.sqrt(tf.cast(self.num_hid, tf.float32)))\n        '''\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        return x + positions\n        '''\n        return x + self.pos_emb[:maxlen, :]\n    \n    def positional_encoding(self, maxlen, num_hid):\n        depth = num_hid/2\n        positions = tf.range(maxlen, dtype = tf.float32)[..., tf.newaxis]\n        depths = tf.range(depth, dtype = tf.float32)[np.newaxis, :]/depth\n        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n        angle_rads = tf.linalg.matmul(positions, angle_rates)\n        pos_encoding = tf.concat(\n          [tf.math.sin(angle_rads), tf.math.cos(angle_rads)],\n          axis=-1)\n        return pos_encoding\n\n\n","metadata":{"id":"Zh4liqdb3D44"}},{"cell_type":"code","source":"class LandmarkEmbedding(tf.keras.Model):\n    def __init__(self, num_hid=256, maxlen=100):\n        super(LandmarkEmbedding, self).__init__()\n        self.conv1 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn1 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu1 = tf.keras.layers.LeakyReLU()\n\n        self.conv2 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn2 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu2 = tf.keras.layers.LeakyReLU()\n        self.dropout2 = tf.keras.layers.Dropout(0.2)\n\n        self.conv3 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn3 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu3 = tf.keras.layers.LeakyReLU()\n        self.dropout3 = tf.keras.layers.Dropout(0.2)\n\n        self.conv4 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn4 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu4 = tf.keras.layers.LeakyReLU()\n        self.dropout4 = tf.keras.layers.Dropout(0.2)\n\n        self.sigmoid = tf.keras.layers.Activation('sigmoid')\n        self.pos_emb = self.positional_encoding(maxlen, num_hid)\n        self.maxlen = maxlen\n        self.num_hid = num_hid\n\n    def call(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.leaky_relu1(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.leaky_relu2(x)\n        x = self.dropout2(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.leaky_relu3(x)\n        x = self.dropout3(x)\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.leaky_relu4(x)\n        x = self.dropout4(x)\n        x = tf.math.multiply(x, tf.math.sqrt(tf.cast(self.num_hid, tf.float32)))\n        x = x + self.pos_emb\n\n        return self.sigmoid(x)\n\n    def positional_encoding(self, maxlen, num_hid):\n        depth = num_hid/2\n        positions = tf.range(maxlen, dtype=tf.float32)[..., tf.newaxis]\n        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth\n        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n        angle_rads = tf.linalg.matmul(positions, angle_rates)\n        pos_encoding = tf.concat([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], axis=-1)\n        return pos_encoding\n","metadata":{"id":"hhEzMMAN3D45","execution":{"iopub.status.busy":"2023-07-20T04:35:23.018957Z","iopub.execute_input":"2023-07-20T04:35:23.019295Z","iopub.status.idle":"2023-07-20T04:35:23.040583Z","shell.execute_reply.started":"2023-07-20T04:35:23.019265Z","shell.execute_reply":"2023-07-20T04:35:23.039485Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def gelu(x):\n    \"\"\"Gaussian Error Linear Unit activation function.\"\"\"\n    cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n    return x * cdf\n\nclass TransformerEncoder(keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=gelu),  # 活性化関数を gelu に変更\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\n# 4. より深いモデルの実装\nclass TransformerEncoderStack(keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, num_layers=4, rate=0.1):\n        super().__init__()\n        self.encoders = [TransformerEncoder(embed_dim, num_heads, feed_forward_dim, rate) for _ in range(num_layers)]\n\n    def call(self, inputs, training):\n        x = inputs\n        for encoder in self.encoders:\n            x = encoder(x, training)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-20T04:35:23.045176Z","iopub.execute_input":"2023-07-20T04:35:23.045489Z","iopub.status.idle":"2023-07-20T04:35:23.060212Z","shell.execute_reply.started":"2023-07-20T04:35:23.045454Z","shell.execute_reply":"2023-07-20T04:35:23.059113Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"class TransformerEncoder(keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"id":"Fhr9HWVb3D46","execution":{"iopub.status.busy":"2023-07-12T23:22:05.601020Z","iopub.execute_input":"2023-07-12T23:22:05.601598Z","iopub.status.idle":"2023-07-12T23:22:05.614449Z","shell.execute_reply.started":"2023-07-12T23:22:05.601565Z","shell.execute_reply":"2023-07-12T23:22:05.613752Z"}}},{"cell_type":"markdown","source":"#### Here I added the training flag to the TransformerDecoder's Dropout layers.","metadata":{"id":"6cbOcrFK3D47"}},{"cell_type":"markdown","source":"class TransformerDecoder(keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n        super().__init__()\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n        self.self_att = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.self_dropout = layers.Dropout(0.5)\n        self.enc_dropout = layers.Dropout(0.1)\n        self.ffn_dropout = layers.Dropout(0.1)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n\n    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n        \"\"\"Masks the upper half of the dot product matrix in self attention.\n\n        This prevents flow of information from future tokens to current token.\n        1's in the lower triangle, counting from the lower right corner.\n        \"\"\"\n        i = tf.range(n_dest)[:, None]\n        j = tf.range(n_src)\n        m = i >= j - n_src + n_dest\n        mask = tf.cast(m, dtype)\n        mask = tf.reshape(mask, [1, n_dest, n_src])\n        mult = tf.concat(\n            [batch_size[..., tf.newaxis], tf.constant([1, 1], dtype=tf.int32)], 0\n        )\n        return tf.tile(mask, mult)\n\n    def call(self, enc_out, target, training):\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n        target_att = self.self_att(target, target, attention_mask=causal_mask)\n        target_norm = self.layernorm1(target + self.self_dropout(target_att, training = training))\n        enc_out = self.enc_att(target_norm, enc_out)\n        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out, training = training) + target_norm)\n        ffn_out = self.ffn(enc_out_norm)\n        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out, training = training))\n        return ffn_out_norm","metadata":{"id":"Y35bT4wb3D48","execution":{"iopub.status.busy":"2023-07-20T01:09:50.014092Z","iopub.execute_input":"2023-07-20T01:09:50.014480Z","iopub.status.idle":"2023-07-20T01:09:50.027943Z","shell.execute_reply.started":"2023-07-20T01:09:50.014449Z","shell.execute_reply":"2023-07-20T01:09:50.026758Z"}}},{"cell_type":"code","source":"class TransformerDecoder(keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n        super().__init__()\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n        self.self_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.self_dropout = layers.Dropout(dropout_rate)  # ドロップアウト率を統一\n        self.enc_dropout = layers.Dropout(dropout_rate)\n        self.ffn_dropout = layers.Dropout(dropout_rate)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=gelu),  # 活性化関数を gelu に変更\n                layers.Dense(embed_dim),\n            ]\n        )\n\n    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n        \"\"\"Masks the upper half of the dot product matrix in self attention.\n\n        This prevents flow of information from future tokens to current token.\n        1's in the lower triangle, counting from the lower right corner.\n        \"\"\"\n        i = tf.range(n_dest)[:, None]\n        j = tf.range(n_src)\n        m = i >= j - n_src + n_dest\n        mask = tf.cast(m, dtype)\n        mask = tf.reshape(mask, [1, n_dest, n_src])\n        mult = tf.concat(\n            [batch_size[..., tf.newaxis], tf.constant([1, 1], dtype=tf.int32)], 0\n        )\n        return tf.tile(mask, mult)\n\n    def call(self, enc_out, target, training):\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n        target_att = self.self_att(target, target, attention_mask=causal_mask)\n        target_norm = self.layernorm1(target + self.self_dropout(target_att, training = training))\n        enc_out = self.enc_att(target_norm, enc_out)\n        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out, training = training) + target_norm)\n        ffn_out = self.ffn(enc_out_norm)\n        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out, training = training))\n        return ffn_out_norm\n\n# 3. より深いモデルの実装\nclass TransformerDecoderStack(keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, num_layers=2, rate=0.1):\n        super().__init__()\n        self.decoders = [TransformerDecoder(embed_dim, num_heads, feed_forward_dim, rate) for _ in range(num_layers)]\n\n    def call(self, enc_out, target, training):\n        x = target\n        for decoder in self.decoders:\n            x = decoder(enc_out, x, training)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-07-20T04:35:23.061799Z","iopub.execute_input":"2023-07-20T04:35:23.062299Z","iopub.status.idle":"2023-07-20T04:35:23.079415Z","shell.execute_reply.started":"2023-07-20T04:35:23.062269Z","shell.execute_reply":"2023-07-20T04:35:23.078535Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"#### Here I made the passing of the training flag explicit.","metadata":{"id":"McbjptyD3D49"}},{"cell_type":"code","source":"class Transformer(tf.keras.Model):\n    def __init__(\n        self,\n        num_hid=64,\n        num_head=2,\n        num_feed_forward=128,\n        source_maxlen=100,\n        target_maxlen=100,\n        num_layers_enc=4,\n        num_layers_dec=1,\n        num_classes=60,\n    ):\n        super().__init__()\n        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n        self.num_layers_enc = num_layers_enc\n        self.num_layers_dec = num_layers_dec\n        self.target_maxlen = target_maxlen\n        self.num_classes = num_classes\n        self.enc_input = LandmarkEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n        self.dec_input = TokenEmbedding(\n            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n        )\n\n        self.encoder = keras.Sequential(\n            [self.enc_input]\n            + [\n                TransformerEncoder(num_hid, num_head, num_feed_forward)\n                for _ in range(num_layers_enc)\n            ]\n        )\n\n        for i in range(num_layers_dec):\n            setattr(\n                self,\n                f\"dec_layer_{i}\",\n                TransformerDecoder(num_hid, num_head, num_feed_forward),\n            )\n\n        self.classifier = layers.Dense(num_classes)\n\n    def decode(self, enc_out, target, training):\n        y = self.dec_input(target)\n        for i in range(self.num_layers_dec):\n            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y, training)\n        return y\n\n    def call(self, inputs, training):\n        source = inputs[0]\n        target = inputs[1]\n        x = self.encoder(source, training)\n        y = self.decode(x, target, training)\n        return self.classifier(y)\n\n    @property\n    def metrics(self):\n        return [self.loss_metric]\n\n    def train_step(self, batch):\n        \"\"\"Processes one batch inside model.fit().\"\"\"\n        source = batch[0]\n        target = batch[1]\n\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n\n        dec_input = target[:, :-1]\n        dec_target = target[:, 1:]\n        with tf.GradientTape() as tape:\n            preds = self([source, dec_input])\n            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n            mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        self.loss_metric.update_state(loss)\n        return {\"loss\": self.loss_metric.result()}\n\n    def test_step(self, batch):\n        source = batch[0]\n        target = batch[1]\n\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n\n        dec_input = target[:, :-1]\n        dec_target = target[:, 1:]\n        preds = self([source, dec_input])\n        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n        mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n        self.loss_metric.update_state(loss)\n        return {\"loss\": self.loss_metric.result()}\n\n    def generate(self, source, target_start_token_idx):\n        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n        bs = tf.shape(source)[0]\n        enc = self.encoder(source, training = False)\n        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n        dec_logits = []\n        for i in range(self.target_maxlen - 1):\n            dec_out = self.decode(enc, dec_input, training = False)\n            logits = self.classifier(dec_out)\n            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n            last_logit = logits[:, -1][..., tf.newaxis]\n            dec_logits.append(last_logit)\n            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n        return dec_input","metadata":{"id":"1LDSYoCh3D49","execution":{"iopub.status.busy":"2023-07-20T04:35:23.081087Z","iopub.execute_input":"2023-07-20T04:35:23.081428Z","iopub.status.idle":"2023-07-20T04:35:23.103880Z","shell.execute_reply.started":"2023-07-20T04:35:23.081394Z","shell.execute_reply":"2023-07-20T04:35:23.102730Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# 正解率を計算するためのメトリクスを作成\ntrain_accuracy = tf.keras.metrics.CategoricalAccuracy()\nval_accuracy = tf.keras.metrics.CategoricalAccuracy()\n\n# 学習ループ内で正解率を更新するコールバックを定義\nclass AccuracyCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        train_acc = train_accuracy.result()\n        val_acc = val_accuracy.result()\n        print(f\"Epoch {epoch+1}: Train Accuracy = {train_acc}, Validation Accuracy = {val_acc}\")\n        # 正解率をリセット\n        train_accuracy.reset_states()\n        val_accuracy.reset_states()\n# val_lossが3回マイナスになった場合に学習を停止するコールバック\nclass EarlyStoppingCallback(tf.keras.callbacks.Callback):\n    def __init__(self, patience=3):\n        super(EarlyStoppingCallback, self).__init__()\n        self.patience = patience\n        self.min_val_loss = float('inf')\n        self.wait = 0\n\n    def on_epoch_end(self, epoch, logs=None):\n        val_loss = logs.get('val_loss')\n        if val_loss < self.min_val_loss:\n            self.min_val_loss = val_loss\n            self.wait = 0\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                self.model.stop_training = True\n                print(\"Training stopped due to early stopping.\")\n\nbatch = next(iter(val_dataset))\nidx_to_char = list(char_to_num.keys())\n\nmodel = Transformer(\n    num_hid=256,\n    num_head=4,\n    num_feed_forward=400,\n    source_maxlen = FRAME_LEN,\n    target_maxlen=64,\n    num_layers_enc=2,\n    num_layers_dec=1,\n    num_classes=62,\n)\n\n\n\nloss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1)\naccuracy_callback = AccuracyCallback()\noptimizer = keras.optimizers.Adam(0.0001)\n\n\n# モデルのコンパイル\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=[train_accuracy])","metadata":{"id":"RoQoEAwf3D4-","execution":{"iopub.status.busy":"2023-07-20T04:35:23.105425Z","iopub.execute_input":"2023-07-20T04:35:23.105807Z","iopub.status.idle":"2023-07-20T04:35:25.597400Z","shell.execute_reply.started":"2023-07-20T04:35:23.105775Z","shell.execute_reply":"2023-07-20T04:35:25.596412Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#modelアーキテクト\n#tf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True, show_layer_names=True, expand_nested=True, show_layer_activations=True)\n","metadata":{"id":"0IUChNM23D4_","execution":{"iopub.status.busy":"2023-07-20T04:35:25.598655Z","iopub.execute_input":"2023-07-20T04:35:25.598997Z","iopub.status.idle":"2023-07-20T04:35:25.603983Z","shell.execute_reply.started":"2023-07-20T04:35:25.598967Z","shell.execute_reply":"2023-07-20T04:35:25.602862Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# ##Optuna\n# pip install optuna","metadata":{"id":"yflrYw4v3D5A","execution":{"iopub.status.busy":"2023-07-20T04:35:25.605396Z","iopub.execute_input":"2023-07-20T04:35:25.605991Z","iopub.status.idle":"2023-07-20T04:35:25.619128Z","shell.execute_reply.started":"2023-07-20T04:35:25.605959Z","shell.execute_reply":"2023-07-20T04:35:25.617858Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"%%time\n# EarlyStoppingCallbackをコールバックリストに追加して学習を行う\nhistory = model.fit(train_dataset, verbose=2, validation_data=val_dataset, epochs=100,\n                    callbacks=[AccuracyCallback(), EarlyStoppingCallback()])\n","metadata":{"id":"7FvvtVwc3D5A","outputId":"dbfdc8d4-6f7e-480d-e503-af2f207cd5d9","execution":{"iopub.status.busy":"2023-07-20T04:35:25.620970Z","iopub.execute_input":"2023-07-20T04:35:25.621365Z","iopub.status.idle":"2023-07-20T06:34:34.295928Z","shell.execute_reply.started":"2023-07-20T04:35:25.621334Z","shell.execute_reply":"2023-07-20T06:34:34.294936Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/100\nEpoch 1: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 211s - loss: 0.8362 - val_loss: 0.7708 - 211s/epoch - 139ms/step\nEpoch 2/100\nEpoch 2: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 177s - loss: 0.6971 - val_loss: 0.6390 - 177s/epoch - 116ms/step\nEpoch 3/100\nEpoch 3: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 166s - loss: 0.6025 - val_loss: 0.5666 - 166s/epoch - 110ms/step\nEpoch 4/100\nEpoch 4: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 176s - loss: 0.5440 - val_loss: 0.5326 - 176s/epoch - 116ms/step\nEpoch 5/100\nEpoch 5: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 172s - loss: 0.5170 - val_loss: 0.5112 - 172s/epoch - 113ms/step\nEpoch 6/100\nEpoch 6: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 157s - loss: 0.4998 - val_loss: 0.5003 - 157s/epoch - 103ms/step\nEpoch 7/100\nEpoch 7: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 157s - loss: 0.4868 - val_loss: 0.4923 - 157s/epoch - 103ms/step\nEpoch 8/100\nEpoch 8: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 158s - loss: 0.4764 - val_loss: 0.4880 - 158s/epoch - 104ms/step\nEpoch 9/100\nEpoch 9: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 157s - loss: 0.4681 - val_loss: 0.4763 - 157s/epoch - 104ms/step\nEpoch 10/100\nEpoch 10: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 172s - loss: 0.4610 - val_loss: 0.4739 - 172s/epoch - 113ms/step\nEpoch 11/100\nEpoch 11: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 173s - loss: 0.4550 - val_loss: 0.4667 - 173s/epoch - 114ms/step\nEpoch 12/100\nEpoch 12: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 184s - loss: 0.4503 - val_loss: 0.4662 - 184s/epoch - 121ms/step\nEpoch 13/100\nEpoch 13: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 196s - loss: 0.4452 - val_loss: 0.4650 - 196s/epoch - 129ms/step\nEpoch 14/100\nEpoch 14: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 192s - loss: 0.4405 - val_loss: 0.4581 - 192s/epoch - 126ms/step\nEpoch 15/100\nEpoch 15: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 194s - loss: 0.4366 - val_loss: 0.4575 - 194s/epoch - 128ms/step\nEpoch 16/100\nEpoch 16: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 183s - loss: 0.4328 - val_loss: 0.4541 - 183s/epoch - 121ms/step\nEpoch 17/100\nEpoch 17: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 186s - loss: 0.4298 - val_loss: 0.4520 - 186s/epoch - 123ms/step\nEpoch 18/100\nEpoch 18: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 179s - loss: 0.4268 - val_loss: 0.4505 - 179s/epoch - 118ms/step\nEpoch 19/100\nEpoch 19: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 176s - loss: 0.4231 - val_loss: 0.4476 - 176s/epoch - 116ms/step\nEpoch 20/100\nEpoch 20: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 160s - loss: 0.4200 - val_loss: 0.4470 - 160s/epoch - 105ms/step\nEpoch 21/100\nEpoch 21: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 169s - loss: 0.4177 - val_loss: 0.4488 - 169s/epoch - 111ms/step\nEpoch 22/100\nEpoch 22: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 188s - loss: 0.4146 - val_loss: 0.4438 - 188s/epoch - 123ms/step\nEpoch 23/100\nEpoch 23: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 193s - loss: 0.4122 - val_loss: 0.4440 - 193s/epoch - 127ms/step\nEpoch 24/100\nEpoch 24: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 181s - loss: 0.4100 - val_loss: 0.4429 - 181s/epoch - 119ms/step\nEpoch 25/100\nEpoch 25: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 172s - loss: 0.4077 - val_loss: 0.4400 - 172s/epoch - 113ms/step\nEpoch 26/100\nEpoch 26: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 178s - loss: 0.4056 - val_loss: 0.4399 - 178s/epoch - 117ms/step\nEpoch 27/100\nEpoch 27: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 173s - loss: 0.4034 - val_loss: 0.4398 - 173s/epoch - 114ms/step\nEpoch 28/100\nEpoch 28: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 173s - loss: 0.4011 - val_loss: 0.4394 - 173s/epoch - 114ms/step\nEpoch 29/100\nEpoch 29: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 173s - loss: 0.3993 - val_loss: 0.4384 - 173s/epoch - 114ms/step\nEpoch 30/100\nEpoch 30: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 180s - loss: 0.3972 - val_loss: 0.4386 - 180s/epoch - 118ms/step\nEpoch 31/100\nEpoch 31: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 172s - loss: 0.3946 - val_loss: 0.4381 - 172s/epoch - 113ms/step\nEpoch 32/100\nEpoch 32: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 177s - loss: 0.3934 - val_loss: 0.4380 - 177s/epoch - 116ms/step\nEpoch 33/100\nEpoch 33: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 177s - loss: 0.3911 - val_loss: 0.4373 - 177s/epoch - 116ms/step\nEpoch 34/100\nEpoch 34: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 178s - loss: 0.3891 - val_loss: 0.4374 - 178s/epoch - 117ms/step\nEpoch 35/100\nEpoch 35: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 165s - loss: 0.3873 - val_loss: 0.4374 - 165s/epoch - 109ms/step\nEpoch 36/100\nEpoch 36: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 160s - loss: 0.3853 - val_loss: 0.4352 - 160s/epoch - 105ms/step\nEpoch 37/100\nEpoch 37: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 177s - loss: 0.3838 - val_loss: 0.4351 - 177s/epoch - 117ms/step\nEpoch 38/100\nEpoch 38: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 181s - loss: 0.3819 - val_loss: 0.4363 - 181s/epoch - 119ms/step\nEpoch 39/100\nEpoch 39: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 188s - loss: 0.3798 - val_loss: 0.4384 - 188s/epoch - 123ms/step\nEpoch 40/100\nEpoch 40: Train Accuracy = 0.0, Validation Accuracy = 0.0\nTraining stopped due to early stopping.\n1520/1520 - 185s - loss: 0.3786 - val_loss: 0.4368 - 185s/epoch - 122ms/step\nCPU times: user 2h 41min 5s, sys: 8min 9s, total: 2h 49min 14s\nWall time: 1h 59min 8s\n","output_type":"stream"}]},{"cell_type":"code","source":"print(model.summary())","metadata":{"id":"aT4MU6D43D5B","outputId":"e5575cd8-e58a-4a4b-84a2-03158c3e3381","execution":{"iopub.status.busy":"2023-07-20T06:36:53.453669Z","iopub.execute_input":"2023-07-20T06:36:53.454608Z","iopub.status.idle":"2023-07-20T06:36:53.494960Z","shell.execute_reply.started":"2023-07-20T06:36:53.454568Z","shell.execute_reply":"2023-07-20T06:36:53.494029Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Model: \"transformer\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n landmark_embedding (Landmar  (None, 128, 256)         2387456   \n kEmbedding)                                                     \n                                                                 \n token_embedding (TokenEmbed  multiple                 410624    \n ding)                                                           \n                                                                 \n sequential_3 (Sequential)   (None, 128, 256)          4904224   \n                                                                 \n transformer_decoder (Transf  multiple                 2310800   \n ormerDecoder)                                                   \n                                                                 \n dense_12 (Dense)            multiple                  15934     \n                                                                 \n=================================================================\nTotal params: 7,641,584\nTrainable params: 7,639,534\nNon-trainable params: 2,050\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])","metadata":{"id":"Dmc_Vs_f3D5B","outputId":"5f77c6b0-016f-48f9-aa0b-1b5116365bd7","execution":{"iopub.status.busy":"2023-07-20T06:37:09.588551Z","iopub.execute_input":"2023-07-20T06:37:09.588936Z","iopub.status.idle":"2023-07-20T06:37:09.869377Z","shell.execute_reply.started":"2023-07-20T06:37:09.588906Z","shell.execute_reply":"2023-07-20T06:37:09.868425Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7a54681daa70>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"# Validation","metadata":{"id":"8Fxn_TUt3D5D"}},{"cell_type":"code","source":"pip install termcolor","metadata":{"execution":{"iopub.status.busy":"2023-07-20T06:34:34.919535Z","iopub.execute_input":"2023-07-20T06:34:34.920123Z","iopub.status.idle":"2023-07-20T06:34:48.174066Z","shell.execute_reply.started":"2023-07-20T06:34:34.920079Z","shell.execute_reply":"2023-07-20T06:34:48.172822Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Requirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (2.3.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from collections import Counter\n\nbatches = [batch for batch in val_dataset]\n\npreds_list = []\nground_truth_list = []\n\nfor batch in batches[:1]:\n    source = batch[0]\n    target = batch[1].numpy()\n    bs = tf.shape(source)[0]\n    preds = model.generate(source, start_token_idx)\n    preds = preds.numpy()\n\n    for i in range(bs):\n        target_text = \"\".join([idx_to_char[_] for _ in target[i, :]])\n        ground_truth_list.append(target_text.replace('P', ''))\n        prediction = \"\"\n        for idx in preds[i, :]:\n            prediction += idx_to_char[idx]\n            if idx == end_token_idx:\n                break\n        preds_list.append(prediction)\n\ndata = {'Ground Truth': ground_truth_list, 'Prediction': preds_list}\ndf = pd.DataFrame(data)\n\ncorrect_predictions = (df['Ground Truth'] == df['Prediction']).sum()\ncorrect_words_list = [truth for truth, pred in zip(ground_truth_list, preds_list) if truth == pred]\n\ntotal_predictions = len(df)\naccuracy = correct_predictions / total_predictions\n\nprint(\"Ground Truth vs. Prediction:\")\nprint(df)\nprint(\"\\nAccuracy: {:.2%}\".format(accuracy))\n\nprint(\"\\nCorrect Predictions:\")\nfor word in correct_words_list:\n    print(word)\n\nword_counts = Counter(correct_words_list)\n\nprint(\"\\nCorrect Word Counts:\")\nfor word, count in word_counts.items():\n    print(f\"'{word}': {count} times\")\n","metadata":{"id":"2XcqFl4p3D5D","outputId":"a902365d-dda4-4f1d-d79e-8d291f8e0657","execution":{"iopub.status.busy":"2023-07-20T06:37:14.884844Z","iopub.execute_input":"2023-07-20T06:37:14.885449Z","iopub.status.idle":"2023-07-20T06:37:23.034989Z","shell.execute_reply.started":"2023-07-20T06:37:14.885387Z","shell.execute_reply":"2023-07-20T06:37:23.033991Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Ground Truth vs. Prediction:\n                        Ground Truth                        Prediction\n0                     S3 creekhouseE                  S33 creek houseE\n1                  Sscales/kuhaylahE                 Sseales chugaylaE\n2              S1383 william lanierE             S1385 william lanierE\n3                S988 franklin laneE                 S988 funtun 1120E\n4        S6920 northeast 661st roadE       S6920 northeast 666th roadE\n5                  Swww.freem.ne.jpE                 Swww.freem-me.jpE\n6           Shttps://jsi.is/hukuokaE         Shttps://jssi.is/hkurokaE\n7             S239613 stolze streetE          S239610 strolz earor stE\n8        S271097 bayshore boulevardE       S271097 brig coreboulevardE\n9                 Sfederico pearsonE                 S9 derico pearotE\n10     S/carpina/hope_&_faith/litleE  S/carpina/pe-s-s--fith/litli-ceE\n11                   Sdine-in/code/E                  Skine_cin/code/E\n12             S+264-97-568-217-145E             S+64-97-548-27-27-45E\n13                 S+51-2721-208-63E                 S+51-7121-308-63E\n14                  Swildberries_ruE                     S2661 georisE\n15                     Sleona owensE                      Slena owensE\n16                 S+220-557-859-04E                S+220-547-3859-47E\n17                     Skati castroE                     Skatic casroE\n18            S6867 granville driveE            S6867 pranville driveE\n19                 S1600 fire waterE                    S1600 tawaterE\n20                 S+45-39-007-1887E                   S+45-19-07-187E\n21         S65634/tennessee%20riverE              S65634/tennesseontoE\n22            S18 cutter ridge roadE            S18 cutter ridge roadE\n23                        Stampa flE                         Stama faE\n24  S492288 west 28th terrace southE     S49186 st 28th terrace southE\n25                  Sjami mcfarlandE             Sjamin mccfarll landE\n26       Swww.horseillustrated.com/E        Swww.horestraterated.com/E\n27           S1121 east 2609th roadE           S1121 east 2609th roadE\n28               S1786 cll cinturonE                 S7886 glee driveE\n29                    S620-510-6135E                    S620-510-6139E\n30      Sangeli/bombillas-a-presionE        Sangeli/bombillas-presionE\n31      S3719 oakview terrace driveE      S37190 akview terrace driveE\n\nAccuracy: 6.25%\n\nCorrect Predictions:\nS18 cutter ridge roadE\nS1121 east 2609th roadE\n\nCorrect Word Counts:\n'S18 cutter ridge roadE': 1 times\n'S1121 east 2609th roadE': 1 times\n","output_type":"stream"}]},{"cell_type":"code","source":"ground_truth_processed = [ground_truth_list[i][1:-1] for i in range(len(ground_truth_list))]\npreds_list_processed = [preds_list[i][1:-1] for i in range(len(preds_list))]\nlev_dist = [lev.distance(ground_truth_processed[i], preds_list_processed[i])\n            for i in range(len(preds_list_processed))]\nN = [len(phrase) for phrase in ground_truth_processed]\n\nprint('Validation score: '+str((np.sum(N) - np.sum(lev_dist))/np.sum(N)))","metadata":{"id":"F8EMyPau3D5E","outputId":"7a556d6f-3eb3-4c34-be4a-6a3ddb0f6a43","execution":{"iopub.status.busy":"2023-07-20T06:37:42.716425Z","iopub.execute_input":"2023-07-20T06:37:42.716888Z","iopub.status.idle":"2023-07-20T06:37:42.725229Z","shell.execute_reply.started":"2023-07-20T06:37:42.716854Z","shell.execute_reply":"2023-07-20T06:37:42.723940Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Validation score: 0.758147512864494\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Levenstein Distance Train","metadata":{"id":"3PwS5F3j3D5F"}},{"cell_type":"code","source":"# Compute Levenstein Distances\ndef get_ld_train():\n    N = 100 if IS_INTERACTIVE else 1000\n    LD_TRAIN = []\n    for idx, (frames, phrase_true) in enumerate(zip(tqdm(X_train, total=N), y_train)):\n        # Predict Phrase and Convert to String\n        phrase_pred = predict_phrase(frames).numpy()\n        phrase_pred = outputs2phrase(phrase_pred)\n        # True Phrase Ordinal to String\n        phrase_true = outputs2phrase(phrase_true)\n        # Add Levenstein Distance\n        LD_TRAIN.append({\n            'phrase_true': phrase_true,\n            'phrase_pred': phrase_pred,\n            'levenshtein_distance': levenshtein(phrase_pred, phrase_true),\n        })\n        # Take subset in interactive mode\n        if idx == N:\n            break\n\n    # Convert to DataFrame\n    LD_TRAIN_DF = pd.DataFrame(LD_TRAIN)\n\n    return LD_TRAIN_DF","metadata":{"id":"C289LerY3D5F","execution":{"iopub.status.busy":"2023-07-20T06:38:14.327287Z","iopub.execute_input":"2023-07-20T06:38:14.327659Z","iopub.status.idle":"2023-07-20T06:38:14.335082Z","shell.execute_reply.started":"2023-07-20T06:38:14.327629Z","shell.execute_reply":"2023-07-20T06:38:14.334178Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# TFLiteModel","metadata":{"id":"yXPYfSur3D5G"}},{"cell_type":"code","source":" class TFLiteModel(tf.Module):\n    def __init__(self, model):\n        super(TFLiteModel, self).__init__()\n        self.target_start_token_idx = start_token_idx\n        self.target_end_token_idx = end_token_idx\n        # Load the feature generation and main models\n        self.model = model\n\n    @tf.function(input_signature=[tf.TensorSpec(shape=[None, len(SEL_COLS)], dtype=tf.float32, name='inputs')])\n    def __call__(self, inputs, training=False):\n        # Preprocess Data\n        x = tf.cast(inputs, tf.float32)\n        x = x[None]\n        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, len(SEL_COLS))), lambda: tf.identity(x))\n        x = x[0]\n        x = pre_process(x)\n        x = x[None]\n        x = self.model.generate(x, self.target_start_token_idx)\n        x = x[0]\n        idx = tf.argmax(tf.cast(tf.equal(x, self.target_end_token_idx), tf.int32))\n        idx = tf.where(tf.math.less(idx, 1), tf.constant(2, dtype=tf.int64), idx)\n        x = x[1:idx]\n        x = tf.one_hot(x, 59)\n        return {'outputs': x}\n\ntflitemodel_base = TFLiteModel(model)","metadata":{"id":"q62jNFIt3D5G","execution":{"iopub.status.busy":"2023-07-20T06:38:31.169156Z","iopub.execute_input":"2023-07-20T06:38:31.169515Z","iopub.status.idle":"2023-07-20T06:38:31.182851Z","shell.execute_reply.started":"2023-07-20T06:38:31.169470Z","shell.execute_reply":"2023-07-20T06:38:31.181457Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"#モデル保存\nmodel.save_weights(\"model.h5\")\nmodel.save('model_and_weight', save_format=\"tf\")","metadata":{"id":"n7LiyIW43D5r","execution":{"iopub.status.busy":"2023-07-20T06:41:52.539125Z","iopub.execute_input":"2023-07-20T06:41:52.539535Z","iopub.status.idle":"2023-07-20T06:42:09.247388Z","shell.execute_reply.started":"2023-07-20T06:41:52.539483Z","shell.execute_reply":"2023-07-20T06:42:09.246244Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# 重みの読み込み\nmodel.load_weights('model.h5')\n\n# モデルの形状を表示\nmodel.summary()","metadata":{"id":"mmIvipm83D5s","outputId":"551af2fe-b8ff-4c1c-c5d3-fd59b53129a2","execution":{"iopub.status.busy":"2023-07-20T06:42:17.254658Z","iopub.execute_input":"2023-07-20T06:42:17.255055Z","iopub.status.idle":"2023-07-20T06:42:17.449787Z","shell.execute_reply.started":"2023-07-20T06:42:17.255022Z","shell.execute_reply":"2023-07-20T06:42:17.448824Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Model: \"transformer\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n landmark_embedding (Landmar  (None, 128, 256)         2387456   \n kEmbedding)                                                     \n                                                                 \n token_embedding (TokenEmbed  multiple                 410624    \n ding)                                                           \n                                                                 \n sequential_3 (Sequential)   (None, 128, 256)          4904224   \n                                                                 \n transformer_decoder (Transf  multiple                 2310800   \n ormerDecoder)                                                   \n                                                                 \n dense_12 (Dense)            multiple                  15934     \n                                                                 \n=================================================================\nTotal params: 7,641,584\nTrainable params: 7,639,534\nNon-trainable params: 2,050\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\nkeras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]#, tf.lite.OpsSet.SELECT_TF_OPS]\ntflite_model = keras_model_converter.convert()\nwith open('/kaggle/working/model.tflite', 'wb') as f:\n    f.write(tflite_model)\n\ninfargs = {\"selected_columns\" : SEL_COLS}\n\nwith open('inference_args.json', \"w\") as json_file:\n    json.dump(infargs, json_file)","metadata":{"id":"MygKEnwt3D5t","outputId":"70e17996-ced5-4a09-c532-bb28ddc1718f","execution":{"iopub.status.busy":"2023-07-20T06:42:24.606835Z","iopub.execute_input":"2023-07-20T06:42:24.607434Z","iopub.status.idle":"2023-07-20T06:43:50.208702Z","shell.execute_reply.started":"2023-07-20T06:42:24.607393Z","shell.execute_reply":"2023-07-20T06:43:50.207657Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Tensor(\"cond_2/Pad:0\", shape=(None, 26, 3), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"code","source":"!zip submission.zip  './model.tflite' './inference_args.json'","metadata":{"id":"P02lJRDR3D5u","outputId":"ccc74bc4-c180-42ae-f07c-bbe9ce0ce045","execution":{"iopub.status.busy":"2023-07-20T06:43:55.260323Z","iopub.execute_input":"2023-07-20T06:43:55.260713Z","iopub.status.idle":"2023-07-20T06:43:58.312412Z","shell.execute_reply.started":"2023-07-20T06:43:55.260682Z","shell.execute_reply":"2023-07-20T06:43:58.311267Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"  adding: model.tflite (deflated 15%)\n  adding: inference_args.json (deflated 85%)\n","output_type":"stream"}]},{"cell_type":"code","source":"interpreter = tf.lite.Interpreter(\"model.tflite\")\n\nREQUIRED_SIGNATURE = \"serving_default\"\nREQUIRED_OUTPUT = \"outputs\"\n\nwith open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n    character_map = json.load(f)\nrev_character_map = {j:i for i,j in character_map.items()}\n\nfound_signatures = list(interpreter.get_signature_list().keys())\n\nif REQUIRED_SIGNATURE not in found_signatures:\n    raise KernelEvalException('Required input signature not found.')\n\nprediction_fn = interpreter.get_signature_runner(\"serving_default\")\noutput = prediction_fn(inputs=batch[0][0])\nprediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\nprint(prediction_str)","metadata":{"id":"eKivsj_33D5v","outputId":"97aadf82-3a01-4a6d-9e0e-18f16157502b","trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"9995 burg rouge\n","output_type":"stream"}]}]}