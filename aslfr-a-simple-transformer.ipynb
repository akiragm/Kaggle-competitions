{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n1. I used two transformer layer in the encoder instead of four.\n2. I used four attention heads instead of two.\n3. I used new tokens for SOS, EOS, and padding (very minor since Rohith used rare tokens for these purposes, but still- more 'correct').\n2. I fixed a bug (probably?) in the decoder's dropout layers, which did not have the training flag, resulting in dropout during inference. This change gave a nice bump in the score.\n3. I made the passing of the training flag explicit. I know it can be implicit since it is a kwarg, but explicit passing makes the whole thing more straightforward and maybe fix another one or two training-flag-related bugs along the way.\n4. I changed the positional encoding in the decoder from tf.keras.layers.Embedding to proper positional embeddings (i.e., the usual sines and cosines usually used for this purpose). This had a significant impact.\n5. I added positional embedding to the encoder. This, too, had a significant impact.\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.metrics import Accuracy\nfrom skimage.transform import resize\nfrom sklearn.model_selection import train_test_split\nimport json\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport Levenshtein as lev\nimport os\nimport gc","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:29:45.864958Z","iopub.execute_input":"2023-07-11T04:29:45.865358Z","iopub.status.idle":"2023-07-11T04:29:49.371894Z","shell.execute_reply.started":"2023-07-11T04:29:45.865329Z","shell.execute_reply":"2023-07-11T04:29:49.370660Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"inpdir = \"/kaggle/input/asl-fingerspelling\"\ndf = pd.read_csv(f'{inpdir}/train.csv')\ndf[\"phrase_bytes\"] = df[\"phrase\"].map(lambda x: x.encode(\"utf-8\"))\ndisplay(df.head())","metadata":{}},{"cell_type":"markdown","source":"train_landmarks = pd.read_parquet('/kaggle/input/asl-fingerspelling/train_landmarks/1019715464.parquet')\nkeys = train_landmarks.keys()[1:]\ntrain_landmarks.head()","metadata":{}},{"cell_type":"markdown","source":"# TFRecord","metadata":{}},{"cell_type":"markdown","source":"LPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\nPOSE = LPOSE + RPOSE\n\nRHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\nLHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\nPOSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n\nX = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\nY = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\nZ = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n\nSEL_COLS = X + Y + Z\nFRAME_LEN = 128\n\nX_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\nY_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\nZ_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n\nRHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\nLHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\nRPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\nLPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n\nprint('SEL_COLS size:' + str(len(SEL_COLS)))","metadata":{}},{"cell_type":"markdown","source":"def load_relevant_data_subset(pq_path):\n    return pd.read_parquet(pq_path, columns=SEL_COLS)\n\ncounter = 0\nfor file_id in tqdm(df.file_id.unique()):\n    \n    print(counter)\n    counter+=1\n    \n    pqfile = f\"{inpdir}/train_landmarks/{file_id}.parquet\"\n    if not os.path.isdir(\"tfds\"): os.mkdir(\"tfds\")\n    tffile = f\"tfds/{file_id}.tfrecord\"\n    seq_refs = df.loc[df.file_id == file_id]\n    seqs = load_relevant_data_subset(pqfile)\n    seqs_numpy = seqs.to_numpy()\n    with tf.io.TFRecordWriter(tffile) as file_writer:\n        for seq_id, phrase in zip(seq_refs.sequence_id, seq_refs.phrase_bytes):\n            frames = seqs_numpy[seqs.index == seq_id]\n            \n            r_nonan = np.sum(np.sum(np.isnan(frames[:, RHAND_IDX]), axis = 1) == 0)\n            l_nonan = np.sum(np.sum(np.isnan(frames[:, LHAND_IDX]), axis = 1) == 0)\n            no_nan = max(r_nonan, l_nonan)\n            \n            if 2*len(phrase)<no_nan:\n                features = {SEL_COLS[i]: tf.train.Feature(\n                    float_list=tf.train.FloatList(value=frames[:, i])) for i in range(len(SEL_COLS))}\n                features[\"phrase\"] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[phrase]))\n                record_bytes = tf.train.Example(features=tf.train.Features(feature=features)).SerializeToString()\n                file_writer.write(record_bytes)","metadata":{}},{"cell_type":"markdown","source":"# Data loading","metadata":{}},{"cell_type":"markdown","source":"#### Here I use new tokens for padding, start and end of sentences. (Capitals are good since the original phrases have only lower case letters, besides numbers and various signs).","metadata":{}},{"cell_type":"code","source":"pad_token = 'P'\nstart_token = 'S'\nend_token = 'E'\npad_token_idx = 59\nstart_token_idx = 60\nend_token_idx = 61","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:29:54.002625Z","iopub.execute_input":"2023-07-11T04:29:54.003344Z","iopub.status.idle":"2023-07-11T04:29:54.009044Z","shell.execute_reply.started":"2023-07-11T04:29:54.003311Z","shell.execute_reply":"2023-07-11T04:29:54.007921Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n    char_to_num = json.load(f)\n\n\nchar_to_num[pad_token] = pad_token_idx\nchar_to_num[start_token] = start_token_idx\nchar_to_num[end_token] = end_token_idx\n\nnum_to_char = {j:i for i,j in char_to_num.items()}\n\n\ninpdir = \"/kaggle/input/asl-fingerspelling\"\ndf = pd.read_csv(f'{inpdir}/train.csv')\n\nLPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\nPOSE = LPOSE + RPOSE\n\nRHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\nLHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\nPOSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n\nX = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\nY = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\nZ = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n\nSEL_COLS = X + Y + Z\nFRAME_LEN = 128\n\nX_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\nY_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\nZ_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n\nRHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\nLHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\nRPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\nLPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n\nprint(RPOSE_IDX)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:29:56.627107Z","iopub.execute_input":"2023-07-11T04:29:56.627472Z","iopub.status.idle":"2023-07-11T04:29:56.739106Z","shell.execute_reply.started":"2023-07-11T04:29:56.627444Z","shell.execute_reply":"2023-07-11T04:29:56.738070Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[47, 48, 49, 50, 51, 99, 100, 101, 102, 103, 151, 152, 153, 154, 155]\n","output_type":"stream"}]},{"cell_type":"code","source":"def resize_pad(x):\n    if tf.shape(x)[0] < FRAME_LEN:\n        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0], [0, 0]]))\n        print(x)\n    else:\n        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n    return x\n\ndef translate_landmarks(landmarks, max_translation):\n    translation = tf.random.uniform(shape=tf.shape(landmarks), minval=-max_translation, maxval=max_translation)\n    translated_landmarks = landmarks + translation\n    return translated_landmarks\n\n# def scale_landmarks(landmarks, min_scale, max_scale):\n#     scale_factor = tf.random.uniform(shape=tf.shape(landmarks), minval=min_scale, maxval=max_scale)\n#     scaled_landmarks = landmarks * scale_factor\n#     return scaled_landmarks\n\ndef pre_process(x):\n\n    rhand = tf.gather(x, RHAND_IDX, axis=1)\n    lhand = tf.gather(x, LHAND_IDX, axis=1)\n    rpose = tf.gather(x, RPOSE_IDX, axis=1)\n    lpose = tf.gather(x, LPOSE_IDX, axis=1)\n\n    rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)\n    lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)\n\n    rnans = tf.math.count_nonzero(rnan_idx)\n    lnans = tf.math.count_nonzero(lnan_idx)\n\n    # For dominant hand\n    if rnans > lnans:\n        hand = lhand\n        pose = lpose\n\n        hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n        hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n        hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n        hand = tf.concat([1-hand_x, hand_y, hand_z], axis=1)\n\n        pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n        pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n        pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n        pose = tf.concat([1-pose_x, pose_y, pose_z], axis=1)\n    else:\n        hand = rhand\n        pose = rpose\n\n    hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n    hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n    hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n    hand = tf.concat([hand_x[..., tf.newaxis], hand_y[..., tf.newaxis], hand_z[..., tf.newaxis]], axis=-1)\n\n    mean = tf.math.reduce_mean(hand, axis=1)[:, tf.newaxis, :]\n    std = tf.math.reduce_std(hand, axis=1)[:, tf.newaxis, :]\n    hand = (hand - mean) / std\n\n    pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n    pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n    pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n    pose = tf.concat([pose_x[..., tf.newaxis], pose_y[..., tf.newaxis], pose_z[..., tf.newaxis]], axis=-1)\n\n    x = tf.concat([hand, pose], axis=1)\n    x = resize_pad(x)\n\n    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n    x = tf.reshape(x, (FRAME_LEN, len(LHAND_IDX) + len(LPOSE_IDX)))\n    return x","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:29:59.962426Z","iopub.execute_input":"2023-07-11T04:29:59.962826Z","iopub.status.idle":"2023-07-11T04:29:59.985442Z","shell.execute_reply.started":"2023-07-11T04:29:59.962793Z","shell.execute_reply":"2023-07-11T04:29:59.983906Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"table = tf.lookup.StaticHashTable(\n    initializer=tf.lookup.KeyValueTensorInitializer(\n        keys=list(char_to_num.keys()),\n        values=list(char_to_num.values()),\n    ),\n    default_value=tf.constant(-1),\n    name=\"class_weight\"\n)\n\ndef preprocess_fn(landmarks, phrase):\n    phrase = start_token + phrase + end_token\n    phrase = tf.strings.bytes_split(phrase)\n    phrase = table.lookup(phrase)\n    phrase = tf.pad(phrase, paddings=[[0, 64 - tf.shape(phrase)[0]]], mode = 'CONSTANT',\n                    constant_values = pad_token_idx)\n\n    # landmarksを前処理する\n    translated_landmarks = translate_landmarks(landmarks, max_translation=10)\n    #scaled_landmarks = scale_landmarks(landmarks, min_scale=0.8, max_scale=1.2)\n\n    # 前処理済みのlandmarksを結合する\n    #combined_landmarks = tf.concat([landmarks, translated_landmarks, scaled_landmarks], axis=1)\n    combined_landmarks = tf.concat([landmarks, translated_landmarks], axis=1)\n    return pre_process(combined_landmarks), phrase\n\ndef decode_fn(record_bytes):\n    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in SEL_COLS}\n    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n    features = tf.io.parse_single_example(record_bytes, schema)\n    phrase = features[\"phrase\"]\n    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in SEL_COLS])\n    landmarks = tf.transpose(landmarks)\n\n    return landmarks, phrase\n\ninpdir = \"/kaggle/input/aslfr-parquets-to-tfrecords-cleaned\"\ntffiles = df.file_id.map(lambda x: f'{inpdir}/tfds/{x}.tfrecord').unique()\n\nbatch_size = 32\nval_len = int(0.05 * len(tffiles))\n\ntrain_dataset = tf.data.TFRecordDataset(tffiles[val_len:]).map(decode_fn).map(preprocess_fn).shuffle(30000, reshuffle_each_iteration=True).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\nval_dataset = tf.data.TFRecordDataset(tffiles[:val_len]).map(decode_fn).map(preprocess_fn).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:30:09.866505Z","iopub.execute_input":"2023-07-11T04:30:09.866872Z","iopub.status.idle":"2023-07-11T04:30:10.821039Z","shell.execute_reply.started":"2023-07-11T04:30:09.866837Z","shell.execute_reply":"2023-07-11T04:30:10.820013Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Tensor(\"cond_1/Pad:0\", shape=(None, 26, 3), dtype=float32)\nTensor(\"cond_1/Pad:0\", shape=(None, 26, 3), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# The model","metadata":{}},{"cell_type":"code","source":"LPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\n\nPOSE = LPOSE + RPOSE\n\nRHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\nLHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\nPOSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n\nX = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\nY = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\nZ = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n\nSEL_COLS = X + Y + Z\nFRAME_LEN = 128\n\nX_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\nY_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\nZ_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n\nRHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\nLHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\nRPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\nLPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n\nprint('SEL_COLS size:' + str(len(SEL_COLS)))","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:30:16.362876Z","iopub.execute_input":"2023-07-11T04:30:16.363254Z","iopub.status.idle":"2023-07-11T04:30:16.379665Z","shell.execute_reply.started":"2023-07-11T04:30:16.363225Z","shell.execute_reply":"2023-07-11T04:30:16.378530Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"SEL_COLS size:156\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Here I implemented proper positional embeddings for both the encoder and the decoder.","metadata":{}},{"cell_type":"code","source":"class MLPBlock(tf.keras.layers.Layer):\n    def __init__(self, num_hid=64, num_layers=5):\n        super().__init__()\n        self.mlp = tf.keras.Sequential()\n        for _ in range(num_layers):\n            self.mlp.add(tf.keras.layers.Dense(num_hid, activation=tf.nn.gelu))\n        self.mlp.add(tf.keras.layers.Dense(num_hid))\n\n    def call(self, inputs):\n        return self.mlp(inputs)\n\n\nclass TokenEmbedding(keras.layers.Layer):\n    def __init__(self, num_vocab=61, maxlen=50, num_hid=256, mlp_num_layers=5):\n        super().__init__()\n        self.num_hid = num_hid\n        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n        self.pos_emb = self.positional_encoding(maxlen - 1, num_hid)\n        self.mlp_block = MLPBlock(num_hid, num_layers=mlp_num_layers)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        x = self.emb(x) * tf.math.sqrt(tf.cast(self.num_hid, tf.float32))\n        x = x + self.pos_emb[:maxlen, :]\n        x = self.mlp_block(x)\n        return x\n\n    def positional_encoding(self, maxlen, num_hid):\n        positions = tf.range(maxlen, dtype=tf.float32)[..., tf.newaxis]\n        depth = num_hid // 2\n        angles = positions / tf.pow(10000, tf.range(0, depth, 1, dtype=tf.float32) / num_hid)  # depthのインクリメントを修正\n        pos_encoding = tf.concat([tf.sin(angles), tf.cos(angles)], axis=-1)\n        return pos_encoding\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:30:20.755941Z","iopub.execute_input":"2023-07-11T04:30:20.756302Z","iopub.status.idle":"2023-07-11T04:30:20.770649Z","shell.execute_reply.started":"2023-07-11T04:30:20.756273Z","shell.execute_reply":"2023-07-11T04:30:20.768705Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"class TokenEmbedding(layers.Layer):\n    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n        super().__init__()\n        self.num_hid = num_hid\n        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n        #self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n        '''\n        self.pos_emb = tf.math.divide(\n            self.positional_encoding(maxlen-1, num_hid),\n            tf.math.sqrt(tf.cast(num_hid, tf.float32)))\n        '''\n        self.pos_emb = self.positional_encoding(maxlen-1, num_hid)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        x = self.emb(x)\n        x = tf.math.multiply(x, tf.math.sqrt(tf.cast(self.num_hid, tf.float32)))\n        '''\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        return x + positions\n        '''\n        return x + self.pos_emb[:maxlen, :]\n    \n    def positional_encoding(self, maxlen, num_hid):\n        depth = num_hid/2\n        positions = tf.range(maxlen, dtype = tf.float32)[..., tf.newaxis]\n        depths = tf.range(depth, dtype = tf.float32)[np.newaxis, :]/depth\n        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n        angle_rads = tf.linalg.matmul(positions, angle_rates)\n        pos_encoding = tf.concat(\n          [tf.math.sin(angle_rads), tf.math.cos(angle_rads)],\n          axis=-1)\n        return pos_encoding\n\n\n","metadata":{}},{"cell_type":"code","source":"class LandmarkEmbedding(tf.keras.Model):\n    def __init__(self, num_hid=256, maxlen=100):\n        super(LandmarkEmbedding, self).__init__()\n        self.conv1 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn1 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu1 = tf.keras.layers.LeakyReLU()\n\n        self.conv2 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn2 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu2 = tf.keras.layers.LeakyReLU()\n        self.dropout2 = tf.keras.layers.Dropout(0.2)\n\n        self.conv3 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn3 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu3 = tf.keras.layers.LeakyReLU()\n        self.dropout3 = tf.keras.layers.Dropout(0.2)\n\n        self.conv4 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn4 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu4 = tf.keras.layers.LeakyReLU()\n        self.dropout4 = tf.keras.layers.Dropout(0.2)\n\n        self.sigmoid = tf.keras.layers.Activation('sigmoid')\n        self.pos_emb = self.positional_encoding(maxlen, num_hid)\n        self.maxlen = maxlen\n        self.num_hid = num_hid\n\n    def call(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.leaky_relu1(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.leaky_relu2(x)\n        x = self.dropout2(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.leaky_relu3(x)\n        x = self.dropout3(x)\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.leaky_relu4(x)\n        x = self.dropout4(x)\n        x = tf.math.multiply(x, tf.math.sqrt(tf.cast(self.num_hid, tf.float32)))\n        x = x + self.pos_emb\n\n        return self.sigmoid(x)\n\n    def positional_encoding(self, maxlen, num_hid):\n        depth = num_hid/2\n        positions = tf.range(maxlen, dtype=tf.float32)[..., tf.newaxis]\n        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth\n        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n        angle_rads = tf.linalg.matmul(positions, angle_rates)\n        pos_encoding = tf.concat([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], axis=-1)\n        return pos_encoding\n","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:30:28.481955Z","iopub.execute_input":"2023-07-11T04:30:28.482310Z","iopub.status.idle":"2023-07-11T04:30:28.501538Z","shell.execute_reply.started":"2023-07-11T04:30:28.482281Z","shell.execute_reply":"2023-07-11T04:30:28.500488Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class TransformerEncoder(keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:30:34.957557Z","iopub.execute_input":"2023-07-11T04:30:34.957933Z","iopub.status.idle":"2023-07-11T04:30:34.967044Z","shell.execute_reply.started":"2023-07-11T04:30:34.957905Z","shell.execute_reply":"2023-07-11T04:30:34.965631Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"#### Here I added the training flag to the TransformerDecoder's Dropout layers.","metadata":{}},{"cell_type":"code","source":"class TransformerDecoder(keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n        super().__init__()\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n        self.self_att = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.self_dropout = layers.Dropout(0.5)\n        self.enc_dropout = layers.Dropout(0.1)\n        self.ffn_dropout = layers.Dropout(0.1)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n\n    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n        \"\"\"Masks the upper half of the dot product matrix in self attention.\n\n        This prevents flow of information from future tokens to current token.\n        1's in the lower triangle, counting from the lower right corner.\n        \"\"\"\n        i = tf.range(n_dest)[:, None]\n        j = tf.range(n_src)\n        m = i >= j - n_src + n_dest\n        mask = tf.cast(m, dtype)\n        mask = tf.reshape(mask, [1, n_dest, n_src])\n        mult = tf.concat(\n            [batch_size[..., tf.newaxis], tf.constant([1, 1], dtype=tf.int32)], 0\n        )\n        return tf.tile(mask, mult)\n\n    def call(self, enc_out, target, training):\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n        target_att = self.self_att(target, target, attention_mask=causal_mask)\n        target_norm = self.layernorm1(target + self.self_dropout(target_att, training = training))\n        enc_out = self.enc_att(target_norm, enc_out)\n        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out, training = training) + target_norm)\n        ffn_out = self.ffn(enc_out_norm)\n        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out, training = training))\n        return ffn_out_norm","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:30:39.523835Z","iopub.execute_input":"2023-07-11T04:30:39.524190Z","iopub.status.idle":"2023-07-11T04:30:39.539207Z","shell.execute_reply.started":"2023-07-11T04:30:39.524162Z","shell.execute_reply":"2023-07-11T04:30:39.538224Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"#### Here I made the passing of the training flag explicit.","metadata":{}},{"cell_type":"code","source":"class Transformer(tf.keras.Model):\n    def __init__(\n        self,\n        num_hid=64,\n        num_head=2,\n        num_feed_forward=128,\n        source_maxlen=100,\n        target_maxlen=100,\n        num_layers_enc=4,\n        num_layers_dec=1,\n        num_classes=60,\n    ):\n        super().__init__()\n        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n        self.num_layers_enc = num_layers_enc\n        self.num_layers_dec = num_layers_dec\n        self.target_maxlen = target_maxlen\n        self.num_classes = num_classes\n\n        self.enc_input = LandmarkEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n        self.dec_input = TokenEmbedding(\n            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n        )\n\n        self.encoder = keras.Sequential(\n            [self.enc_input]\n            + [\n                TransformerEncoder(num_hid, num_head, num_feed_forward)\n                for _ in range(num_layers_enc)\n            ]\n        )\n\n        for i in range(num_layers_dec):\n            setattr(\n                self,\n                f\"dec_layer_{i}\",\n                TransformerDecoder(num_hid, num_head, num_feed_forward),\n            )\n\n        self.classifier = layers.Dense(num_classes)\n\n    def decode(self, enc_out, target, training):\n        y = self.dec_input(target)\n        for i in range(self.num_layers_dec):\n            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y, training)\n        return y\n\n    def call(self, inputs, training):\n        source = inputs[0]\n        target = inputs[1]\n        x = self.encoder(source, training)\n        y = self.decode(x, target, training)\n        return self.classifier(y)\n\n    @property\n    def metrics(self):\n        return [self.loss_metric]\n\n    def train_step(self, batch):\n        \"\"\"Processes one batch inside model.fit().\"\"\"\n        source = batch[0]\n        target = batch[1]\n\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        \n        dec_input = target[:, :-1]\n        dec_target = target[:, 1:]\n        with tf.GradientTape() as tape:\n            preds = self([source, dec_input])\n            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n            mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        self.loss_metric.update_state(loss)\n        return {\"loss\": self.loss_metric.result()}\n\n    def test_step(self, batch):        \n        source = batch[0]\n        target = batch[1]\n\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        \n        dec_input = target[:, :-1]\n        dec_target = target[:, 1:]\n        preds = self([source, dec_input])\n        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n        mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n        self.loss_metric.update_state(loss)\n        return {\"loss\": self.loss_metric.result()}\n\n    def generate(self, source, target_start_token_idx):\n        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n        bs = tf.shape(source)[0]\n        enc = self.encoder(source, training = False)\n        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n        dec_logits = []\n        for i in range(self.target_maxlen - 1):\n            dec_out = self.decode(enc, dec_input, training = False)\n            logits = self.classifier(dec_out)\n            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n            last_logit = logits[:, -1][..., tf.newaxis]\n            dec_logits.append(last_logit)\n            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n        return dec_input","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:30:44.756130Z","iopub.execute_input":"2023-07-11T04:30:44.756485Z","iopub.status.idle":"2023-07-11T04:30:44.779131Z","shell.execute_reply.started":"2023-07-11T04:30:44.756458Z","shell.execute_reply":"2023-07-11T04:30:44.778054Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# 正解率を計算するためのメトリクスを作成\ntrain_accuracy = tf.keras.metrics.CategoricalAccuracy()\nval_accuracy = tf.keras.metrics.CategoricalAccuracy()\n\n# 学習ループ内で正解率を更新するコールバックを定義\nclass AccuracyCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        train_acc = train_accuracy.result()\n        val_acc = val_accuracy.result()\n        print(f\"Epoch {epoch+1}: Train Accuracy = {train_acc}, Validation Accuracy = {val_acc}\")\n        # 正解率をリセット\n        train_accuracy.reset_states()\n        val_accuracy.reset_states()\n# val_lossが3回マイナスになった場合に学習を停止するコールバック\nclass EarlyStoppingCallback(tf.keras.callbacks.Callback):\n    def __init__(self, patience=7):\n        super(EarlyStoppingCallback, self).__init__()\n        self.patience = patience\n        self.min_val_loss = float('inf')\n        self.wait = 0\n\n    def on_epoch_end(self, epoch, logs=None):\n        val_loss = logs.get('val_loss')\n        if val_loss < self.min_val_loss:\n            self.min_val_loss = val_loss\n            self.wait = 0\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                self.model.stop_training = True\n                print(\"Training stopped due to early stopping.\")\n\nbatch = next(iter(val_dataset))\nidx_to_char = list(char_to_num.keys())\n\nmodel = Transformer(\n    num_hid=200,\n    num_head=4,\n    num_feed_forward=400,\n    source_maxlen = FRAME_LEN,\n    target_maxlen=64,\n    num_layers_enc=2,\n    num_layers_dec=2,\n    num_classes=62,\n)\n\n\n\nloss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1)\naccuracy_callback = AccuracyCallback()\noptimizer = keras.optimizers.Adam(0.0001)\n\n\n# モデルのコンパイル\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=[train_accuracy])","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:54:00.771415Z","iopub.execute_input":"2023-07-11T04:54:00.771803Z","iopub.status.idle":"2023-07-11T04:54:01.165405Z","shell.execute_reply.started":"2023-07-11T04:54:00.771769Z","shell.execute_reply":"2023-07-11T04:54:01.164451Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"#modelアーキテクト\n#tf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True, show_layer_names=True, expand_nested=True, show_layer_activations=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:22:40.870153Z","iopub.status.idle":"2023-07-11T04:22:40.871145Z","shell.execute_reply.started":"2023-07-11T04:22:40.870904Z","shell.execute_reply":"2023-07-11T04:22:40.870926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nhistory = model.fit(train_dataset, verbose=2, validation_data=val_dataset, epochs=1)\n# EarlyStoppingCallbackをコールバックリストに追加して学習を行う\n# history = model.fit(train_dataset, verbose=2, validation_data=val_dataset, epochs=1,\n#                     callbacks=[AccuracyCallback(), EarlyStoppingCallback()])\n","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:54:05.697953Z","iopub.execute_input":"2023-07-11T04:54:05.698347Z","iopub.status.idle":"2023-07-11T04:57:48.972785Z","shell.execute_reply.started":"2023-07-11T04:54:05.698316Z","shell.execute_reply":"2023-07-11T04:57:48.971649Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"1520/1520 - 223s - loss: 0.7723 - val_loss: 0.6236 - 223s/epoch - 147ms/step\nCPU times: user 4min 44s, sys: 16.8 s, total: 5min 1s\nWall time: 3min 43s\n","output_type":"stream"}]},{"cell_type":"code","source":"print(model.summary())","metadata":{"execution":{"iopub.status.busy":"2023-07-11T05:21:27.996318Z","iopub.execute_input":"2023-07-11T05:21:27.996803Z","iopub.status.idle":"2023-07-11T05:21:28.051604Z","shell.execute_reply.started":"2023-07-11T05:21:27.996765Z","shell.execute_reply":"2023-07-11T05:21:28.050613Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Model: \"transformer_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n landmark_embedding_5 (Landm  (None, 128, 200)         1495600   \n arkEmbedding)                                                   \n                                                                 \n token_embedding_5 (TokenEmb  multiple                 253600    \n edding)                                                         \n                                                                 \n sequential_33 (Sequential)  (None, 128, 200)          3103600   \n                                                                 \n transformer_decoder_8 (Tran  multiple                 1447000   \n sformerDecoder)                                                 \n                                                                 \n transformer_decoder_9 (Tran  multiple                 1447000   \n sformerDecoder)                                                 \n                                                                 \n dense_89 (Dense)            multiple                  12462     \n                                                                 \n=================================================================\nTotal params: 6,263,664\nTrainable params: 6,262,062\nNon-trainable params: 1,602\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])","metadata":{"execution":{"iopub.status.busy":"2023-07-11T05:21:37.671122Z","iopub.execute_input":"2023-07-11T05:21:37.671501Z","iopub.status.idle":"2023-07-11T05:21:37.933192Z","shell.execute_reply.started":"2023-07-11T05:21:37.671472Z","shell.execute_reply":"2023-07-11T05:21:37.932215Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"[<matplotlib.lines.Line2D at 0x79220c22a200>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAopklEQVR4nO3df1BUV57//1eL0jijtCIRCSJxzA8xxKoVSiIOk90thkhmHdnMBIwJjkZrh8Rx/bFmRwuzRkzEmKwbqjKQqLBuqhx1119r7aCVzg+NBmesuLprDUbXQQNqEwZMuk10QOF8/vBrf9M2GJoRPU2ej6r7R5/7PtdzTlnpV869fXUYY4wAAAAs1udODwAAAOCbEFgAAID1CCwAAMB6BBYAAGA9AgsAALAegQUAAFiPwAIAAKxHYAEAANbre6cHcKu0t7fr/PnzGjhwoBwOx50eDgAA6AJjjC5evKi7775bffp0vo/SawLL+fPnlZiYeKeHAQAAuqG+vl7Dhw/v9HyvCSwDBw6UdG3C0dHRd3g0AACgK3w+nxITE/3f453pNYHl+m2g6OhoAgsAAGHmmx7n4KFbAABgPQILAACwHoEFAABYj8ACAACsR2ABAADWI7AAAADrEVgAAID1CCwAAMB6BBYAAGA9AgsAALAegQUAAFiPwAIAAKxHYAEAANYjsAAAAOsRWAAAgPUILAAAwHoEFgAAYD0CCwAAsB6BBQAAWI/AAgAArEdgAQAA1iOwAAAA6xFYAACA9QgsAADAegQWAABgPQILAACwHoEFAABYj8ACAACsR2ABAADWI7AAAADrEVgAAID1CCwAAMB6BBYAAGA9AgsAALAegQUAAFiPwAIAAKxHYAEAANbrVmApKyvTyJEjFRUVpdTUVO3fv7/T2hkzZsjhcAQdDz74YEDdF198oTlz5ig+Pl5RUVFKTk5WVVVVd4YHAAB6mZADy5YtWzR//nwVFRXpyJEjyszMVE5Ojurq6jqsLy0tlcfj8R/19fWKiYnRE0884a9pbW3VD3/4Q505c0Zbt27ViRMntG7dOiUkJHR/ZgAAoNdwGGNMKB3S09M1btw4lZeX+9uSk5OVm5urkpKSb+y/c+dOPf744zp9+rSSkpIkSW+++aZeffVVffLJJ+rXr1+IU7jG5/PJ5XLJ6/UqOjq6W9cAAAC3V1e/v0PaYWltbdXhw4eVnZ0d0J6dna3q6uouXaOiokJZWVn+sCJJu3bt0oQJEzRnzhzFxcUpJSVFK1euVFtbW6fXaWlpkc/nCzgAAEDvFFJgaWpqUltbm+Li4gLa4+Li1NDQ8I39PR6Pdu/erdmzZwe019bWauvWrWpra1NVVZWWLl2qf/7nf9bLL7/c6bVKSkrkcrn8R2JiYihTAQAAYaRbD906HI6Az8aYoLaObNiwQYMGDVJubm5Ae3t7u4YOHaq1a9cqNTVVU6dOVVFRUcBtpxstWbJEXq/Xf9TX13dnKgAAIAz0DaU4NjZWERERQbspjY2NQbsuNzLGqLKyUgUFBYqMjAw4Fx8fr379+ikiIsLflpycrIaGBrW2tgbVS5LT6ZTT6Qxl+AAAIEyFtMMSGRmp1NRUud3ugHa3262MjIyb9t23b59OnTqlWbNmBZ2bOHGiTp06pfb2dn/byZMnFR8f32FYAQAA3y4h3xJauHCh1q9fr8rKSh0/flwLFixQXV2dCgsLJV27VTN9+vSgfhUVFUpPT1dKSkrQuWeffVbNzc2aN2+eTp48qd/85jdauXKl5syZ040pAQCA3iakW0KSlJ+fr+bmZhUXF8vj8SglJUVVVVX+X/14PJ6gd7J4vV5t27ZNpaWlHV4zMTFR77zzjhYsWKCxY8cqISFB8+bN0y9/+ctuTAkAAPQ2Ib+HxVa8hwUAgPDTI+9hAQAAuBMILAAAwHoEFgAAYD0CCwAAsB6BBQAAWI/AAgAArEdgAQAA1iOwAAAA6xFYAACA9QgsAADAegQWAABgPQILAACwHoEFAABYj8ACAACsR2ABAADWI7AAAADrEVgAAID1CCwAAMB6BBYAAGA9AgsAALAegQUAAFiPwAIAAKxHYAEAANYjsAAAAOsRWAAAgPUILAAAwHoEFgAAYD0CCwAAsB6BBQAAWI/AAgAArEdgAQAA1iOwAAAA6xFYAACA9QgsAADAet0KLGVlZRo5cqSioqKUmpqq/fv3d1o7Y8YMORyOoOPBBx/ssH7z5s1yOBzKzc3tztAAAEAvFHJg2bJli+bPn6+ioiIdOXJEmZmZysnJUV1dXYf1paWl8ng8/qO+vl4xMTF64okngmo//fRTLVq0SJmZmaHPBAAA9FohB5Y1a9Zo1qxZmj17tpKTk/X6668rMTFR5eXlHda7XC4NGzbMf3z88cf6/PPPNXPmzIC6trY2PfXUU1q+fLm+973vdW82AACgVwopsLS2turw4cPKzs4OaM/OzlZ1dXWXrlFRUaGsrCwlJSUFtBcXF+uuu+7SrFmzQhkSAAD4FugbSnFTU5Pa2toUFxcX0B4XF6eGhoZv7O/xeLR79279+te/Dmj/6KOPVFFRoaNHj3Z5LC0tLWppafF/9vl8Xe4LAADCS7ceunU4HAGfjTFBbR3ZsGGDBg0aFPBA7cWLF/X0009r3bp1io2N7fIYSkpK5HK5/EdiYmKX+wIAgPAS0g5LbGysIiIignZTGhsbg3ZdbmSMUWVlpQoKChQZGelv/8Mf/qAzZ85o8uTJ/rb29vZrg+vbVydOnNCoUaOCrrdkyRItXLjQ/9nn8xFaAADopUIKLJGRkUpNTZXb7dbf/u3f+tvdbremTJly07779u3TqVOngp5RGT16tI4dOxbQtnTpUl28eFGlpaWdhhCn0ymn0xnK8AEAQJgKKbBI0sKFC1VQUKC0tDRNmDBBa9euVV1dnQoLCyVd2/k4d+6c3n777YB+FRUVSk9PV0pKSkB7VFRUUNugQYMkKagdAAB8O4UcWPLz89Xc3Kzi4mJ5PB6lpKSoqqrK/6sfj8cT9E4Wr9erbdu2qbS09NaMGgAAfKs4jDHmTg/iVvD5fHK5XPJ6vYqOjr7TwwEAAF3Q1e9v/i0hAABgPQILAACwHoEFAABYj8ACAACsR2ABAADWI7AAAADrEVgAAID1CCwAAMB6BBYAAGA9AgsAALAegQUAAFiPwAIAAKxHYAEAANYjsAAAAOsRWAAAgPUILAAAwHoEFgAAYD0CCwAAsB6BBQAAWI/AAgAArEdgAQAA1iOwAAAA6xFYAACA9QgsAADAegQWAABgPQILAACwHoEFAABYj8ACAACsR2ABAADWI7AAAADrEVgAAID1CCwAAMB6BBYAAGA9AgsAALAegQUAAFivW4GlrKxMI0eOVFRUlFJTU7V///5Oa2fMmCGHwxF0PPjgg/6adevWKTMzU4MHD9bgwYOVlZWlQ4cOdWdoAACgFwo5sGzZskXz589XUVGRjhw5oszMTOXk5Kiurq7D+tLSUnk8Hv9RX1+vmJgYPfHEE/6avXv36sknn9QHH3yggwcPasSIEcrOzta5c+e6PzMAANBrOIwxJpQO6enpGjdunMrLy/1tycnJys3NVUlJyTf237lzpx5//HGdPn1aSUlJHda0tbVp8ODBeuONNzR9+vQujcvn88nlcsnr9So6OrprkwEAAHdUV7+/Q9phaW1t1eHDh5WdnR3Qnp2drerq6i5do6KiQllZWZ2GFUm6dOmSrly5opiYmE5rWlpa5PP5Ag4AANA7hRRYmpqa1NbWpri4uID2uLg4NTQ0fGN/j8ej3bt3a/bs2TetW7x4sRISEpSVldVpTUlJiVwul/9ITEzs2iQAAEDY6dZDtw6HI+CzMSaorSMbNmzQoEGDlJub22nN6tWrtWnTJm3fvl1RUVGd1i1ZskRer9d/1NfXd3n8AAAgvPQNpTg2NlYRERFBuymNjY1Buy43MsaosrJSBQUFioyM7LDmtdde08qVK/Xuu+9q7NixN72e0+mU0+kMZfgAACBMhbTDEhkZqdTUVLnd7oB2t9utjIyMm/bdt2+fTp06pVmzZnV4/tVXX9WKFSu0Z88epaWlhTIsAADQy4W0wyJJCxcuVEFBgdLS0jRhwgStXbtWdXV1KiwslHTtVs25c+f09ttvB/SrqKhQenq6UlJSgq65evVqvfDCC/r1r3+te+65x7+DM2DAAA0YMKA78wIAAL1IyIElPz9fzc3NKi4ulsfjUUpKiqqqqvy/+vF4PEHvZPF6vdq2bZtKS0s7vGZZWZlaW1v105/+NKB92bJlevHFF0MdIgAA6GVCfg+LrXgPCwAA4adH3sMCAABwJxBYAACA9QgsAADAegQWAABgPQILAACwHoEFAABYj8ACAACsR2ABAADWI7AAAADrEVgAAID1CCwAAMB6BBYAAGA9AgsAALAegQUAAFiPwAIAAKxHYAEAANYjsAAAAOsRWAAAgPUILAAAwHoEFgAAYD0CCwAAsB6BBQAAWI/AAgAArEdgAQAA1iOwAAAA6xFYAACA9QgsAADAegQWAABgPQILAACwHoEFAABYj8ACAACsR2ABAADWI7AAAADrEVgAAID1uhVYysrKNHLkSEVFRSk1NVX79+/vtHbGjBlyOBxBx4MPPhhQt23bNo0ZM0ZOp1NjxozRjh07ujM0AADQC4UcWLZs2aL58+erqKhIR44cUWZmpnJyclRXV9dhfWlpqTwej/+or69XTEyMnnjiCX/NwYMHlZ+fr4KCAv3P//yPCgoKlJeXp9/97nfdnxkAAOg1HMYYE0qH9PR0jRs3TuXl5f625ORk5ebmqqSk5Bv779y5U48//rhOnz6tpKQkSVJ+fr58Pp92797tr5s0aZIGDx6sTZs2dWlcPp9PLpdLXq9X0dHRoUwJAADcIV39/g5ph6W1tVWHDx9WdnZ2QHt2draqq6u7dI2KigplZWX5w4p0bYflxms++uijN71mS0uLfD5fwAEAAHqnkAJLU1OT2traFBcXF9AeFxenhoaGb+zv8Xi0e/duzZ49O6C9oaEh5GuWlJTI5XL5j8TExBBmAgAAwkm3Hrp1OBwBn40xQW0d2bBhgwYNGqTc3Nw/+5pLliyR1+v1H/X19V0bPAAACDt9QymOjY1VRERE0M5HY2Nj0A7JjYwxqqysVEFBgSIjIwPODRs2LORrOp1OOZ3OUIYPAADCVEg7LJGRkUpNTZXb7Q5od7vdysjIuGnfffv26dSpU5o1a1bQuQkTJgRd85133vnGawIAgG+HkHZYJGnhwoUqKChQWlqaJkyYoLVr16qurk6FhYWSrt2qOXfunN5+++2AfhUVFUpPT1dKSkrQNefNm6cf/OAHeuWVVzRlyhT953/+p959910dOHCgm9MCAAC9SciBJT8/X83NzSouLpbH41FKSoqqqqr8v/rxeDxB72Txer3atm2bSktLO7xmRkaGNm/erKVLl+qFF17QqFGjtGXLFqWnp3djSgAAoLcJ+T0stuI9LAAAhJ8eeQ8LAADAnUBgAQAA1iOwAAAA6xFYAACA9QgsAADAegQWAABgPQILAACwHoEFAABYj8ACAACsR2ABAADWI7AAAADrEVgAAID1CCwAAMB6BBYAAGA9AgsAALAegQUAAFiPwAIAAKxHYAEAANYjsAAAAOsRWAAAgPUILAAAwHoEFgAAYD0CCwAAsB6BBQAAWI/AAgAArEdgAQAA1iOwAAAA6xFYAACA9QgsAADAegQWAABgPQILAACwHoEFAABYj8ACAACsR2ABAADW61ZgKSsr08iRIxUVFaXU1FTt37//pvUtLS0qKipSUlKSnE6nRo0apcrKyoCa119/XQ888ID69++vxMRELViwQH/605+6MzwAANDL9A21w5YtWzR//nyVlZVp4sSJeuutt5STk6OamhqNGDGiwz55eXn67LPPVFFRoXvvvVeNjY26evWq//zGjRu1ePFiVVZWKiMjQydPntSMGTMkSf/yL//SvZkBAIBew2GMMaF0SE9P17hx41ReXu5vS05OVm5urkpKSoLq9+zZo6lTp6q2tlYxMTEdXvMXv/iFjh8/rvfee8/f9g//8A86dOjQN+7eXOfz+eRyueT1ehUdHR3KlAAAwB3S1e/vkG4Jtba26vDhw8rOzg5oz87OVnV1dYd9du3apbS0NK1evVoJCQm6//77tWjRIl2+fNlf8/3vf1+HDx/WoUOHJEm1tbWqqqrSj370o07H0tLSIp/PF3AAAIDeKaRbQk1NTWpra1NcXFxAe1xcnBoaGjrsU1tbqwMHDigqKko7duxQU1OTnnvuOV24cMH/HMvUqVP1xz/+Ud///vdljNHVq1f17LPPavHixZ2OpaSkRMuXLw9l+AAAIEx166Fbh8MR8NkYE9R2XXt7uxwOhzZu3Kjx48frscce05o1a7Rhwwb/LsvevXv18ssvq6ysTP/93/+t7du367/+67+0YsWKTsewZMkSeb1e/1FfX9+dqQAAgDAQ0g5LbGysIiIignZTGhsbg3ZdrouPj1dCQoJcLpe/LTk5WcYYnT17Vvfdd59eeOEFFRQUaPbs2ZKkhx56SF999ZX+7u/+TkVFRerTJzhXOZ1OOZ3OUIYPAADCVEg7LJGRkUpNTZXb7Q5od7vdysjI6LDPxIkTdf78eX355Zf+tpMnT6pPnz4aPny4JOnSpUtBoSQiIkLGGIX4TDAAAOiFQr4ltHDhQq1fv16VlZU6fvy4FixYoLq6OhUWFkq6dqtm+vTp/vpp06ZpyJAhmjlzpmpqavThhx/q+eef1zPPPKP+/ftLkiZPnqzy8nJt3rxZp0+fltvt1gsvvKAf//jHioiIuEVTBQAA4Srk97Dk5+erublZxcXF8ng8SklJUVVVlZKSkiRJHo9HdXV1/voBAwbI7XZr7ty5SktL05AhQ5SXl6eXXnrJX7N06VI5HA4tXbpU586d01133aXJkyfr5ZdfvgVTBAAA4S7k97DYivewAAAQfnrkPSwAAAB3AoEFAABYj8ACAACsR2ABAADWI7AAAADrEVgAAID1CCwAAMB6BBYAAGA9AgsAALAegQUAAFiPwAIAAKxHYAEAANYjsAAAAOsRWAAAgPUILAAAwHoEFgAAYD0CCwAAsB6BBQAAWI/AAgAArEdgAQAA1iOwAAAA6xFYAACA9QgsAADAegQWAABgPQILAACwHoEFAABYj8ACAACsR2ABAADWI7AAAADrEVgAAID1CCwAAMB6BBYAAGA9AgsAALAegQUAAFivW4GlrKxMI0eOVFRUlFJTU7V///6b1re0tKioqEhJSUlyOp0aNWqUKisrA2q++OILzZkzR/Hx8YqKilJycrKqqqq6MzwAANDL9A21w5YtWzR//nyVlZVp4sSJeuutt5STk6OamhqNGDGiwz55eXn67LPPVFFRoXvvvVeNjY26evWq/3xra6t++MMfaujQodq6dauGDx+u+vp6DRw4sPszAwAAvYbDGGNC6ZCenq5x48apvLzc35acnKzc3FyVlJQE1e/Zs0dTp05VbW2tYmJiOrzmm2++qVdffVWffPKJ+vXrF+IUrvH5fHK5XPJ6vYqOju7WNQAAwO3V1e/vkG4Jtba26vDhw8rOzg5oz87OVnV1dYd9du3apbS0NK1evVoJCQm6//77tWjRIl2+fDmgZsKECZozZ47i4uKUkpKilStXqq2trdOxtLS0yOfzBRwAAKB3CumWUFNTk9ra2hQXFxfQHhcXp4aGhg771NbW6sCBA4qKitKOHTvU1NSk5557ThcuXPA/x1JbW6v3339fTz31lKqqqvR///d/mjNnjq5evap/+qd/6vC6JSUlWr58eSjDBwAAYapbD906HI6Az8aYoLbr2tvb5XA4tHHjRo0fP16PPfaY1qxZow0bNvh3Wdrb2zV06FCtXbtWqampmjp1qoqKigJuO91oyZIl8nq9/qO+vr47UwEAAGEgpB2W2NhYRUREBO2mNDY2Bu26XBcfH6+EhAS5XC5/W3JysowxOnv2rO677z7Fx8erX79+ioiICKhpaGhQa2urIiMjg67rdDrldDpDGT4AAAhTIe2wREZGKjU1VW63O6Dd7XYrIyOjwz4TJ07U+fPn9eWXX/rbTp48qT59+mj48OH+mlOnTqm9vT2gJj4+vsOwAgAAvl1CviW0cOFCrV+/XpWVlTp+/LgWLFiguro6FRYWSrp2q2b69On++mnTpmnIkCGaOXOmampq9OGHH+r555/XM888o/79+0uSnn32WTU3N2vevHk6efKkfvOb32jlypWaM2fOLZomAAAIZyG/hyU/P1/Nzc0qLi6Wx+NRSkqKqqqqlJSUJEnyeDyqq6vz1w8YMEBut1tz585VWlqahgwZory8PL300kv+msTERL3zzjtasGCBxo4dq4SEBM2bN0+//OUvb8EUAQBAuAv5PSy24j0sAACEnx55DwsAAMCdQGABAADWI7AAAADrEVgAAID1CCwAAMB6BBYAAGA9AgsAALAegQUAAFiPwAIAAKxHYAEAANYjsAAAAOsRWAAAgPUILAAAwHoEFgAAYD0CCwAAsB6BBQAAWI/AAgAArEdgAQAA1iOwAAAA6xFYAACA9QgsAADAegQWAABgPQILAACwHoEFAABYj8ACAACsR2ABAADWI7AAAADrEVgAAID1CCwAAMB6BBYAAGA9AgsAALAegQUAAFiPwAIAAKxHYAEAANbrVmApKyvTyJEjFRUVpdTUVO3fv/+m9S0tLSoqKlJSUpKcTqdGjRqlysrKDms3b94sh8Oh3Nzc7gwNAAD0Qn1D7bBlyxbNnz9fZWVlmjhxot566y3l5OSopqZGI0aM6LBPXl6ePvvsM1VUVOjee+9VY2Ojrl69GlT36aefatGiRcrMzAx9JgAAoNdyGGNMKB3S09M1btw4lZeX+9uSk5OVm5urkpKSoPo9e/Zo6tSpqq2tVUxMTKfXbWtr0yOPPKKZM2dq//79+uKLL7Rz584uj8vn88nlcsnr9So6OjqUKQEAgDukq9/fId0Sam1t1eHDh5WdnR3Qnp2drerq6g777Nq1S2lpaVq9erUSEhJ0//33a9GiRbp8+XJAXXFxse666y7NmjWrS2NpaWmRz+cLOAAAQO8U0i2hpqYmtbW1KS4uLqA9Li5ODQ0NHfapra3VgQMHFBUVpR07dqipqUnPPfecLly44H+O5aOPPlJFRYWOHj3a5bGUlJRo+fLloQwfAACEqW49dOtwOAI+G2OC2q5rb2+Xw+HQxo0bNX78eD322GNas2aNNmzYoMuXL+vixYt6+umntW7dOsXGxnZ5DEuWLJHX6/Uf9fX13ZkKAAAIAyHtsMTGxioiIiJoN6WxsTFo1+W6+Ph4JSQkyOVy+duSk5NljNHZs2f11Vdf6cyZM5o8ebL/fHt7+7XB9e2rEydOaNSoUUHXdTqdcjqdoQwfAACEqZB2WCIjI5Wamiq32x3Q7na7lZGR0WGfiRMn6vz58/ryyy/9bSdPnlSfPn00fPhwjR49WseOHdPRo0f9x49//GP91V/9lY4eParExMRuTAsAAPQmIf+seeHChSooKFBaWpomTJigtWvXqq6uToWFhZKu3ao5d+6c3n77bUnStGnTtGLFCs2cOVPLly9XU1OTnn/+eT3zzDPq37+/JCklJSXgzxg0aFCH7QAA4Nsp5MCSn5+v5uZmFRcXy+PxKCUlRVVVVUpKSpIkeTwe1dXV+esHDBggt9utuXPnKi0tTUOGDFFeXp5eeumlWzcLAADQq4X8HhZb8R4WAADCT4+8hwUAAOBOILAAAADrEVgAAID1CCwAAMB6BBYAAGA9AgsAALAegQUAAFiPwAIAAKxHYAEAANYjsAAAAOsRWAAAgPUILAAAwHoEFgAAYD0CCwAAsB6BBQAAWI/AAgAArEdgAQAA1iOwAAAA6xFYAACA9QgsAADAegQWAABgPQILAACwHoEFAABYj8ACAACsR2ABAADWI7AAAADrEVgAAID1CCwAAMB6BBYAAGA9AgsAALAegQUAAFiPwAIAAKxHYAEAANYjsAAAAOt1K7CUlZVp5MiRioqKUmpqqvbv33/T+paWFhUVFSkpKUlOp1OjRo1SZWWl//y6deuUmZmpwYMHa/DgwcrKytKhQ4e6MzQAANALhRxYtmzZovnz56uoqEhHjhxRZmamcnJyVFdX12mfvLw8vffee6qoqNCJEye0adMmjR492n9+7969evLJJ/XBBx/o4MGDGjFihLKzs3Xu3LnuzQoAAPQqDmOMCaVDenq6xo0bp/Lycn9bcnKycnNzVVJSElS/Z88eTZ06VbW1tYqJienSn9HW1qbBgwfrjTfe0PTp07vUx+fzyeVyyev1Kjo6umuTAQAAd1RXv79D2mFpbW3V4cOHlZ2dHdCenZ2t6urqDvvs2rVLaWlpWr16tRISEnT//fdr0aJFunz5cqd/zqVLl3TlypUuBxwAANC79Q2luKmpSW1tbYqLiwtoj4uLU0NDQ4d9amtrdeDAAUVFRWnHjh1qamrSc889pwsXLgQ8x/J1ixcvVkJCgrKysjodS0tLi1paWvyffT5fKFMBAABhpFsP3TocjoDPxpigtuva29vlcDi0ceNGjR8/Xo899pjWrFmjDRs2dLjLsnr1am3atEnbt29XVFRUp2MoKSmRy+XyH4mJid2ZCgAACAMhBZbY2FhFREQE7aY0NjYG7bpcFx8fr4SEBLlcLn9bcnKyjDE6e/ZsQO1rr72mlStX6p133tHYsWNvOpYlS5bI6/X6j/r6+lCmAgAAwkhIgSUyMlKpqalyu90B7W63WxkZGR32mThxos6fP68vv/zS33by5En16dNHw4cP97e9+uqrWrFihfbs2aO0tLRvHIvT6VR0dHTAAQAAeqeQbwktXLhQ69evV2VlpY4fP64FCxaorq5OhYWFkq7tfHz9lz3Tpk3TkCFDNHPmTNXU1OjDDz/U888/r2eeeUb9+/eXdO020NKlS1VZWal77rlHDQ0NamhoCAg5AADg2yukh24lKT8/X83NzSouLpbH41FKSoqqqqqUlJQkSfJ4PAHvZBkwYIDcbrfmzp2rtLQ0DRkyRHl5eXrppZf8NWVlZWptbdVPf/rTgD9r2bJlevHFF7s5NQAA0FuE/B4WW/EeFgAAwk+PvIcFAADgTgj5lpCtrm8U8T4WAADCx/Xv7W+64dNrAsvFixclifexAAAQhi5evBjwCpQb9ZpnWNrb23X+/HkNHDiw05fYfVv4fD4lJiaqvr6e53l6GGt9e7DOtwfrfHuwzoGMMbp48aLuvvtu9enT+ZMqvWaH5cb3ukC8n+Y2Yq1vD9b59mCdbw/W+f93s52V63joFgAAWI/AAgAArEdg6YWcTqeWLVsmp9N5p4fS67HWtwfrfHuwzrcH69w9veahWwAA0HuxwwIAAKxHYAEAANYjsAAAAOsRWAAAgPUILGHq888/V0FBgVwul1wulwoKCvTFF1/ctI8xRi+++KLuvvtu9e/fX3/5l3+p3//+953W5uTkyOFwaOfOnbd+AmGiJ9b5woULmjt3rh544AF95zvf0YgRI/T3f//38nq9PTwbe5SVlWnkyJGKiopSamqq9u/ff9P6ffv2KTU1VVFRUfre976nN998M6hm27ZtGjNmjJxOp8aMGaMdO3b01PDDxq1e53Xr1ikzM1ODBw/W4MGDlZWVpUOHDvXkFMJCT/x9vm7z5s1yOBzKzc29xaMOQwZhadKkSSYlJcVUV1eb6upqk5KSYv7mb/7mpn1WrVplBg4caLZt22aOHTtm8vPzTXx8vPH5fEG1a9asMTk5OUaS2bFjRw/Nwn49sc7Hjh0zjz/+uNm1a5c5deqUee+998x9991nfvKTn9yOKd1xmzdvNv369TPr1q0zNTU1Zt68eea73/2u+fTTTzusr62tNd/5znfMvHnzTE1NjVm3bp3p16+f2bp1q7+murraREREmJUrV5rjx4+blStXmr59+5rf/va3t2ta1umJdZ42bZr51a9+ZY4cOWKOHz9uZs6caVwulzl79uztmpZ1emKdrztz5oxJSEgwmZmZZsqUKT08E/sRWMJQTU2NkRTwH+ODBw8aSeaTTz7psE97e7sZNmyYWbVqlb/tT3/6k3G5XObNN98MqD169KgZPny48Xg83+rA0tPr/HX//u//biIjI82VK1du3QQsNX78eFNYWBjQNnr0aLN48eIO6//xH//RjB49OqDt5z//uXn44Yf9n/Py8sykSZMCah599FEzderUWzTq8NMT63yjq1evmoEDB5p/+7d/+/MHHKZ6ap2vXr1qJk6caNavX29+9rOfEViMMdwSCkMHDx6Uy+VSenq6v+3hhx+Wy+VSdXV1h31Onz6thoYGZWdn+9ucTqceeeSRgD6XLl3Sk08+qTfeeEPDhg3ruUmEgZ5c5xt5vV5FR0erb99e8897dai1tVWHDx8OWB9Jys7O7nR9Dh48GFT/6KOP6uOPP9aVK1duWnOzNe/Nemqdb3Tp0iVduXJFMTExt2bgYaYn17m4uFh33XWXZs2adesHHqYILGGooaFBQ4cODWofOnSoGhoaOu0jSXFxcQHtcXFxAX0WLFigjIwMTZky5RaOODz15Dp/XXNzs1asWKGf//znf+aI7dfU1KS2traQ1qehoaHD+qtXr6qpqemmNZ1ds7frqXW+0eLFi5WQkKCsrKxbM/Aw01Pr/NFHH6miokLr1q3rmYGHKQKLRV588UU5HI6bHh9//LEkyeFwBPU3xnTY/nU3nv96n127dun999/X66+/fmsmZKk7vc5f5/P59KMf/UhjxozRsmXL/oxZhZeurs/N6m9sD/Wa3wY9sc7XrV69Wps2bdL27dsVFRV1C0Ybvm7lOl+8eFFPP/201q1bp9jY2Fs/2DDWu/efw8wvfvELTZ069aY199xzj/73f/9Xn332WdC5P/7xj0HJ/brrt3caGhoUHx/vb29sbPT3ef/99/WHP/xBgwYNCuj7k5/8RJmZmdq7d28Is7HXnV7n6y5evKhJkyZpwIAB2rFjh/r16xfqVMJObGysIiIigv7vs6P1uW7YsGEd1vft21dDhgy5aU1n1+ztemqdr3vttde0cuVKvfvuuxo7duytHXwY6Yl1/v3vf68zZ85o8uTJ/vPt7e2SpL59++rEiRMaNWrULZ5JmLhDz87gz3D9YdDf/e53/rbf/va3XXoY9JVXXvG3tbS0BDwM6vF4zLFjxwIOSaa0tNTU1tb27KQs1FPrbIwxXq/XPPzww+aRRx4xX331Vc9NwkLjx483zz77bEBbcnLyTR9STE5ODmgrLCwMeug2JycnoGbSpEnf+odub/U6G2PM6tWrTXR0tDl48OCtHXCYutXrfPny5aD/Dk+ZMsX89V//tTl27JhpaWnpmYmEAQJLmJo0aZIZO3asOXjwoDl48KB56KGHgn5u+8ADD5jt27f7P69atcq4XC6zfft2c+zYMfPkk092+rPm6/Qt/pWQMT2zzj6fz6Snp5uHHnrInDp1yng8Hv9x9erV2zq/O+H6z0ArKipMTU2NmT9/vvnud79rzpw5Y4wxZvHixaagoMBff/1noAsWLDA1NTWmoqIi6GegH330kYmIiDCrVq0yx48fN6tWreJnzT2wzq+88oqJjIw0W7duDfh7e/Hixds+P1v0xDrfiF8JXUNgCVPNzc3mqaeeMgMHDjQDBw40Tz31lPn8888DaiSZf/3Xf/V/bm9vN8uWLTPDhg0zTqfT/OAHPzDHjh276Z/zbQ8sPbHOH3zwgZHU4XH69OnbM7E77Fe/+pVJSkoykZGRZty4cWbfvn3+cz/72c/MI488ElC/d+9e8xd/8RcmMjLS3HPPPaa8vDzomv/xH/9hHnjgAdOvXz8zevRos23btp6ehvVu9TonJSV1+Pd22bJlt2E29uqJv89fR2C5xmHM//e0DwAAgKX4lRAAALAegQUAAFiPwAIAAKxHYAEAANYjsAAAAOsRWAAAgPUILAAAwHoEFgAAYD0CCwAAsB6BBQAAWI/AAgAArEdgAQAA1vt//78rJURN+sUAAAAASUVORK5CYII="},"metadata":{}}]},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"code","source":"batches = [batch for batch in val_dataset]\n\npreds_list = []\nground_truth_list = []\n\nfor batch in batches[:1]:\n    source = batch[0]\n    target = batch[1].numpy()\n    bs = tf.shape(source)[0]\n    preds = model.generate(source, start_token_idx)\n    preds = preds.numpy()\n\n    for i in range(bs):\n        target_text = \"\".join([idx_to_char[_] for _ in target[i, :]])\n        ground_truth_list.append(target_text.replace('P', ''))\n        prediction = \"\"\n        for idx in preds[i, :]:\n            prediction += idx_to_char[idx]\n            if idx == end_token_idx:\n                break\n        preds_list.append(prediction)\n\nfor i in range(10):\n    print(ground_truth_list[i])\n    print(preds_list[i])\n    print('\\n~~~\\n')","metadata":{"execution":{"iopub.status.busy":"2023-07-11T05:21:40.719163Z","iopub.execute_input":"2023-07-11T05:21:40.719527Z","iopub.status.idle":"2023-07-11T05:21:50.132681Z","shell.execute_reply.started":"2023-07-11T05:21:40.719498Z","shell.execute_reply":"2023-07-11T05:21:50.131374Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"S3 creekhouseE\nS323 crerse raeeE\n\n~~~\n\nSscales/kuhaylahE\nSslales.com/paiaE\n\n~~~\n\nS1383 william lanierE\nS1386 willlanisE\n\n~~~\n\nS988 franklin laneE\nS9898 fanth staneE\n\n~~~\n\nS6920 northeast 661st roadE\nS6929 ortheast wastroadE\n\n~~~\n\nSwww.freem.ne.jpE\nSwww.frememes.mE\n\n~~~\n\nShttps://jsi.is/hukuokaE\nShttps://pisisisis.uromE\n\n~~~\n\nS239613 stolze streetE\nS2961 nattolest lestE\n\n~~~\n\nS271097 bayshore boulevardE\nS2710 biatobord bordE\n\n~~~\n\nSfederico pearsonE\nS9010 couronE\n\n~~~\n\n","output_type":"stream"}]},{"cell_type":"code","source":"ground_truth_processed = [ground_truth_list[i][1:-1] for i in range(len(ground_truth_list))]\npreds_list_processed = [preds_list[i][1:-1] for i in range(len(preds_list))]\nlev_dist = [lev.distance(ground_truth_processed[i], preds_list_processed[i])\n            for i in range(len(preds_list_processed))]\nN = [len(phrase) for phrase in ground_truth_processed]\n\nprint('Validation score: '+str((np.sum(N) - np.sum(lev_dist))/np.sum(N)))","metadata":{"execution":{"iopub.status.busy":"2023-07-11T05:22:01.267074Z","iopub.execute_input":"2023-07-11T05:22:01.267457Z","iopub.status.idle":"2023-07-11T05:22:01.276110Z","shell.execute_reply.started":"2023-07-11T05:22:01.267427Z","shell.execute_reply":"2023-07-11T05:22:01.274845Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Validation score: 0.4665523156089194\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Levenstein Distance Train","metadata":{}},{"cell_type":"code","source":"# Compute Levenstein Distances\ndef get_ld_train():\n    N = 100 if IS_INTERACTIVE else 1000\n    LD_TRAIN = []\n    for idx, (frames, phrase_true) in enumerate(zip(tqdm(X_train, total=N), y_train)):\n        # Predict Phrase and Convert to String\n        phrase_pred = predict_phrase(frames).numpy()\n        phrase_pred = outputs2phrase(phrase_pred)\n        # True Phrase Ordinal to String\n        phrase_true = outputs2phrase(phrase_true)\n        # Add Levenstein Distance\n        LD_TRAIN.append({\n            'phrase_true': phrase_true,\n            'phrase_pred': phrase_pred,\n            'levenshtein_distance': levenshtein(phrase_pred, phrase_true),\n        })\n        # Take subset in interactive mode\n        if idx == N:\n            break\n            \n    # Convert to DataFrame\n    LD_TRAIN_DF = pd.DataFrame(LD_TRAIN)\n    \n    return LD_TRAIN_DF","metadata":{"execution":{"iopub.status.busy":"2023-07-11T05:22:04.341761Z","iopub.execute_input":"2023-07-11T05:22:04.342171Z","iopub.status.idle":"2023-07-11T05:22:04.350124Z","shell.execute_reply.started":"2023-07-11T05:22:04.342139Z","shell.execute_reply":"2023-07-11T05:22:04.348789Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"# TFLiteModel","metadata":{}},{"cell_type":"code","source":" class TFLiteModel(tf.Module):\n    def __init__(self, model):\n        super(TFLiteModel, self).__init__()\n        self.target_start_token_idx = start_token_idx\n        self.target_end_token_idx = end_token_idx\n        # Load the feature generation and main models\n        self.model = model\n\n    @tf.function(input_signature=[tf.TensorSpec(shape=[None, len(SEL_COLS)], dtype=tf.float32, name='inputs')])\n    def __call__(self, inputs, training=False):\n        # Preprocess Data\n        x = tf.cast(inputs, tf.float32)\n        x = x[None]\n        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, len(SEL_COLS))), lambda: tf.identity(x))\n        x = x[0]\n        x = pre_process(x)\n        x = x[None]\n        x = self.model.generate(x, self.target_start_token_idx)\n        x = x[0]\n        idx = tf.argmax(tf.cast(tf.equal(x, self.target_end_token_idx), tf.int32))\n        idx = tf.where(tf.math.less(idx, 1), tf.constant(2, dtype=tf.int64), idx)\n        x = x[1:idx]\n        x = tf.one_hot(x, 59)\n        return {'outputs': x}\n\ntflitemodel_base = TFLiteModel(model)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T05:25:07.566607Z","iopub.execute_input":"2023-07-11T05:25:07.568271Z","iopub.status.idle":"2023-07-11T05:25:07.582983Z","shell.execute_reply.started":"2023-07-11T05:25:07.568235Z","shell.execute_reply":"2023-07-11T05:25:07.581982Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"model.save_weights(\"model.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-07-11T05:25:13.472337Z","iopub.execute_input":"2023-07-11T05:25:13.472728Z","iopub.status.idle":"2023-07-11T05:25:13.663652Z","shell.execute_reply.started":"2023-07-11T05:25:13.472698Z","shell.execute_reply":"2023-07-11T05:25:13.662488Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# 重みの読み込み\nmodel.load_weights('model.h5')\n\n# モデルの形状を表示\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-11T05:25:15.747322Z","iopub.execute_input":"2023-07-11T05:25:15.747696Z","iopub.status.idle":"2023-07-11T05:25:15.950253Z","shell.execute_reply.started":"2023-07-11T05:25:15.747667Z","shell.execute_reply":"2023-07-11T05:25:15.949244Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Model: \"transformer_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n landmark_embedding_5 (Landm  (None, 128, 200)         1495600   \n arkEmbedding)                                                   \n                                                                 \n token_embedding_5 (TokenEmb  multiple                 253600    \n edding)                                                         \n                                                                 \n sequential_33 (Sequential)  (None, 128, 200)          3103600   \n                                                                 \n transformer_decoder_8 (Tran  multiple                 1447000   \n sformerDecoder)                                                 \n                                                                 \n transformer_decoder_9 (Tran  multiple                 1447000   \n sformerDecoder)                                                 \n                                                                 \n dense_89 (Dense)            multiple                  12462     \n                                                                 \n=================================================================\nTotal params: 6,263,664\nTrainable params: 6,262,062\nNon-trainable params: 1,602\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\nkeras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]#, tf.lite.OpsSet.SELECT_TF_OPS]\ntflite_model = keras_model_converter.convert()\nwith open('/kaggle/working/model.tflite', 'wb') as f:\n    f.write(tflite_model)\n\ninfargs = {\"selected_columns\" : SEL_COLS}\n\nwith open('inference_args.json', \"w\") as json_file:\n    json.dump(infargs, json_file)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T05:25:20.551214Z","iopub.execute_input":"2023-07-11T05:25:20.551591Z","iopub.status.idle":"2023-07-11T05:27:21.832593Z","shell.execute_reply.started":"2023-07-11T05:25:20.551561Z","shell.execute_reply":"2023-07-11T05:27:21.831298Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Tensor(\"cond_2/Pad:0\", shape=(None, 26, 3), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"code","source":"!zip submission.zip  './model.tflite' './inference_args.json'","metadata":{"execution":{"iopub.status.busy":"2023-07-11T05:27:29.291273Z","iopub.execute_input":"2023-07-11T05:27:29.291675Z","iopub.status.idle":"2023-07-11T05:27:32.187791Z","shell.execute_reply.started":"2023-07-11T05:27:29.291644Z","shell.execute_reply":"2023-07-11T05:27:32.186534Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"updating: model.tflite (deflated 20%)\nupdating: inference_args.json (deflated 85%)\n","output_type":"stream"}]},{"cell_type":"code","source":"interpreter = tf.lite.Interpreter(\"model.tflite\")\n\nREQUIRED_SIGNATURE = \"serving_default\"\nREQUIRED_OUTPUT = \"outputs\"\n\nwith open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n    character_map = json.load(f)\nrev_character_map = {j:i for i,j in character_map.items()}\n\nfound_signatures = list(interpreter.get_signature_list().keys())\n\nif REQUIRED_SIGNATURE not in found_signatures:\n    raise KernelEvalException('Required input signature not found.')\n\nprediction_fn = interpreter.get_signature_runner(\"serving_default\")\noutput = prediction_fn(inputs=batch[0][0])\nprediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\nprint(prediction_str)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T05:27:34.815825Z","iopub.execute_input":"2023-07-11T05:27:34.816859Z","iopub.status.idle":"2023-07-11T05:27:35.946476Z","shell.execute_reply.started":"2023-07-11T05:27:34.816801Z","shell.execute_reply":"2023-07-11T05:27:35.945399Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"888-888-888\n","output_type":"stream"}]}]}