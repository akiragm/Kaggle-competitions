{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n1. I used two transformer layer in the encoder instead of four.\n2. I used four attention heads instead of two.\n3. I used new tokens for SOS, EOS, and padding (very minor since Rohith used rare tokens for these purposes, but still- more 'correct').\n2. I fixed a bug (probably?) in the decoder's dropout layers, which did not have the training flag, resulting in dropout during inference. This change gave a nice bump in the score.\n3. I made the passing of the training flag explicit. I know it can be implicit since it is a kwarg, but explicit passing makes the whole thing more straightforward and maybe fix another one or two training-flag-related bugs along the way.\n4. I changed the positional encoding in the decoder from tf.keras.layers.Embedding to proper positional embeddings (i.e., the usual sines and cosines usually used for this purpose). This had a significant impact.\n5. I added positional embedding to the encoder. This, too, had a significant impact.\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.metrics import Accuracy\nfrom skimage.transform import resize\nfrom sklearn.model_selection import train_test_split\nimport json\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport Levenshtein as lev\nimport os\nimport gc","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:29:45.864958Z","iopub.execute_input":"2023-07-11T04:29:45.865358Z","iopub.status.idle":"2023-07-11T04:29:49.371894Z","shell.execute_reply.started":"2023-07-11T04:29:45.865329Z","shell.execute_reply":"2023-07-11T04:29:49.370660Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"inpdir = \"/kaggle/input/asl-fingerspelling\"\ndf = pd.read_csv(f'{inpdir}/train.csv')\ndf[\"phrase_bytes\"] = df[\"phrase\"].map(lambda x: x.encode(\"utf-8\"))\ndisplay(df.head())","metadata":{}},{"cell_type":"markdown","source":"train_landmarks = pd.read_parquet('/kaggle/input/asl-fingerspelling/train_landmarks/1019715464.parquet')\nkeys = train_landmarks.keys()[1:]\ntrain_landmarks.head()","metadata":{}},{"cell_type":"markdown","source":"# TFRecord","metadata":{}},{"cell_type":"markdown","source":"LPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\nPOSE = LPOSE + RPOSE\n\nRHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\nLHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\nPOSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n\nX = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\nY = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\nZ = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n\nSEL_COLS = X + Y + Z\nFRAME_LEN = 128\n\nX_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\nY_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\nZ_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n\nRHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\nLHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\nRPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\nLPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n\nprint('SEL_COLS size:' + str(len(SEL_COLS)))","metadata":{}},{"cell_type":"markdown","source":"def load_relevant_data_subset(pq_path):\n    return pd.read_parquet(pq_path, columns=SEL_COLS)\n\ncounter = 0\nfor file_id in tqdm(df.file_id.unique()):\n    \n    print(counter)\n    counter+=1\n    \n    pqfile = f\"{inpdir}/train_landmarks/{file_id}.parquet\"\n    if not os.path.isdir(\"tfds\"): os.mkdir(\"tfds\")\n    tffile = f\"tfds/{file_id}.tfrecord\"\n    seq_refs = df.loc[df.file_id == file_id]\n    seqs = load_relevant_data_subset(pqfile)\n    seqs_numpy = seqs.to_numpy()\n    with tf.io.TFRecordWriter(tffile) as file_writer:\n        for seq_id, phrase in zip(seq_refs.sequence_id, seq_refs.phrase_bytes):\n            frames = seqs_numpy[seqs.index == seq_id]\n            \n            r_nonan = np.sum(np.sum(np.isnan(frames[:, RHAND_IDX]), axis = 1) == 0)\n            l_nonan = np.sum(np.sum(np.isnan(frames[:, LHAND_IDX]), axis = 1) == 0)\n            no_nan = max(r_nonan, l_nonan)\n            \n            if 2*len(phrase)<no_nan:\n                features = {SEL_COLS[i]: tf.train.Feature(\n                    float_list=tf.train.FloatList(value=frames[:, i])) for i in range(len(SEL_COLS))}\n                features[\"phrase\"] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[phrase]))\n                record_bytes = tf.train.Example(features=tf.train.Features(feature=features)).SerializeToString()\n                file_writer.write(record_bytes)","metadata":{}},{"cell_type":"markdown","source":"# Data loading","metadata":{}},{"cell_type":"markdown","source":"#### Here I use new tokens for padding, start and end of sentences. (Capitals are good since the original phrases have only lower case letters, besides numbers and various signs).","metadata":{}},{"cell_type":"code","source":"pad_token = 'P'\nstart_token = 'S'\nend_token = 'E'\npad_token_idx = 59\nstart_token_idx = 60\nend_token_idx = 61","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:29:54.002625Z","iopub.execute_input":"2023-07-11T04:29:54.003344Z","iopub.status.idle":"2023-07-11T04:29:54.009044Z","shell.execute_reply.started":"2023-07-11T04:29:54.003311Z","shell.execute_reply":"2023-07-11T04:29:54.007921Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n    char_to_num = json.load(f)\n\n\nchar_to_num[pad_token] = pad_token_idx\nchar_to_num[start_token] = start_token_idx\nchar_to_num[end_token] = end_token_idx\n\nnum_to_char = {j:i for i,j in char_to_num.items()}\n\n\ninpdir = \"/kaggle/input/asl-fingerspelling\"\ndf = pd.read_csv(f'{inpdir}/train.csv')\n\nLPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\nPOSE = LPOSE + RPOSE\n\nRHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\nLHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\nPOSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n\nX = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\nY = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\nZ = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n\nSEL_COLS = X + Y + Z\nFRAME_LEN = 128\n\nX_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\nY_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\nZ_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n\nRHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\nLHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\nRPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\nLPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n\nprint(RPOSE_IDX)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:29:56.627107Z","iopub.execute_input":"2023-07-11T04:29:56.627472Z","iopub.status.idle":"2023-07-11T04:29:56.739106Z","shell.execute_reply.started":"2023-07-11T04:29:56.627444Z","shell.execute_reply":"2023-07-11T04:29:56.738070Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[47, 48, 49, 50, 51, 99, 100, 101, 102, 103, 151, 152, 153, 154, 155]\n","output_type":"stream"}]},{"cell_type":"code","source":"def resize_pad(x):\n    if tf.shape(x)[0] < FRAME_LEN:\n        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0], [0, 0]]))\n        print(x)\n    else:\n        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n    return x\n\ndef translate_landmarks(landmarks, max_translation):\n    translation = tf.random.uniform(shape=tf.shape(landmarks), minval=-max_translation, maxval=max_translation)\n    translated_landmarks = landmarks + translation\n    return translated_landmarks\n\n# def scale_landmarks(landmarks, min_scale, max_scale):\n#     scale_factor = tf.random.uniform(shape=tf.shape(landmarks), minval=min_scale, maxval=max_scale)\n#     scaled_landmarks = landmarks * scale_factor\n#     return scaled_landmarks\n\ndef pre_process(x):\n\n    rhand = tf.gather(x, RHAND_IDX, axis=1)\n    lhand = tf.gather(x, LHAND_IDX, axis=1)\n    rpose = tf.gather(x, RPOSE_IDX, axis=1)\n    lpose = tf.gather(x, LPOSE_IDX, axis=1)\n\n    rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)\n    lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)\n\n    rnans = tf.math.count_nonzero(rnan_idx)\n    lnans = tf.math.count_nonzero(lnan_idx)\n\n    # For dominant hand\n    if rnans > lnans:\n        hand = lhand\n        pose = lpose\n\n        hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n        hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n        hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n        hand = tf.concat([1-hand_x, hand_y, hand_z], axis=1)\n\n        pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n        pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n        pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n        pose = tf.concat([1-pose_x, pose_y, pose_z], axis=1)\n    else:\n        hand = rhand\n        pose = rpose\n\n    hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n    hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n    hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n    hand = tf.concat([hand_x[..., tf.newaxis], hand_y[..., tf.newaxis], hand_z[..., tf.newaxis]], axis=-1)\n\n    mean = tf.math.reduce_mean(hand, axis=1)[:, tf.newaxis, :]\n    std = tf.math.reduce_std(hand, axis=1)[:, tf.newaxis, :]\n    hand = (hand - mean) / std\n\n    pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n    pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n    pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n    pose = tf.concat([pose_x[..., tf.newaxis], pose_y[..., tf.newaxis], pose_z[..., tf.newaxis]], axis=-1)\n\n    x = tf.concat([hand, pose], axis=1)\n    x = resize_pad(x)\n\n    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n    x = tf.reshape(x, (FRAME_LEN, len(LHAND_IDX) + len(LPOSE_IDX)))\n    return x","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:29:59.962426Z","iopub.execute_input":"2023-07-11T04:29:59.962826Z","iopub.status.idle":"2023-07-11T04:29:59.985442Z","shell.execute_reply.started":"2023-07-11T04:29:59.962793Z","shell.execute_reply":"2023-07-11T04:29:59.983906Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"table = tf.lookup.StaticHashTable(\n    initializer=tf.lookup.KeyValueTensorInitializer(\n        keys=list(char_to_num.keys()),\n        values=list(char_to_num.values()),\n    ),\n    default_value=tf.constant(-1),\n    name=\"class_weight\"\n)\n\ndef preprocess_fn(landmarks, phrase):\n    phrase = start_token + phrase + end_token\n    phrase = tf.strings.bytes_split(phrase)\n    phrase = table.lookup(phrase)\n    phrase = tf.pad(phrase, paddings=[[0, 64 - tf.shape(phrase)[0]]], mode = 'CONSTANT',\n                    constant_values = pad_token_idx)\n\n    # landmarksを前処理する\n    translated_landmarks = translate_landmarks(landmarks, max_translation=10)\n    #scaled_landmarks = scale_landmarks(landmarks, min_scale=0.8, max_scale=1.2)\n\n    # 前処理済みのlandmarksを結合する\n    #combined_landmarks = tf.concat([landmarks, translated_landmarks, scaled_landmarks], axis=1)\n    combined_landmarks = tf.concat([landmarks, translated_landmarks], axis=1)\n    return pre_process(combined_landmarks), phrase\n\ndef decode_fn(record_bytes):\n    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in SEL_COLS}\n    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n    features = tf.io.parse_single_example(record_bytes, schema)\n    phrase = features[\"phrase\"]\n    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in SEL_COLS])\n    landmarks = tf.transpose(landmarks)\n\n    return landmarks, phrase\n\ninpdir = \"/kaggle/input/aslfr-parquets-to-tfrecords-cleaned\"\ntffiles = df.file_id.map(lambda x: f'{inpdir}/tfds/{x}.tfrecord').unique()\n\nbatch_size = 32\nval_len = int(0.05 * len(tffiles))\n\ntrain_dataset = tf.data.TFRecordDataset(tffiles[val_len:]).map(decode_fn).map(preprocess_fn).shuffle(30000, reshuffle_each_iteration=True).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\nval_dataset = tf.data.TFRecordDataset(tffiles[:val_len]).map(decode_fn).map(preprocess_fn).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:30:09.866505Z","iopub.execute_input":"2023-07-11T04:30:09.866872Z","iopub.status.idle":"2023-07-11T04:30:10.821039Z","shell.execute_reply.started":"2023-07-11T04:30:09.866837Z","shell.execute_reply":"2023-07-11T04:30:10.820013Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Tensor(\"cond_1/Pad:0\", shape=(None, 26, 3), dtype=float32)\nTensor(\"cond_1/Pad:0\", shape=(None, 26, 3), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# The model","metadata":{}},{"cell_type":"code","source":"LPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\n\nPOSE = LPOSE + RPOSE\n\nRHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\nLHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\nPOSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n\nX = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\nY = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\nZ = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n\nSEL_COLS = X + Y + Z\nFRAME_LEN = 128\n\nX_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\nY_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\nZ_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n\nRHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\nLHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\nRPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\nLPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n\nprint('SEL_COLS size:' + str(len(SEL_COLS)))","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:30:16.362876Z","iopub.execute_input":"2023-07-11T04:30:16.363254Z","iopub.status.idle":"2023-07-11T04:30:16.379665Z","shell.execute_reply.started":"2023-07-11T04:30:16.363225Z","shell.execute_reply":"2023-07-11T04:30:16.378530Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"SEL_COLS size:156\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Here I implemented proper positional embeddings for both the encoder and the decoder.","metadata":{}},{"cell_type":"code","source":"class MLPBlock(tf.keras.layers.Layer):\n    def __init__(self, num_hid=64, num_layers=5):\n        super().__init__()\n        self.mlp = tf.keras.Sequential()\n        for _ in range(num_layers):\n            self.mlp.add(tf.keras.layers.Dense(num_hid, activation=tf.nn.gelu))\n        self.mlp.add(tf.keras.layers.Dense(num_hid))\n\n    def call(self, inputs):\n        return self.mlp(inputs)\n\n\nclass TokenEmbedding(keras.layers.Layer):\n    def __init__(self, num_vocab=61, maxlen=50, num_hid=256, mlp_num_layers=5):\n        super().__init__()\n        self.num_hid = num_hid\n        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n        self.pos_emb = self.positional_encoding(maxlen - 1, num_hid)\n        self.mlp_block = MLPBlock(num_hid, num_layers=mlp_num_layers)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        x = self.emb(x) * tf.math.sqrt(tf.cast(self.num_hid, tf.float32))\n        x = x + self.pos_emb[:maxlen, :]\n        x = self.mlp_block(x)\n        return x\n\n    def positional_encoding(self, maxlen, num_hid):\n        positions = tf.range(maxlen, dtype=tf.float32)[..., tf.newaxis]\n        depth = num_hid // 2\n        angles = positions / tf.pow(10000, tf.range(0, depth, 1, dtype=tf.float32) / num_hid)  # depthのインクリメントを修正\n        pos_encoding = tf.concat([tf.sin(angles), tf.cos(angles)], axis=-1)\n        return pos_encoding\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:30:20.755941Z","iopub.execute_input":"2023-07-11T04:30:20.756302Z","iopub.status.idle":"2023-07-11T04:30:20.770649Z","shell.execute_reply.started":"2023-07-11T04:30:20.756273Z","shell.execute_reply":"2023-07-11T04:30:20.768705Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"class TokenEmbedding(layers.Layer):\n    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n        super().__init__()\n        self.num_hid = num_hid\n        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n        #self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n        '''\n        self.pos_emb = tf.math.divide(\n            self.positional_encoding(maxlen-1, num_hid),\n            tf.math.sqrt(tf.cast(num_hid, tf.float32)))\n        '''\n        self.pos_emb = self.positional_encoding(maxlen-1, num_hid)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        x = self.emb(x)\n        x = tf.math.multiply(x, tf.math.sqrt(tf.cast(self.num_hid, tf.float32)))\n        '''\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        return x + positions\n        '''\n        return x + self.pos_emb[:maxlen, :]\n    \n    def positional_encoding(self, maxlen, num_hid):\n        depth = num_hid/2\n        positions = tf.range(maxlen, dtype = tf.float32)[..., tf.newaxis]\n        depths = tf.range(depth, dtype = tf.float32)[np.newaxis, :]/depth\n        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n        angle_rads = tf.linalg.matmul(positions, angle_rates)\n        pos_encoding = tf.concat(\n          [tf.math.sin(angle_rads), tf.math.cos(angle_rads)],\n          axis=-1)\n        return pos_encoding\n\n\n","metadata":{}},{"cell_type":"code","source":"class LandmarkEmbedding(tf.keras.Model):\n    def __init__(self, num_hid=256, maxlen=100):\n        super(LandmarkEmbedding, self).__init__()\n        self.conv1 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn1 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu1 = tf.keras.layers.LeakyReLU()\n\n        self.conv2 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn2 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu2 = tf.keras.layers.LeakyReLU()\n        self.dropout2 = tf.keras.layers.Dropout(0.2)\n\n        self.conv3 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn3 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu3 = tf.keras.layers.LeakyReLU()\n        self.dropout3 = tf.keras.layers.Dropout(0.2)\n\n        self.conv4 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn4 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu4 = tf.keras.layers.LeakyReLU()\n        self.dropout4 = tf.keras.layers.Dropout(0.2)\n\n        self.sigmoid = tf.keras.layers.Activation('sigmoid')\n        self.pos_emb = self.positional_encoding(maxlen, num_hid)\n        self.maxlen = maxlen\n        self.num_hid = num_hid\n\n    def call(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.leaky_relu1(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.leaky_relu2(x)\n        x = self.dropout2(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.leaky_relu3(x)\n        x = self.dropout3(x)\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.leaky_relu4(x)\n        x = self.dropout4(x)\n        x = tf.math.multiply(x, tf.math.sqrt(tf.cast(self.num_hid, tf.float32)))\n        x = x + self.pos_emb\n\n        return self.sigmoid(x)\n\n    def positional_encoding(self, maxlen, num_hid):\n        depth = num_hid/2\n        positions = tf.range(maxlen, dtype=tf.float32)[..., tf.newaxis]\n        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth\n        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n        angle_rads = tf.linalg.matmul(positions, angle_rates)\n        pos_encoding = tf.concat([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], axis=-1)\n        return pos_encoding\n","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:30:28.481955Z","iopub.execute_input":"2023-07-11T04:30:28.482310Z","iopub.status.idle":"2023-07-11T04:30:28.501538Z","shell.execute_reply.started":"2023-07-11T04:30:28.482281Z","shell.execute_reply":"2023-07-11T04:30:28.500488Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class TransformerEncoder(keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:30:34.957557Z","iopub.execute_input":"2023-07-11T04:30:34.957933Z","iopub.status.idle":"2023-07-11T04:30:34.967044Z","shell.execute_reply.started":"2023-07-11T04:30:34.957905Z","shell.execute_reply":"2023-07-11T04:30:34.965631Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"#### Here I added the training flag to the TransformerDecoder's Dropout layers.","metadata":{}},{"cell_type":"code","source":"class TransformerDecoder(keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n        super().__init__()\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n        self.self_att = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.self_dropout = layers.Dropout(0.5)\n        self.enc_dropout = layers.Dropout(0.1)\n        self.ffn_dropout = layers.Dropout(0.1)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n\n    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n        \"\"\"Masks the upper half of the dot product matrix in self attention.\n\n        This prevents flow of information from future tokens to current token.\n        1's in the lower triangle, counting from the lower right corner.\n        \"\"\"\n        i = tf.range(n_dest)[:, None]\n        j = tf.range(n_src)\n        m = i >= j - n_src + n_dest\n        mask = tf.cast(m, dtype)\n        mask = tf.reshape(mask, [1, n_dest, n_src])\n        mult = tf.concat(\n            [batch_size[..., tf.newaxis], tf.constant([1, 1], dtype=tf.int32)], 0\n        )\n        return tf.tile(mask, mult)\n\n    def call(self, enc_out, target, training):\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n        target_att = self.self_att(target, target, attention_mask=causal_mask)\n        target_norm = self.layernorm1(target + self.self_dropout(target_att, training = training))\n        enc_out = self.enc_att(target_norm, enc_out)\n        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out, training = training) + target_norm)\n        ffn_out = self.ffn(enc_out_norm)\n        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out, training = training))\n        return ffn_out_norm","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:30:39.523835Z","iopub.execute_input":"2023-07-11T04:30:39.524190Z","iopub.status.idle":"2023-07-11T04:30:39.539207Z","shell.execute_reply.started":"2023-07-11T04:30:39.524162Z","shell.execute_reply":"2023-07-11T04:30:39.538224Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"#### Here I made the passing of the training flag explicit.","metadata":{}},{"cell_type":"code","source":"class Transformer(tf.keras.Model):\n    def __init__(\n        self,\n        num_hid=64,\n        num_head=2,\n        num_feed_forward=128,\n        source_maxlen=100,\n        target_maxlen=100,\n        num_layers_enc=4,\n        num_layers_dec=1,\n        num_classes=60,\n    ):\n        super().__init__()\n        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n        self.num_layers_enc = num_layers_enc\n        self.num_layers_dec = num_layers_dec\n        self.target_maxlen = target_maxlen\n        self.num_classes = num_classes\n\n        self.enc_input = LandmarkEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n        self.dec_input = TokenEmbedding(\n            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n        )\n\n        self.encoder = keras.Sequential(\n            [self.enc_input]\n            + [\n                TransformerEncoder(num_hid, num_head, num_feed_forward)\n                for _ in range(num_layers_enc)\n            ]\n        )\n\n        for i in range(num_layers_dec):\n            setattr(\n                self,\n                f\"dec_layer_{i}\",\n                TransformerDecoder(num_hid, num_head, num_feed_forward),\n            )\n\n        self.classifier = layers.Dense(num_classes)\n\n    def decode(self, enc_out, target, training):\n        y = self.dec_input(target)\n        for i in range(self.num_layers_dec):\n            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y, training)\n        return y\n\n    def call(self, inputs, training):\n        source = inputs[0]\n        target = inputs[1]\n        x = self.encoder(source, training)\n        y = self.decode(x, target, training)\n        return self.classifier(y)\n\n    @property\n    def metrics(self):\n        return [self.loss_metric]\n\n    def train_step(self, batch):\n        \"\"\"Processes one batch inside model.fit().\"\"\"\n        source = batch[0]\n        target = batch[1]\n\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        \n        dec_input = target[:, :-1]\n        dec_target = target[:, 1:]\n        with tf.GradientTape() as tape:\n            preds = self([source, dec_input])\n            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n            mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        self.loss_metric.update_state(loss)\n        return {\"loss\": self.loss_metric.result()}\n\n    def test_step(self, batch):        \n        source = batch[0]\n        target = batch[1]\n\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        \n        dec_input = target[:, :-1]\n        dec_target = target[:, 1:]\n        preds = self([source, dec_input])\n        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n        mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n        self.loss_metric.update_state(loss)\n        return {\"loss\": self.loss_metric.result()}\n\n    def generate(self, source, target_start_token_idx):\n        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n        bs = tf.shape(source)[0]\n        enc = self.encoder(source, training = False)\n        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n        dec_logits = []\n        for i in range(self.target_maxlen - 1):\n            dec_out = self.decode(enc, dec_input, training = False)\n            logits = self.classifier(dec_out)\n            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n            last_logit = logits[:, -1][..., tf.newaxis]\n            dec_logits.append(last_logit)\n            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n        return dec_input","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:30:44.756130Z","iopub.execute_input":"2023-07-11T04:30:44.756485Z","iopub.status.idle":"2023-07-11T04:30:44.779131Z","shell.execute_reply.started":"2023-07-11T04:30:44.756458Z","shell.execute_reply":"2023-07-11T04:30:44.778054Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# 正解率を計算するためのメトリクスを作成\ntrain_accuracy = tf.keras.metrics.CategoricalAccuracy()\nval_accuracy = tf.keras.metrics.CategoricalAccuracy()\n\n# 学習ループ内で正解率を更新するコールバックを定義\nclass AccuracyCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        train_acc = train_accuracy.result()\n        val_acc = val_accuracy.result()\n        print(f\"Epoch {epoch+1}: Train Accuracy = {train_acc}, Validation Accuracy = {val_acc}\")\n        # 正解率をリセット\n        train_accuracy.reset_states()\n        val_accuracy.reset_states()\n# val_lossが3回マイナスになった場合に学習を停止するコールバック\nclass EarlyStoppingCallback(tf.keras.callbacks.Callback):\n    def __init__(self, patience=7):\n        super(EarlyStoppingCallback, self).__init__()\n        self.patience = patience\n        self.min_val_loss = float('inf')\n        self.wait = 0\n\n    def on_epoch_end(self, epoch, logs=None):\n        val_loss = logs.get('val_loss')\n        if val_loss < self.min_val_loss:\n            self.min_val_loss = val_loss\n            self.wait = 0\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                self.model.stop_training = True\n                print(\"Training stopped due to early stopping.\")\n\nbatch = next(iter(val_dataset))\nidx_to_char = list(char_to_num.keys())\n\nmodel = Transformer(\n    num_hid=200,\n    num_head=4,\n    num_feed_forward=400,\n    source_maxlen = FRAME_LEN,\n    target_maxlen=64,\n    num_layers_enc=2,\n    num_layers_dec=2,\n    num_classes=62,\n)\n\n\n\nloss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1)\naccuracy_callback = AccuracyCallback()\noptimizer = keras.optimizers.Adam(0.0001)\n\n\n# モデルのコンパイル\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=[train_accuracy])","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:48:11.185530Z","iopub.execute_input":"2023-07-11T04:48:11.185913Z","iopub.status.idle":"2023-07-11T04:48:11.612621Z","shell.execute_reply.started":"2023-07-11T04:48:11.185884Z","shell.execute_reply":"2023-07-11T04:48:11.611648Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"#modelアーキテクト\n#tf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True, show_layer_names=True, expand_nested=True, show_layer_activations=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:22:40.870153Z","iopub.status.idle":"2023-07-11T04:22:40.871145Z","shell.execute_reply.started":"2023-07-11T04:22:40.870904Z","shell.execute_reply":"2023-07-11T04:22:40.870926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nhistory = model.fit(train_dataset, verbose=2, validation_data=val_dataset, epochs=1)\n# EarlyStoppingCallbackをコールバックリストに追加して学習を行う\n# history = model.fit(train_dataset, verbose=2, validation_data=val_dataset, epochs=1,\n#                     callbacks=[AccuracyCallback(), EarlyStoppingCallback()])\n","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:48:16.900105Z","iopub.execute_input":"2023-07-11T04:48:16.900482Z","iopub.status.idle":"2023-07-11T04:49:50.005259Z","shell.execute_reply.started":"2023-07-11T04:48:16.900452Z","shell.execute_reply":"2023-07-11T04:49:50.004195Z"},"trusted":true},"execution_count":37,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m<timed exec>:1\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:959\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    956\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m   _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    962\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn\u001b[38;5;241m.\u001b[39m_function_spec  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    963\u001b[0m       \u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(\n\u001b[1;32m    964\u001b[0m           args, kwds))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"print(model.summary())","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:43:14.492291Z","iopub.execute_input":"2023-07-11T04:43:14.492706Z","iopub.status.idle":"2023-07-11T04:43:14.540797Z","shell.execute_reply.started":"2023-07-11T04:43:14.492676Z","shell.execute_reply":"2023-07-11T04:43:14.539832Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Model: \"transformer_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n landmark_embedding_1 (Landm  (None, 128, 200)         1495600   \n arkEmbedding)                                                   \n                                                                 \n token_embedding_1 (TokenEmb  multiple                 253600    \n edding)                                                         \n                                                                 \n sequential_10 (Sequential)  (None, 128, 200)          3907600   \n                                                                 \n transformer_decoder_1 (Tran  multiple                 1447000   \n sformerDecoder)                                                 \n                                                                 \n dense_29 (Dense)            multiple                  12462     \n                                                                 \n=================================================================\nTotal params: 5,620,664\nTrainable params: 5,619,062\nNon-trainable params: 1,602\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:43:22.523140Z","iopub.execute_input":"2023-07-11T04:43:22.523513Z","iopub.status.idle":"2023-07-11T04:43:22.805108Z","shell.execute_reply.started":"2023-07-11T04:43:22.523485Z","shell.execute_reply":"2023-07-11T04:43:22.804119Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[<matplotlib.lines.Line2D at 0x791d6b5d8460>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"code","source":"batches = [batch for batch in val_dataset]\n\npreds_list = []\nground_truth_list = []\n\nfor batch in batches[:1]:\n    source = batch[0]\n    target = batch[1].numpy()\n    bs = tf.shape(source)[0]\n    preds = model.generate(source, start_token_idx)\n    preds = preds.numpy()\n\n    for i in range(bs):\n        target_text = \"\".join([idx_to_char[_] for _ in target[i, :]])\n        ground_truth_list.append(target_text.replace('P', ''))\n        prediction = \"\"\n        for idx in preds[i, :]:\n            prediction += idx_to_char[idx]\n            if idx == end_token_idx:\n                break\n        preds_list.append(prediction)\n\nfor i in range(10):\n    print(ground_truth_list[i])\n    print(preds_list[i])\n    print('\\n~~~\\n')","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:43:34.860570Z","iopub.execute_input":"2023-07-11T04:43:34.860951Z","iopub.status.idle":"2023-07-11T04:43:40.934203Z","shell.execute_reply.started":"2023-07-11T04:43:34.860922Z","shell.execute_reply":"2023-07-11T04:43:40.933181Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"S3 creekhouseE\nS353 routh rteeE\n\n~~~\n\nSscales/kuhaylahE\nSsalales-paiaiaE\n\n~~~\n\nS1383 william lanierE\nS1385 wians laneE\n\n~~~\n\nS988 franklin laneE\nS989 fannt staneE\n\n~~~\n\nS6920 northeast 661st roadE\nSwww.orterthest.um/wadelelE\n\n~~~\n\nSwww.freem.ne.jpE\nSwww.fememes.memE\n\n~~~\n\nShttps://jsi.is/hukuokaE\nShttps://isisililerti.comE\n\n~~~\n\nS239613 stolze streetE\nS2961 nanglllles destE\n\n~~~\n\nS271097 bayshore boulevardE\nS27097 baimerd bourdE\n\n~~~\n\nSfederico pearsonE\nS990 dence roonE\n\n~~~\n\n","output_type":"stream"}]},{"cell_type":"code","source":"ground_truth_processed = [ground_truth_list[i][1:-1] for i in range(len(ground_truth_list))]\npreds_list_processed = [preds_list[i][1:-1] for i in range(len(preds_list))]\nlev_dist = [lev.distance(ground_truth_processed[i], preds_list_processed[i])\n            for i in range(len(preds_list_processed))]\nN = [len(phrase) for phrase in ground_truth_processed]\n\nprint('Validation score: '+str((np.sum(N) - np.sum(lev_dist))/np.sum(N)))","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:43:44.245639Z","iopub.execute_input":"2023-07-11T04:43:44.246822Z","iopub.status.idle":"2023-07-11T04:43:44.261798Z","shell.execute_reply.started":"2023-07-11T04:43:44.246779Z","shell.execute_reply":"2023-07-11T04:43:44.260571Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Validation score: 0.40823327615780447\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Levenstein Distance Train","metadata":{}},{"cell_type":"code","source":"# Compute Levenstein Distances\ndef get_ld_train():\n    N = 100 if IS_INTERACTIVE else 1000\n    LD_TRAIN = []\n    for idx, (frames, phrase_true) in enumerate(zip(tqdm(X_train, total=N), y_train)):\n        # Predict Phrase and Convert to String\n        phrase_pred = predict_phrase(frames).numpy()\n        phrase_pred = outputs2phrase(phrase_pred)\n        # True Phrase Ordinal to String\n        phrase_true = outputs2phrase(phrase_true)\n        # Add Levenstein Distance\n        LD_TRAIN.append({\n            'phrase_true': phrase_true,\n            'phrase_pred': phrase_pred,\n            'levenshtein_distance': levenshtein(phrase_pred, phrase_true),\n        })\n        # Take subset in interactive mode\n        if idx == N:\n            break\n            \n    # Convert to DataFrame\n    LD_TRAIN_DF = pd.DataFrame(LD_TRAIN)\n    \n    return LD_TRAIN_DF","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:43:52.389000Z","iopub.execute_input":"2023-07-11T04:43:52.389371Z","iopub.status.idle":"2023-07-11T04:43:52.396811Z","shell.execute_reply.started":"2023-07-11T04:43:52.389343Z","shell.execute_reply":"2023-07-11T04:43:52.395766Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# TFLiteModel","metadata":{}},{"cell_type":"code","source":" class TFLiteModel(tf.Module):\n    def __init__(self, model):\n        super(TFLiteModel, self).__init__()\n        self.target_start_token_idx = start_token_idx\n        self.target_end_token_idx = end_token_idx\n        # Load the feature generation and main models\n        self.model = model\n\n    @tf.function(input_signature=[tf.TensorSpec(shape=[None, len(SEL_COLS)], dtype=tf.float32, name='inputs')])\n    def __call__(self, inputs, training=False):\n        # Preprocess Data\n        x = tf.cast(inputs, tf.float32)\n        x = x[None]\n        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, len(SEL_COLS))), lambda: tf.identity(x))\n        x = x[0]\n        x = pre_process(x)\n        x = x[None]\n        x = self.model.generate(x, self.target_start_token_idx)\n        x = x[0]\n        idx = tf.argmax(tf.cast(tf.equal(x, self.target_end_token_idx), tf.int32))\n        idx = tf.where(tf.math.less(idx, 1), tf.constant(2, dtype=tf.int64), idx)\n        x = x[1:idx]\n        x = tf.one_hot(x, 59)\n        return {'outputs': x}\n\ntflitemodel_base = TFLiteModel(model)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:44:00.676596Z","iopub.execute_input":"2023-07-11T04:44:00.677565Z","iopub.status.idle":"2023-07-11T04:44:00.689513Z","shell.execute_reply.started":"2023-07-11T04:44:00.677528Z","shell.execute_reply":"2023-07-11T04:44:00.688584Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model.save_weights(\"model.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:44:07.983597Z","iopub.execute_input":"2023-07-11T04:44:07.984304Z","iopub.status.idle":"2023-07-11T04:44:08.150037Z","shell.execute_reply.started":"2023-07-11T04:44:07.984268Z","shell.execute_reply":"2023-07-11T04:44:08.148830Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# 重みの読み込み\nmodel.load_weights('model.h5')\n\n# モデルの形状を表示\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:44:12.708555Z","iopub.execute_input":"2023-07-11T04:44:12.708930Z","iopub.status.idle":"2023-07-11T04:44:13.045453Z","shell.execute_reply.started":"2023-07-11T04:44:12.708899Z","shell.execute_reply":"2023-07-11T04:44:13.040987Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Model: \"transformer_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n landmark_embedding_1 (Landm  (None, 128, 200)         1495600   \n arkEmbedding)                                                   \n                                                                 \n token_embedding_1 (TokenEmb  multiple                 253600    \n edding)                                                         \n                                                                 \n sequential_10 (Sequential)  (None, 128, 200)          3907600   \n                                                                 \n transformer_decoder_1 (Tran  multiple                 1447000   \n sformerDecoder)                                                 \n                                                                 \n dense_29 (Dense)            multiple                  12462     \n                                                                 \n=================================================================\nTotal params: 5,620,664\nTrainable params: 5,619,062\nNon-trainable params: 1,602\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\nkeras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]#, tf.lite.OpsSet.SELECT_TF_OPS]\ntflite_model = keras_model_converter.convert()\nwith open('/kaggle/working/model.tflite', 'wb') as f:\n    f.write(tflite_model)\n\ninfargs = {\"selected_columns\" : SEL_COLS}\n\nwith open('inference_args.json', \"w\") as json_file:\n    json.dump(infargs, json_file)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:44:16.917846Z","iopub.execute_input":"2023-07-11T04:44:16.918223Z","iopub.status.idle":"2023-07-11T04:45:43.497192Z","shell.execute_reply.started":"2023-07-11T04:44:16.918194Z","shell.execute_reply":"2023-07-11T04:45:43.496117Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Tensor(\"cond_2/Pad:0\", shape=(None, 26, 3), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"code","source":"!zip submission.zip  './model.tflite' './inference_args.json'","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:50:09.726388Z","iopub.execute_input":"2023-07-11T04:50:09.727119Z","iopub.status.idle":"2023-07-11T04:50:12.280807Z","shell.execute_reply.started":"2023-07-11T04:50:09.727076Z","shell.execute_reply":"2023-07-11T04:50:12.279620Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"updating: model.tflite (deflated 18%)\nupdating: inference_args.json (deflated 85%)\n","output_type":"stream"}]},{"cell_type":"code","source":"interpreter = tf.lite.Interpreter(\"model.tflite\")\n\nREQUIRED_SIGNATURE = \"serving_default\"\nREQUIRED_OUTPUT = \"outputs\"\n\nwith open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n    character_map = json.load(f)\nrev_character_map = {j:i for i,j in character_map.items()}\n\nfound_signatures = list(interpreter.get_signature_list().keys())\n\nif REQUIRED_SIGNATURE not in found_signatures:\n    raise KernelEvalException('Required input signature not found.')\n\nprediction_fn = interpreter.get_signature_runner(\"serving_default\")\noutput = prediction_fn(inputs=batch[0][0])\nprediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\nprint(prediction_str)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T04:50:15.334821Z","iopub.execute_input":"2023-07-11T04:50:15.335217Z","iopub.status.idle":"2023-07-11T04:50:15.770851Z","shell.execute_reply.started":"2023-07-11T04:50:15.335186Z","shell.execute_reply":"2023-07-11T04:50:15.769748Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"+888-88880\n","output_type":"stream"}]}]}