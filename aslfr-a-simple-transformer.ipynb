{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n1. I used two transformer layer in the encoder instead of four.\n2. I used four attention heads instead of two.\n3. I used new tokens for SOS, EOS, and padding (very minor since Rohith used rare tokens for these purposes, but still- more 'correct').\n2. I fixed a bug (probably?) in the decoder's dropout layers, which did not have the training flag, resulting in dropout during inference. This change gave a nice bump in the score.\n3. I made the passing of the training flag explicit. I know it can be implicit since it is a kwarg, but explicit passing makes the whole thing more straightforward and maybe fix another one or two training-flag-related bugs along the way.\n4. I changed the positional encoding in the decoder from tf.keras.layers.Embedding to proper positional embeddings (i.e., the usual sines and cosines usually used for this purpose). This had a significant impact.\n5. I added positional embedding to the encoder. This, too, had a significant impact.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.metrics import Accuracy\nfrom skimage.transform import resize\nfrom sklearn.model_selection import train_test_split\nimport json\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport Levenshtein as lev\nimport os\nimport gc","metadata":{"execution":{"iopub.status.busy":"2023-07-08T14:14:36.404860Z","iopub.execute_input":"2023-07-08T14:14:36.405878Z","iopub.status.idle":"2023-07-08T14:14:46.461730Z","shell.execute_reply.started":"2023-07-08T14:14:36.405841Z","shell.execute_reply":"2023-07-08T14:14:46.460696Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"inpdir = \"/kaggle/input/asl-fingerspelling\"\ndf = pd.read_csv(f'{inpdir}/train.csv')\ndf[\"phrase_bytes\"] = df[\"phrase\"].map(lambda x: x.encode(\"utf-8\"))\ndisplay(df.head())","metadata":{}},{"cell_type":"markdown","source":"train_landmarks = pd.read_parquet('/kaggle/input/asl-fingerspelling/train_landmarks/1019715464.parquet')\nkeys = train_landmarks.keys()[1:]\ntrain_landmarks.head()","metadata":{}},{"cell_type":"markdown","source":"# TFRecord","metadata":{}},{"cell_type":"markdown","source":"LPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\nPOSE = LPOSE + RPOSE\n\nRHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\nLHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\nPOSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n\nX = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\nY = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\nZ = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n\nSEL_COLS = X + Y + Z\nFRAME_LEN = 128\n\nX_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\nY_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\nZ_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n\nRHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\nLHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\nRPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\nLPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n\nprint('SEL_COLS size:' + str(len(SEL_COLS)))","metadata":{}},{"cell_type":"markdown","source":"def load_relevant_data_subset(pq_path):\n    return pd.read_parquet(pq_path, columns=SEL_COLS)\n\ncounter = 0\nfor file_id in tqdm(df.file_id.unique()):\n    \n    print(counter)\n    counter+=1\n    \n    pqfile = f\"{inpdir}/train_landmarks/{file_id}.parquet\"\n    if not os.path.isdir(\"tfds\"): os.mkdir(\"tfds\")\n    tffile = f\"tfds/{file_id}.tfrecord\"\n    seq_refs = df.loc[df.file_id == file_id]\n    seqs = load_relevant_data_subset(pqfile)\n    seqs_numpy = seqs.to_numpy()\n    with tf.io.TFRecordWriter(tffile) as file_writer:\n        for seq_id, phrase in zip(seq_refs.sequence_id, seq_refs.phrase_bytes):\n            frames = seqs_numpy[seqs.index == seq_id]\n            \n            r_nonan = np.sum(np.sum(np.isnan(frames[:, RHAND_IDX]), axis = 1) == 0)\n            l_nonan = np.sum(np.sum(np.isnan(frames[:, LHAND_IDX]), axis = 1) == 0)\n            no_nan = max(r_nonan, l_nonan)\n            \n            if 2*len(phrase)<no_nan:\n                features = {SEL_COLS[i]: tf.train.Feature(\n                    float_list=tf.train.FloatList(value=frames[:, i])) for i in range(len(SEL_COLS))}\n                features[\"phrase\"] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[phrase]))\n                record_bytes = tf.train.Example(features=tf.train.Features(feature=features)).SerializeToString()\n                file_writer.write(record_bytes)","metadata":{}},{"cell_type":"markdown","source":"# Data loading","metadata":{}},{"cell_type":"markdown","source":"#### Here I use new tokens for padding, start and end of sentences. (Capitals are good since the original phrases have only lower case letters, besides numbers and various signs).","metadata":{}},{"cell_type":"code","source":"pad_token = 'P'\nstart_token = 'S'\nend_token = 'E'\npad_token_idx = 59\nstart_token_idx = 60\nend_token_idx = 61","metadata":{"execution":{"iopub.status.busy":"2023-07-08T14:14:57.575858Z","iopub.execute_input":"2023-07-08T14:14:57.576554Z","iopub.status.idle":"2023-07-08T14:14:57.582067Z","shell.execute_reply.started":"2023-07-08T14:14:57.576520Z","shell.execute_reply":"2023-07-08T14:14:57.580774Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n    char_to_num = json.load(f)\n\n\nchar_to_num[pad_token] = pad_token_idx\nchar_to_num[start_token] = start_token_idx\nchar_to_num[end_token] = end_token_idx\n\nnum_to_char = {j:i for i,j in char_to_num.items()}\n\n\ninpdir = \"/kaggle/input/asl-fingerspelling\"\ndf = pd.read_csv(f'{inpdir}/train.csv')\n\nLPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\nPOSE = LPOSE + RPOSE\n\nRHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\nLHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\nPOSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n\nX = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\nY = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\nZ = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n\nSEL_COLS = X + Y + Z\nFRAME_LEN = 128\n\nX_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\nY_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\nZ_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n\nRHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\nLHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\nRPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\nLPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n\nprint(RPOSE_IDX)","metadata":{"execution":{"iopub.status.busy":"2023-07-08T14:15:54.908394Z","iopub.execute_input":"2023-07-08T14:15:54.908796Z","iopub.status.idle":"2023-07-08T14:15:55.087171Z","shell.execute_reply.started":"2023-07-08T14:15:54.908765Z","shell.execute_reply":"2023-07-08T14:15:55.086081Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[47, 48, 49, 50, 51, 99, 100, 101, 102, 103, 151, 152, 153, 154, 155]\n","output_type":"stream"}]},{"cell_type":"code","source":"def resize_pad(x):\n    if tf.shape(x)[0] < FRAME_LEN:\n        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0], [0, 0]]))\n        print(x)\n    else:\n        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n    return x\n\ndef translate_landmarks(landmarks, max_translation):\n    translation = tf.random.uniform(shape=tf.shape(landmarks), minval=-max_translation, maxval=max_translation)\n    translated_landmarks = landmarks + translation\n    return translated_landmarks\n\n# def scale_landmarks(landmarks, min_scale, max_scale):\n#     scale_factor = tf.random.uniform(shape=tf.shape(landmarks), minval=min_scale, maxval=max_scale)\n#     scaled_landmarks = landmarks * scale_factor\n#     return scaled_landmarks\n\ndef pre_process(x):\n\n    rhand = tf.gather(x, RHAND_IDX, axis=1)\n    lhand = tf.gather(x, LHAND_IDX, axis=1)\n    rpose = tf.gather(x, RPOSE_IDX, axis=1)\n    lpose = tf.gather(x, LPOSE_IDX, axis=1)\n\n    rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)\n    lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)\n\n    rnans = tf.math.count_nonzero(rnan_idx)\n    lnans = tf.math.count_nonzero(lnan_idx)\n\n    # For dominant hand\n    if rnans > lnans:\n        hand = lhand\n        pose = lpose\n\n        hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n        hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n        hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n        hand = tf.concat([1-hand_x, hand_y, hand_z], axis=1)\n\n        pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n        pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n        pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n        pose = tf.concat([1-pose_x, pose_y, pose_z], axis=1)\n    else:\n        hand = rhand\n        pose = rpose\n\n    hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n    hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n    hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n    hand = tf.concat([hand_x[..., tf.newaxis], hand_y[..., tf.newaxis], hand_z[..., tf.newaxis]], axis=-1)\n\n    mean = tf.math.reduce_mean(hand, axis=1)[:, tf.newaxis, :]\n    std = tf.math.reduce_std(hand, axis=1)[:, tf.newaxis, :]\n    hand = (hand - mean) / std\n\n    pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n    pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n    pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n    pose = tf.concat([pose_x[..., tf.newaxis], pose_y[..., tf.newaxis], pose_z[..., tf.newaxis]], axis=-1)\n\n    x = tf.concat([hand, pose], axis=1)\n    x = resize_pad(x)\n\n    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n    x = tf.reshape(x, (FRAME_LEN, len(LHAND_IDX) + len(LPOSE_IDX)))\n    return x","metadata":{"execution":{"iopub.status.busy":"2023-07-08T14:16:02.746178Z","iopub.execute_input":"2023-07-08T14:16:02.746563Z","iopub.status.idle":"2023-07-08T14:16:02.771062Z","shell.execute_reply.started":"2023-07-08T14:16:02.746534Z","shell.execute_reply":"2023-07-08T14:16:02.769388Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"table = tf.lookup.StaticHashTable(\n    initializer=tf.lookup.KeyValueTensorInitializer(\n        keys=list(char_to_num.keys()),\n        values=list(char_to_num.values()),\n    ),\n    default_value=tf.constant(-1),\n    name=\"class_weight\"\n)\n\ndef preprocess_fn(landmarks, phrase):\n    phrase = start_token + phrase + end_token\n    phrase = tf.strings.bytes_split(phrase)\n    phrase = table.lookup(phrase)\n    phrase = tf.pad(phrase, paddings=[[0, 64 - tf.shape(phrase)[0]]], mode = 'CONSTANT',\n                    constant_values = pad_token_idx)\n\n    # landmarksを前処理する\n    translated_landmarks = translate_landmarks(landmarks, max_translation=10)\n    #scaled_landmarks = scale_landmarks(landmarks, min_scale=0.8, max_scale=1.2)\n\n    # 前処理済みのlandmarksを結合する\n    #combined_landmarks = tf.concat([landmarks, translated_landmarks, scaled_landmarks], axis=1)\n    combined_landmarks = tf.concat([landmarks, translated_landmarks], axis=1)\n    return pre_process(combined_landmarks), phrase\n\ndef decode_fn(record_bytes):\n    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in SEL_COLS}\n    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n    features = tf.io.parse_single_example(record_bytes, schema)\n    phrase = features[\"phrase\"]\n    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in SEL_COLS])\n    landmarks = tf.transpose(landmarks)\n\n    return landmarks, phrase\n\ninpdir = \"/kaggle/input/aslfr-parquets-to-tfrecords-cleaned\"\ntffiles = df.file_id.map(lambda x: f'{inpdir}/tfds/{x}.tfrecord').unique()\n\nbatch_size = 32\nval_len = int(0.05 * len(tffiles))\n\ntrain_dataset = tf.data.TFRecordDataset(tffiles[val_len:]).map(decode_fn).map(preprocess_fn).shuffle(30000, reshuffle_each_iteration=True).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\nval_dataset = tf.data.TFRecordDataset(tffiles[:val_len]).map(decode_fn).map(preprocess_fn).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-08T14:16:17.717131Z","iopub.execute_input":"2023-07-08T14:16:17.717536Z","iopub.status.idle":"2023-07-08T14:16:23.163372Z","shell.execute_reply.started":"2023-07-08T14:16:17.717504Z","shell.execute_reply":"2023-07-08T14:16:23.162123Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Tensor(\"cond_1/Pad:0\", shape=(None, 26, 3), dtype=float32)\nTensor(\"cond_1/Pad:0\", shape=(None, 26, 3), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# The model","metadata":{}},{"cell_type":"code","source":"LPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\n\nPOSE = LPOSE + RPOSE\n\nRHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\nLHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\nPOSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n\nX = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\nY = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\nZ = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n\nSEL_COLS = X + Y + Z\nFRAME_LEN = 128\n\nX_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\nY_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\nZ_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n\nRHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\nLHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\nRPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\nLPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n\nprint('SEL_COLS size:' + str(len(SEL_COLS)))","metadata":{"execution":{"iopub.status.busy":"2023-07-08T14:16:26.244254Z","iopub.execute_input":"2023-07-08T14:16:26.244658Z","iopub.status.idle":"2023-07-08T14:16:26.262874Z","shell.execute_reply.started":"2023-07-08T14:16:26.244626Z","shell.execute_reply":"2023-07-08T14:16:26.261811Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"SEL_COLS size:156\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Here I implemented proper positional embeddings for both the encoder and the decoder.","metadata":{}},{"cell_type":"code","source":"class MLPBlock(layers.Layer):\n    def __init__(self, num_hid=256, num_layers=5):\n        super().__init__()\n        self.mlp = tf.keras.Sequential()\n        for _ in range(num_layers):\n            self.mlp.add(tf.keras.layers.Dense(num_hid, activation=tf.nn.gelu))\n        self.mlp.add(tf.keras.layers.Dense(num_hid))\n\n    def call(self, inputs):\n        return self.mlp(inputs)\n\n\nclass TokenEmbedding(layers.Layer):\n    def __init__(self, num_vocab=61, maxlen=50, num_hid=256, mlp_num_layers=5):\n        super().__init__()\n        self.num_hid = num_hid\n        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n        self.pos_emb = self.positional_encoding(maxlen - 1, num_hid)\n        self.mlp_block = MLPBlock(num_hid, num_layers=mlp_num_layers)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        x = self.emb(x) * tf.math.sqrt(tf.cast(self.num_hid, tf.float32))\n        x = x + self.pos_emb[:maxlen, :]\n        x = self.mlp_block(x)\n        return x\n\n    def positional_encoding(self, maxlen, num_hid):\n        positions = tf.range(maxlen, dtype=tf.float32)[..., tf.newaxis]\n        depth = num_hid // 2\n        angles = positions / tf.pow(10000, tf.range(0, depth, 1, dtype=tf.float32) / num_hid)  # depthのインクリメントを修正\n        pos_encoding = tf.concat([tf.sin(angles), tf.cos(angles)], axis=-1)\n        return pos_encoding\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-08T14:16:32.423300Z","iopub.execute_input":"2023-07-08T14:16:32.423718Z","iopub.status.idle":"2023-07-08T14:16:32.437358Z","shell.execute_reply.started":"2023-07-08T14:16:32.423676Z","shell.execute_reply":"2023-07-08T14:16:32.435994Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"class TokenEmbedding(layers.Layer):\n    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n        super().__init__()\n        self.num_hid = num_hid\n        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n        #self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n        '''\n        self.pos_emb = tf.math.divide(\n            self.positional_encoding(maxlen-1, num_hid),\n            tf.math.sqrt(tf.cast(num_hid, tf.float32)))\n        '''\n        self.pos_emb = self.positional_encoding(maxlen-1, num_hid)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        x = self.emb(x)\n        x = tf.math.multiply(x, tf.math.sqrt(tf.cast(self.num_hid, tf.float32)))\n        '''\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        return x + positions\n        '''\n        return x + self.pos_emb[:maxlen, :]\n    \n    def positional_encoding(self, maxlen, num_hid):\n        depth = num_hid/2\n        positions = tf.range(maxlen, dtype = tf.float32)[..., tf.newaxis]\n        depths = tf.range(depth, dtype = tf.float32)[np.newaxis, :]/depth\n        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n        angle_rads = tf.linalg.matmul(positions, angle_rates)\n        pos_encoding = tf.concat(\n          [tf.math.sin(angle_rads), tf.math.cos(angle_rads)],\n          axis=-1)\n        return pos_encoding\n\n\n","metadata":{}},{"cell_type":"code","source":"class LandmarkEmbedding(tf.keras.Model):\n    def __init__(self, num_hid=256, maxlen=100):\n        super(LandmarkEmbedding, self).__init__()\n        self.conv1 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn1 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu1 = tf.keras.layers.LeakyReLU()\n\n        self.conv2 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn2 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu2 = tf.keras.layers.LeakyReLU()\n        self.dropout2 = tf.keras.layers.Dropout(0.2)\n\n        self.conv3 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn3 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu3 = tf.keras.layers.LeakyReLU()\n        self.dropout3 = tf.keras.layers.Dropout(0.2)\n\n        self.conv4 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn4 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu4 = tf.keras.layers.LeakyReLU()\n        self.dropout4 = tf.keras.layers.Dropout(0.2)\n\n        self.sigmoid = tf.keras.layers.Activation('sigmoid')\n        self.pos_emb = self.positional_encoding(maxlen, num_hid)\n        self.maxlen = maxlen\n        self.num_hid = num_hid\n\n    def call(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.leaky_relu1(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.leaky_relu2(x)\n        x = self.dropout2(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.leaky_relu3(x)\n        x = self.dropout3(x)\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.leaky_relu4(x)\n        x = self.dropout4(x)\n        x = tf.math.multiply(x, tf.math.sqrt(tf.cast(self.num_hid, tf.float32)))\n        x = x + self.pos_emb\n\n        return self.sigmoid(x)\n\n    def positional_encoding(self, maxlen, num_hid):\n        depth = num_hid/2\n        positions = tf.range(maxlen, dtype=tf.float32)[..., tf.newaxis]\n        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth\n        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n        angle_rads = tf.linalg.matmul(positions, angle_rates)\n        pos_encoding = tf.concat([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], axis=-1)\n        return pos_encoding\n","metadata":{"execution":{"iopub.status.busy":"2023-07-08T14:16:36.971669Z","iopub.execute_input":"2023-07-08T14:16:36.972073Z","iopub.status.idle":"2023-07-08T14:16:36.991306Z","shell.execute_reply.started":"2023-07-08T14:16:36.972019Z","shell.execute_reply":"2023-07-08T14:16:36.990128Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n        # 追加した処理\n        self.dense1 = layers.Dense(embed_dim)\n        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout3 = layers.Dropout(rate)\n        self.dense2 = layers.Dense(feed_forward_dim, activation=\"relu\")\n        self.dense3 = layers.Dense(embed_dim)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n\n        # 追加した処理\n        dense1_output = self.dense1(out1)\n        dense1_output = self.dropout3(dense1_output, training=training)\n        out2 = self.layernorm3(out1 + dense1_output)\n\n        ffn_output = self.ffn(out2)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out2 + self.dense3(self.dense2(ffn_output)))\n","metadata":{"execution":{"iopub.status.busy":"2023-07-08T14:16:43.305576Z","iopub.execute_input":"2023-07-08T14:16:43.305987Z","iopub.status.idle":"2023-07-08T14:16:43.319239Z","shell.execute_reply.started":"2023-07-08T14:16:43.305955Z","shell.execute_reply":"2023-07-08T14:16:43.317895Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\n# TFRecordファイルのパス\ntfrecord_file = \"/kaggle/working/tfds/128822441.tfrecord\"\n\n# TFRecordデータセットの作成\ndataset = tf.data.TFRecordDataset([tfrecord_file])\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-07-08T14:16:48.443813Z","iopub.execute_input":"2023-07-08T14:16:48.444219Z","iopub.status.idle":"2023-07-08T14:16:48.463499Z","shell.execute_reply.started":"2023-07-08T14:16:48.444187Z","shell.execute_reply":"2023-07-08T14:16:48.461431Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Here I added the training flag to the TransformerDecoder's Dropout layers.","metadata":{}},{"cell_type":"code","source":"class TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n        super().__init__()\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n        self.self_att = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.self_dropout = layers.Dropout(0.5)\n        self.enc_dropout = layers.Dropout(0.1)\n        self.ffn_dropout = layers.Dropout(0.1)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n\n    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n        \"\"\"Masks the upper half of the dot product matrix in self attention.\n\n        This prevents flow of information from future tokens to current token.\n        1's in the lower triangle, counting from the lower right corner.\n        \"\"\"\n        i = tf.range(n_dest)[:, None]\n        j = tf.range(n_src)\n        m = i >= j - n_src + n_dest\n        mask = tf.cast(m, dtype)\n        mask = tf.reshape(mask, [1, n_dest, n_src])\n        mult = tf.concat(\n            [batch_size[..., tf.newaxis], tf.constant([1, 1], dtype=tf.int32)], 0\n        )\n        return tf.tile(mask, mult)\n\n    def call(self, enc_out, target, training):\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n        target_att = self.self_att(target, target, attention_mask=causal_mask)\n        target_norm = self.layernorm1(target + self.self_dropout(target_att, training = training))\n        enc_out = self.enc_att(target_norm, enc_out)\n        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out, training = training) + target_norm)\n        ffn_out = self.ffn(enc_out_norm)\n        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out, training = training))\n        return ffn_out_norm","metadata":{"execution":{"iopub.status.busy":"2023-07-08T14:16:55.732329Z","iopub.execute_input":"2023-07-08T14:16:55.732712Z","iopub.status.idle":"2023-07-08T14:16:55.748458Z","shell.execute_reply.started":"2023-07-08T14:16:55.732681Z","shell.execute_reply":"2023-07-08T14:16:55.747229Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"#### Here I made the passing of the training flag explicit.","metadata":{}},{"cell_type":"code","source":"class Transformer(keras.Model):\n    def __init__(\n        self,\n        num_hid=64,\n        num_head=2,\n        num_feed_forward=128,\n        source_maxlen=100,\n        target_maxlen=100,\n        num_layers_enc=4,\n        num_layers_dec=1,\n        num_classes=60,\n    ):\n        super().__init__()\n        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n        self.num_layers_enc = num_layers_enc\n        self.num_layers_dec = num_layers_dec\n        self.target_maxlen = target_maxlen\n        self.num_classes = num_classes\n\n        self.enc_input = LandmarkEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n        self.dec_input = TokenEmbedding(\n            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n        )\n\n        self.encoder = keras.Sequential(\n            [self.enc_input]\n            + [\n                TransformerEncoder(num_hid, num_head, num_feed_forward)\n                for _ in range(num_layers_enc)\n            ]\n        )\n\n        for i in range(num_layers_dec):\n            setattr(\n                self,\n                f\"dec_layer_{i}\",\n                TransformerDecoder(num_hid, num_head, num_feed_forward),\n            )\n\n        self.classifier = layers.Dense(num_classes)\n\n    def decode(self, enc_out, target, training):\n        y = self.dec_input(target)\n        for i in range(self.num_layers_dec):\n            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y, training)\n        return y\n\n    def call(self, inputs, training):\n        source = inputs[0]\n        target = inputs[1]\n        x = self.encoder(source, training)\n        y = self.decode(x, target, training)\n        return self.classifier(y)\n\n    @property\n    def metrics(self):\n        return [self.loss_metric]\n\n    def train_step(self, batch):\n        \"\"\"Processes one batch inside model.fit().\"\"\"\n        source = batch[0]\n        target = batch[1]\n\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        \n        dec_input = target[:, :-1]\n        dec_target = target[:, 1:]\n        with tf.GradientTape() as tape:\n            preds = self([source, dec_input])\n            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n            mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        self.loss_metric.update_state(loss)\n        return {\"loss\": self.loss_metric.result()}\n\n    def test_step(self, batch):        \n        source = batch[0]\n        target = batch[1]\n\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        \n        dec_input = target[:, :-1]\n        dec_target = target[:, 1:]\n        preds = self([source, dec_input])\n        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n        mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n        self.loss_metric.update_state(loss)\n        return {\"loss\": self.loss_metric.result()}\n\n    def generate(self, source, target_start_token_idx):\n        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n        bs = tf.shape(source)[0]\n        enc = self.encoder(source, training = False)\n        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n        dec_logits = []\n        for i in range(self.target_maxlen - 1):\n            dec_out = self.decode(enc, dec_input, training = False)\n            logits = self.classifier(dec_out)\n            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n            last_logit = logits[:, -1][..., tf.newaxis]\n            dec_logits.append(last_logit)\n            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n        return dec_input","metadata":{"execution":{"iopub.status.busy":"2023-07-08T14:17:02.591024Z","iopub.execute_input":"2023-07-08T14:17:02.591906Z","iopub.status.idle":"2023-07-08T14:17:02.616669Z","shell.execute_reply.started":"2023-07-08T14:17:02.591861Z","shell.execute_reply":"2023-07-08T14:17:02.615192Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# 正解率を計算するためのメトリクスを作成\ntrain_accuracy = tf.keras.metrics.CategoricalAccuracy()\nval_accuracy = tf.keras.metrics.CategoricalAccuracy()\n\n# 学習ループ内で正解率を更新するコールバックを定義\nclass AccuracyCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        train_acc = train_accuracy.result()\n        val_acc = val_accuracy.result()\n        print(f\"Epoch {epoch+1}: Train Accuracy = {train_acc}, Validation Accuracy = {val_acc}\")\n        # 正解率をリセット\n        train_accuracy.reset_states()\n        val_accuracy.reset_states()\n# val_lossが3回マイナスになった場合に学習を停止するコールバック\nclass EarlyStoppingCallback(tf.keras.callbacks.Callback):\n    def __init__(self, patience=7):\n        super(EarlyStoppingCallback, self).__init__()\n        self.patience = patience\n        self.min_val_loss = float('inf')\n        self.wait = 0\n\n    def on_epoch_end(self, epoch, logs=None):\n        val_loss = logs.get('val_loss')\n        if val_loss < self.min_val_loss:\n            self.min_val_loss = val_loss\n            self.wait = 0\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                self.model.stop_training = True\n                print(\"Training stopped due to early stopping.\")\n\nbatch = next(iter(val_dataset))\nidx_to_char = list(char_to_num.keys())\n\nmodel = Transformer(\n    num_hid=256,\n    num_head=4,\n    num_feed_forward=400,\n    source_maxlen = FRAME_LEN,\n    target_maxlen=64,\n    num_layers_enc=3,\n    num_layers_dec=2,\n    num_classes=62,\n)\n\n\nloss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1)\naccuracy_callback = AccuracyCallback()\noptimizer = keras.optimizers.Adam(0.0001)\n\n\n# モデルのコンパイル\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=[train_accuracy])","metadata":{"execution":{"iopub.status.busy":"2023-07-08T14:17:11.875930Z","iopub.execute_input":"2023-07-08T14:17:11.876709Z","iopub.status.idle":"2023-07-08T14:17:14.858481Z","shell.execute_reply.started":"2023-07-08T14:17:11.876673Z","shell.execute_reply":"2023-07-08T14:17:14.857405Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#modelアーキテクト\n#tf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True, show_layer_names=True, expand_nested=True, show_layer_activations=True)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# EarlyStoppingCallbackをコールバックリストに追加して学習を行う\nhistory = model.fit(train_dataset, verbose=2, validation_data=val_dataset, epochs=100,\n                    callbacks=[AccuracyCallback(), EarlyStoppingCallback()])","metadata":{"execution":{"iopub.status.busy":"2023-07-08T14:17:38.401010Z","iopub.execute_input":"2023-07-08T14:17:38.401711Z","iopub.status.idle":"2023-07-08T15:58:40.559850Z","shell.execute_reply.started":"2023-07-08T14:17:38.401678Z","shell.execute_reply":"2023-07-08T15:58:40.556376Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/100\nEpoch 1: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 292s - loss: 0.7992 - val_loss: 0.6548 - 292s/epoch - 192ms/step\nEpoch 2/100\nEpoch 2: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 235s - loss: 0.5599 - val_loss: 0.5155 - 235s/epoch - 154ms/step\nEpoch 3/100\nEpoch 3: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 224s - loss: 0.4898 - val_loss: 0.4791 - 224s/epoch - 148ms/step\nEpoch 4/100\nEpoch 4: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 226s - loss: 0.4594 - val_loss: 0.4608 - 226s/epoch - 149ms/step\nEpoch 5/100\nEpoch 5: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 247s - loss: 0.4399 - val_loss: 0.4483 - 247s/epoch - 162ms/step\nEpoch 6/100\nEpoch 6: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 250s - loss: 0.4261 - val_loss: 0.4364 - 250s/epoch - 165ms/step\nEpoch 7/100\nEpoch 7: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 263s - loss: 0.4152 - val_loss: 0.4327 - 263s/epoch - 173ms/step\nEpoch 8/100\nEpoch 8: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 252s - loss: 0.4056 - val_loss: 0.4242 - 252s/epoch - 166ms/step\nEpoch 9/100\nEpoch 9: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 259s - loss: 0.3980 - val_loss: 0.4228 - 259s/epoch - 170ms/step\nEpoch 10/100\nEpoch 10: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 253s - loss: 0.3908 - val_loss: 0.4191 - 253s/epoch - 167ms/step\nEpoch 11/100\nEpoch 11: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 250s - loss: 0.3842 - val_loss: 0.4147 - 250s/epoch - 165ms/step\nEpoch 12/100\nEpoch 12: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 254s - loss: 0.3784 - val_loss: 0.4133 - 254s/epoch - 167ms/step\nEpoch 13/100\nEpoch 13: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 251s - loss: 0.3729 - val_loss: 0.4111 - 251s/epoch - 165ms/step\nEpoch 14/100\nEpoch 14: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 253s - loss: 0.3676 - val_loss: 0.4149 - 253s/epoch - 166ms/step\nEpoch 15/100\nEpoch 15: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 251s - loss: 0.3626 - val_loss: 0.4099 - 251s/epoch - 165ms/step\nEpoch 16/100\nEpoch 16: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 248s - loss: 0.3575 - val_loss: 0.4098 - 248s/epoch - 163ms/step\nEpoch 17/100\nEpoch 17: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 252s - loss: 0.3531 - val_loss: 0.4091 - 252s/epoch - 166ms/step\nEpoch 18/100\nEpoch 18: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 249s - loss: 0.3488 - val_loss: 0.4120 - 249s/epoch - 164ms/step\nEpoch 19/100\nEpoch 19: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 252s - loss: 0.3443 - val_loss: 0.4108 - 252s/epoch - 166ms/step\nEpoch 20/100\nEpoch 20: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 246s - loss: 0.3404 - val_loss: 0.4111 - 246s/epoch - 162ms/step\nEpoch 21/100\nEpoch 21: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 253s - loss: 0.3362 - val_loss: 0.4121 - 253s/epoch - 166ms/step\nEpoch 22/100\nEpoch 22: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 251s - loss: 0.3328 - val_loss: 0.4117 - 251s/epoch - 165ms/step\nEpoch 23/100\nEpoch 23: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 244s - loss: 0.3285 - val_loss: 0.4158 - 244s/epoch - 160ms/step\nEpoch 24/100\nEpoch 24: Train Accuracy = 0.0, Validation Accuracy = 0.0\nTraining stopped due to early stopping.\n1520/1520 - 251s - loss: 0.3252 - val_loss: 0.4159 - 251s/epoch - 165ms/step\nCPU times: user 2h 16min 45s, sys: 8min 7s, total: 2h 24min 53s\nWall time: 1h 41min 2s\n","output_type":"stream"}]},{"cell_type":"code","source":"print(model.summary())","metadata":{"execution":{"iopub.status.busy":"2023-07-08T16:06:40.205165Z","iopub.execute_input":"2023-07-08T16:06:40.205599Z","iopub.status.idle":"2023-07-08T16:06:40.274618Z","shell.execute_reply.started":"2023-07-08T16:06:40.205566Z","shell.execute_reply":"2023-07-08T16:06:40.273757Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Model: \"transformer\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n landmark_embedding (Landmar  (None, 128, 256)         2387456   \n kEmbedding)                                                     \n                                                                 \n token_embedding (TokenEmbed  multiple                 410624    \n ding)                                                           \n                                                                 \n sequential_4 (Sequential)   (None, 128, 256)          6977888   \n                                                                 \n transformer_decoder (Transf  multiple                 2310800   \n ormerDecoder)                                                   \n                                                                 \n transformer_decoder_1 (Tran  multiple                 2310800   \n sformerDecoder)                                                 \n                                                                 \n dense_25 (Dense)            multiple                  15934     \n                                                                 \n=================================================================\nTotal params: 12,026,048\nTrainable params: 12,023,998\nNon-trainable params: 2,050\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])","metadata":{"execution":{"iopub.status.busy":"2023-07-08T16:06:53.129767Z","iopub.execute_input":"2023-07-08T16:06:53.130189Z","iopub.status.idle":"2023-07-08T16:06:53.919101Z","shell.execute_reply.started":"2023-07-08T16:06:53.130155Z","shell.execute_reply":"2023-07-08T16:06:53.918011Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7f7c4ff1b130>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"code","source":"batches = [batch for batch in val_dataset]\n\npreds_list = []\nground_truth_list = []\n\nfor batch in batches[:1]:\n    source = batch[0]\n    target = batch[1].numpy()\n    bs = tf.shape(source)[0]\n    preds = model.generate(source, start_token_idx)\n    preds = preds.numpy()\n\n    for i in range(bs):\n        target_text = \"\".join([idx_to_char[_] for _ in target[i, :]])\n        ground_truth_list.append(target_text.replace('P', ''))\n        prediction = \"\"\n        for idx in preds[i, :]:\n            prediction += idx_to_char[idx]\n            if idx == end_token_idx:\n                break\n        preds_list.append(prediction)\n\nfor i in range(10):\n    print(ground_truth_list[i])\n    print(preds_list[i])\n    print('\\n~~~\\n')","metadata":{"execution":{"iopub.status.busy":"2023-07-08T16:07:02.804718Z","iopub.execute_input":"2023-07-08T16:07:02.805130Z","iopub.status.idle":"2023-07-08T16:07:12.862665Z","shell.execute_reply.started":"2023-07-08T16:07:02.805097Z","shell.execute_reply":"2023-07-08T16:07:12.861652Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"S3 creekhouseE\nS33 creek houseE\n\n~~~\n\nSscales/kuhaylahE\nSsales/coxuhaillay.asE\n\n~~~\n\nS1383 william lanierE\nS1383 william lanierE\n\n~~~\n\nS988 franklin laneE\nS988 funn laneE\n\n~~~\n\nS6920 northeast 661st roadE\nS6920 northeast 661st roadE\n\n~~~\n\nSwww.freem.ne.jpE\nSwww.freem.me.jpE\n\n~~~\n\nShttps://jsi.is/hukuokaE\nShttps://jssi.is/hkuuuokaE\n\n~~~\n\nS239613 stolze streetE\nS239610 3 stold east roadE\n\n~~~\n\nS271097 bayshore boulevardE\nS271097 bay hore boulevardE\n\n~~~\n\nSfederico pearsonE\nSfederico pearonE\n\n~~~\n\n","output_type":"stream"}]},{"cell_type":"code","source":"ground_truth_processed = [ground_truth_list[i][1:-1] for i in range(len(ground_truth_list))]\npreds_list_processed = [preds_list[i][1:-1] for i in range(len(preds_list))]\nlev_dist = [lev.distance(ground_truth_processed[i], preds_list_processed[i])\n            for i in range(len(preds_list_processed))]\nN = [len(phrase) for phrase in ground_truth_processed]\n\nprint('Validation score: '+str((np.sum(N) - np.sum(lev_dist))/np.sum(N)))","metadata":{"execution":{"iopub.status.busy":"2023-07-08T16:07:18.809799Z","iopub.execute_input":"2023-07-08T16:07:18.810597Z","iopub.status.idle":"2023-07-08T16:07:18.828033Z","shell.execute_reply.started":"2023-07-08T16:07:18.810553Z","shell.execute_reply":"2023-07-08T16:07:18.826923Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Validation score: 0.8044596912521441\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Levenstein Distance Train","metadata":{}},{"cell_type":"code","source":"# Compute Levenstein Distances\ndef get_ld_train():\n    N = 100 if IS_INTERACTIVE else 1000\n    LD_TRAIN = []\n    for idx, (frames, phrase_true) in enumerate(zip(tqdm(X_train, total=N), y_train)):\n        # Predict Phrase and Convert to String\n        phrase_pred = predict_phrase(frames).numpy()\n        phrase_pred = outputs2phrase(phrase_pred)\n        # True Phrase Ordinal to String\n        phrase_true = outputs2phrase(phrase_true)\n        # Add Levenstein Distance\n        LD_TRAIN.append({\n            'phrase_true': phrase_true,\n            'phrase_pred': phrase_pred,\n            'levenshtein_distance': levenshtein(phrase_pred, phrase_true),\n        })\n        # Take subset in interactive mode\n        if idx == N:\n            break\n            \n    # Convert to DataFrame\n    LD_TRAIN_DF = pd.DataFrame(LD_TRAIN)\n    \n    return LD_TRAIN_DF","metadata":{"execution":{"iopub.status.busy":"2023-07-08T16:07:27.893158Z","iopub.execute_input":"2023-07-08T16:07:27.893577Z","iopub.status.idle":"2023-07-08T16:07:27.902477Z","shell.execute_reply.started":"2023-07-08T16:07:27.893545Z","shell.execute_reply":"2023-07-08T16:07:27.901105Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# TFLiteModel","metadata":{}},{"cell_type":"code","source":" class TFLiteModel(tf.Module):\n    def __init__(self, model):\n        super(TFLiteModel, self).__init__()\n        self.target_start_token_idx = start_token_idx\n        self.target_end_token_idx = end_token_idx\n        # Load the feature generation and main models\n        self.model = model\n\n    @tf.function(input_signature=[tf.TensorSpec(shape=[None, len(SEL_COLS)], dtype=tf.float32, name='inputs')])\n    def __call__(self, inputs, training=False):\n        # Preprocess Data\n        x = tf.cast(inputs, tf.float32)\n        x = x[None]\n        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, len(SEL_COLS))), lambda: tf.identity(x))\n        x = x[0]\n        x = pre_process(x)\n        x = x[None]\n        x = self.model.generate(x, self.target_start_token_idx)\n        x = x[0]\n        idx = tf.argmax(tf.cast(tf.equal(x, self.target_end_token_idx), tf.int32))\n        idx = tf.where(tf.math.less(idx, 1), tf.constant(2, dtype=tf.int64), idx)\n        x = x[1:idx]\n        x = tf.one_hot(x, 59)\n        return {'outputs': x}\n\ntflitemodel_base = TFLiteModel(model)","metadata":{"execution":{"iopub.status.busy":"2023-07-08T16:07:32.316163Z","iopub.execute_input":"2023-07-08T16:07:32.316553Z","iopub.status.idle":"2023-07-08T16:07:32.331226Z","shell.execute_reply.started":"2023-07-08T16:07:32.316523Z","shell.execute_reply":"2023-07-08T16:07:32.329824Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"model.save_weights(\"model.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-07-08T16:07:36.011635Z","iopub.execute_input":"2023-07-08T16:07:36.012454Z","iopub.status.idle":"2023-07-08T16:07:36.322119Z","shell.execute_reply.started":"2023-07-08T16:07:36.012414Z","shell.execute_reply":"2023-07-08T16:07:36.321062Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\nkeras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]#, tf.lite.OpsSet.SELECT_TF_OPS]\ntflite_model = keras_model_converter.convert()\nwith open('/kaggle/working/model.tflite', 'wb') as f:\n    f.write(tflite_model)\n\ninfargs = {\"selected_columns\" : SEL_COLS}\n\nwith open('inference_args.json', \"w\") as json_file:\n    json.dump(infargs, json_file)","metadata":{"execution":{"iopub.status.busy":"2023-07-08T16:07:39.409424Z","iopub.execute_input":"2023-07-08T16:07:39.409836Z","iopub.status.idle":"2023-07-08T16:10:11.906232Z","shell.execute_reply.started":"2023-07-08T16:07:39.409802Z","shell.execute_reply":"2023-07-08T16:10:11.904711Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Tensor(\"cond_2/Pad:0\", shape=(None, 26, 3), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"code","source":"!zip submission.zip  './model.tflite' './inference_args.json'","metadata":{"execution":{"iopub.status.busy":"2023-07-08T16:10:16.630397Z","iopub.execute_input":"2023-07-08T16:10:16.630843Z","iopub.status.idle":"2023-07-08T16:10:21.682173Z","shell.execute_reply.started":"2023-07-08T16:10:16.630799Z","shell.execute_reply":"2023-07-08T16:10:21.680624Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"updating: model.tflite (deflated 14%)\nupdating: inference_args.json (deflated 85%)\n","output_type":"stream"}]},{"cell_type":"code","source":"interpreter = tf.lite.Interpreter(\"model.tflite\")\n\nREQUIRED_SIGNATURE = \"serving_default\"\nREQUIRED_OUTPUT = \"outputs\"\n\nwith open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n    character_map = json.load(f)\nrev_character_map = {j:i for i,j in character_map.items()}\n\nfound_signatures = list(interpreter.get_signature_list().keys())\n\nif REQUIRED_SIGNATURE not in found_signatures:\n    raise KernelEvalException('Required input signature not found.')\n\nprediction_fn = interpreter.get_signature_runner(\"serving_default\")\noutput = prediction_fn(inputs=batch[0][0])\nprediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\nprint(prediction_str)","metadata":{"execution":{"iopub.status.busy":"2023-07-08T16:18:19.218209Z","iopub.execute_input":"2023-07-08T16:18:19.219520Z","iopub.status.idle":"2023-07-08T16:18:20.590469Z","shell.execute_reply.started":"2023-07-08T16:18:19.219474Z","shell.execute_reply":"2023-07-08T16:18:20.589104Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"888 routh 33\n","output_type":"stream"}]}]}