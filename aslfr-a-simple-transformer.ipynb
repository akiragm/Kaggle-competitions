{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n1. I used two transformer layer in the encoder instead of four.\n2. I used four attention heads instead of two.\n3. I used new tokens for SOS, EOS, and padding (very minor since Rohith used rare tokens for these purposes, but still- more 'correct').\n2. I fixed a bug (probably?) in the decoder's dropout layers, which did not have the training flag, resulting in dropout during inference. This change gave a nice bump in the score.\n3. I made the passing of the training flag explicit. I know it can be implicit since it is a kwarg, but explicit passing makes the whole thing more straightforward and maybe fix another one or two training-flag-related bugs along the way.\n4. I changed the positional encoding in the decoder from tf.keras.layers.Embedding to proper positional embeddings (i.e., the usual sines and cosines usually used for this purpose). This had a significant impact.\n5. I added positional embedding to the encoder. This, too, had a significant impact.\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.metrics import Accuracy\nfrom skimage.transform import resize\nfrom sklearn.model_selection import train_test_split\nimport json\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport Levenshtein as lev\nimport os\nimport gc","metadata":{"execution":{"iopub.status.busy":"2023-07-10T11:49:07.244232Z","iopub.execute_input":"2023-07-10T11:49:07.244660Z","iopub.status.idle":"2023-07-10T11:49:16.019311Z","shell.execute_reply.started":"2023-07-10T11:49:07.244624Z","shell.execute_reply":"2023-07-10T11:49:16.018341Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"inpdir = \"/kaggle/input/asl-fingerspelling\"\ndf = pd.read_csv(f'{inpdir}/train.csv')\ndf[\"phrase_bytes\"] = df[\"phrase\"].map(lambda x: x.encode(\"utf-8\"))\ndisplay(df.head())","metadata":{}},{"cell_type":"markdown","source":"train_landmarks = pd.read_parquet('/kaggle/input/asl-fingerspelling/train_landmarks/1019715464.parquet')\nkeys = train_landmarks.keys()[1:]\ntrain_landmarks.head()","metadata":{}},{"cell_type":"markdown","source":"# TFRecord","metadata":{}},{"cell_type":"markdown","source":"LPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\nPOSE = LPOSE + RPOSE\n\nRHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\nLHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\nPOSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n\nX = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\nY = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\nZ = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n\nSEL_COLS = X + Y + Z\nFRAME_LEN = 128\n\nX_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\nY_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\nZ_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n\nRHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\nLHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\nRPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\nLPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n\nprint('SEL_COLS size:' + str(len(SEL_COLS)))","metadata":{}},{"cell_type":"markdown","source":"def load_relevant_data_subset(pq_path):\n    return pd.read_parquet(pq_path, columns=SEL_COLS)\n\ncounter = 0\nfor file_id in tqdm(df.file_id.unique()):\n    \n    print(counter)\n    counter+=1\n    \n    pqfile = f\"{inpdir}/train_landmarks/{file_id}.parquet\"\n    if not os.path.isdir(\"tfds\"): os.mkdir(\"tfds\")\n    tffile = f\"tfds/{file_id}.tfrecord\"\n    seq_refs = df.loc[df.file_id == file_id]\n    seqs = load_relevant_data_subset(pqfile)\n    seqs_numpy = seqs.to_numpy()\n    with tf.io.TFRecordWriter(tffile) as file_writer:\n        for seq_id, phrase in zip(seq_refs.sequence_id, seq_refs.phrase_bytes):\n            frames = seqs_numpy[seqs.index == seq_id]\n            \n            r_nonan = np.sum(np.sum(np.isnan(frames[:, RHAND_IDX]), axis = 1) == 0)\n            l_nonan = np.sum(np.sum(np.isnan(frames[:, LHAND_IDX]), axis = 1) == 0)\n            no_nan = max(r_nonan, l_nonan)\n            \n            if 2*len(phrase)<no_nan:\n                features = {SEL_COLS[i]: tf.train.Feature(\n                    float_list=tf.train.FloatList(value=frames[:, i])) for i in range(len(SEL_COLS))}\n                features[\"phrase\"] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[phrase]))\n                record_bytes = tf.train.Example(features=tf.train.Features(feature=features)).SerializeToString()\n                file_writer.write(record_bytes)","metadata":{}},{"cell_type":"markdown","source":"# Data loading","metadata":{}},{"cell_type":"markdown","source":"#### Here I use new tokens for padding, start and end of sentences. (Capitals are good since the original phrases have only lower case letters, besides numbers and various signs).","metadata":{}},{"cell_type":"code","source":"pad_token = 'P'\nstart_token = 'S'\nend_token = 'E'\npad_token_idx = 59\nstart_token_idx = 60\nend_token_idx = 61","metadata":{"execution":{"iopub.status.busy":"2023-07-10T11:49:23.883640Z","iopub.execute_input":"2023-07-10T11:49:23.885019Z","iopub.status.idle":"2023-07-10T11:49:23.890361Z","shell.execute_reply.started":"2023-07-10T11:49:23.884970Z","shell.execute_reply":"2023-07-10T11:49:23.889279Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n    char_to_num = json.load(f)\n\n\nchar_to_num[pad_token] = pad_token_idx\nchar_to_num[start_token] = start_token_idx\nchar_to_num[end_token] = end_token_idx\n\nnum_to_char = {j:i for i,j in char_to_num.items()}\n\n\ninpdir = \"/kaggle/input/asl-fingerspelling\"\ndf = pd.read_csv(f'{inpdir}/train.csv')\n\nLPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\nPOSE = LPOSE + RPOSE\n\nRHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\nLHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\nPOSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n\nX = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\nY = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\nZ = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n\nSEL_COLS = X + Y + Z\nFRAME_LEN = 128\n\nX_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\nY_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\nZ_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n\nRHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\nLHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\nRPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\nLPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n\nprint(RPOSE_IDX)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T11:49:23.893567Z","iopub.execute_input":"2023-07-10T11:49:23.894363Z","iopub.status.idle":"2023-07-10T11:49:24.056554Z","shell.execute_reply.started":"2023-07-10T11:49:23.894325Z","shell.execute_reply":"2023-07-10T11:49:24.055608Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[47, 48, 49, 50, 51, 99, 100, 101, 102, 103, 151, 152, 153, 154, 155]\n","output_type":"stream"}]},{"cell_type":"code","source":"def resize_pad(x):\n    if tf.shape(x)[0] < FRAME_LEN:\n        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0], [0, 0]]))\n        print(x)\n    else:\n        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n    return x\n\ndef translate_landmarks(landmarks, max_translation):\n    translation = tf.random.uniform(shape=tf.shape(landmarks), minval=-max_translation, maxval=max_translation)\n    translated_landmarks = landmarks + translation\n    return translated_landmarks\n\n# def scale_landmarks(landmarks, min_scale, max_scale):\n#     scale_factor = tf.random.uniform(shape=tf.shape(landmarks), minval=min_scale, maxval=max_scale)\n#     scaled_landmarks = landmarks * scale_factor\n#     return scaled_landmarks\n\ndef pre_process(x):\n\n    rhand = tf.gather(x, RHAND_IDX, axis=1)\n    lhand = tf.gather(x, LHAND_IDX, axis=1)\n    rpose = tf.gather(x, RPOSE_IDX, axis=1)\n    lpose = tf.gather(x, LPOSE_IDX, axis=1)\n\n    rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)\n    lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)\n\n    rnans = tf.math.count_nonzero(rnan_idx)\n    lnans = tf.math.count_nonzero(lnan_idx)\n\n    # For dominant hand\n    if rnans > lnans:\n        hand = lhand\n        pose = lpose\n\n        hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n        hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n        hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n        hand = tf.concat([1-hand_x, hand_y, hand_z], axis=1)\n\n        pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n        pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n        pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n        pose = tf.concat([1-pose_x, pose_y, pose_z], axis=1)\n    else:\n        hand = rhand\n        pose = rpose\n\n    hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n    hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n    hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n    hand = tf.concat([hand_x[..., tf.newaxis], hand_y[..., tf.newaxis], hand_z[..., tf.newaxis]], axis=-1)\n\n    mean = tf.math.reduce_mean(hand, axis=1)[:, tf.newaxis, :]\n    std = tf.math.reduce_std(hand, axis=1)[:, tf.newaxis, :]\n    hand = (hand - mean) / std\n\n    pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n    pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n    pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n    pose = tf.concat([pose_x[..., tf.newaxis], pose_y[..., tf.newaxis], pose_z[..., tf.newaxis]], axis=-1)\n\n    x = tf.concat([hand, pose], axis=1)\n    x = resize_pad(x)\n\n    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n    x = tf.reshape(x, (FRAME_LEN, len(LHAND_IDX) + len(LPOSE_IDX)))\n    return x","metadata":{"execution":{"iopub.status.busy":"2023-07-10T11:49:24.058649Z","iopub.execute_input":"2023-07-10T11:49:24.059328Z","iopub.status.idle":"2023-07-10T11:49:24.081154Z","shell.execute_reply.started":"2023-07-10T11:49:24.059292Z","shell.execute_reply":"2023-07-10T11:49:24.080102Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"table = tf.lookup.StaticHashTable(\n    initializer=tf.lookup.KeyValueTensorInitializer(\n        keys=list(char_to_num.keys()),\n        values=list(char_to_num.values()),\n    ),\n    default_value=tf.constant(-1),\n    name=\"class_weight\"\n)\n\ndef preprocess_fn(landmarks, phrase):\n    phrase = start_token + phrase + end_token\n    phrase = tf.strings.bytes_split(phrase)\n    phrase = table.lookup(phrase)\n    phrase = tf.pad(phrase, paddings=[[0, 64 - tf.shape(phrase)[0]]], mode = 'CONSTANT',\n                    constant_values = pad_token_idx)\n\n    # landmarksを前処理する\n    translated_landmarks = translate_landmarks(landmarks, max_translation=10)\n    #scaled_landmarks = scale_landmarks(landmarks, min_scale=0.8, max_scale=1.2)\n\n    # 前処理済みのlandmarksを結合する\n    #combined_landmarks = tf.concat([landmarks, translated_landmarks, scaled_landmarks], axis=1)\n    combined_landmarks = tf.concat([landmarks, translated_landmarks], axis=1)\n    return pre_process(combined_landmarks), phrase\n\ndef decode_fn(record_bytes):\n    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in SEL_COLS}\n    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n    features = tf.io.parse_single_example(record_bytes, schema)\n    phrase = features[\"phrase\"]\n    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in SEL_COLS])\n    landmarks = tf.transpose(landmarks)\n\n    return landmarks, phrase\n\ninpdir = \"/kaggle/input/aslfr-parquets-to-tfrecords-cleaned\"\ntffiles = df.file_id.map(lambda x: f'{inpdir}/tfds/{x}.tfrecord').unique()\n\nbatch_size = 32\nval_len = int(0.05 * len(tffiles))\n\ntrain_dataset = tf.data.TFRecordDataset(tffiles[val_len:]).map(decode_fn).map(preprocess_fn).shuffle(30000, reshuffle_each_iteration=True).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\nval_dataset = tf.data.TFRecordDataset(tffiles[:val_len]).map(decode_fn).map(preprocess_fn).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T11:49:24.083070Z","iopub.execute_input":"2023-07-10T11:49:24.083466Z","iopub.status.idle":"2023-07-10T11:49:29.226732Z","shell.execute_reply.started":"2023-07-10T11:49:24.083434Z","shell.execute_reply":"2023-07-10T11:49:29.225785Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Tensor(\"cond_1/Pad:0\", shape=(None, 26, 3), dtype=float32)\nTensor(\"cond_1/Pad:0\", shape=(None, 26, 3), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# The model","metadata":{}},{"cell_type":"code","source":"LPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\n\nPOSE = LPOSE + RPOSE\n\nRHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\nLHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\nPOSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n\nX = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\nY = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\nZ = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n\nSEL_COLS = X + Y + Z\nFRAME_LEN = 128\n\nX_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\nY_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\nZ_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n\nRHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\nLHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\nRPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\nLPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n\nprint('SEL_COLS size:' + str(len(SEL_COLS)))","metadata":{"execution":{"iopub.status.busy":"2023-07-10T11:49:29.229598Z","iopub.execute_input":"2023-07-10T11:49:29.229970Z","iopub.status.idle":"2023-07-10T11:49:29.244390Z","shell.execute_reply.started":"2023-07-10T11:49:29.229943Z","shell.execute_reply":"2023-07-10T11:49:29.243464Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"SEL_COLS size:156\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Here I implemented proper positional embeddings for both the encoder and the decoder.","metadata":{}},{"cell_type":"code","source":"class MLPBlock(keras.layers.Layer):\n    def __init__(self, num_hid=256, num_layers=5):\n        super().__init__()\n        self.mlp = tf.\n        ()\n        for _ in range(num_layers):\n            self.mlp.add(tf.keras.layers.Dense(num_hid, activation=tf.nn.gelu))\n        self.mlp.add(tf.keras.layers.Dense(num_hid))\n\n    def call(self, inputs):\n        return self.mlp(inputs)\n\n\nclass TokenEmbedding(keras.layers.Layer):\n    def __init__(self, num_vocab=61, maxlen=50, num_hid=256, mlp_num_layers=5):\n        super().__init__()\n        self.num_hid = num_hid\n        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n        self.pos_emb = self.positional_encoding(maxlen - 1, num_hid)\n        self.mlp_block = MLPBlock(num_hid, num_layers=mlp_num_layers)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        x = self.emb(x) * tf.math.sqrt(tf.cast(self.num_hid, tf.float32))\n        x = x + self.pos_emb[:maxlen, :]\n        x = self.mlp_block(x)\n        return x\n\n    def positional_encoding(self, maxlen, num_hid):\n        positions = tf.range(maxlen, dtype=tf.float32)[..., tf.newaxis]\n        depth = num_hid // 2\n        angles = positions / tf.pow(10000, tf.range(0, depth, 1, dtype=tf.float32) / num_hid)  # depthのインクリメントを修正\n        pos_encoding = tf.concat([tf.sin(angles), tf.cos(angles)], axis=-1)\n        return pos_encoding\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T11:49:29.246156Z","iopub.execute_input":"2023-07-10T11:49:29.246813Z","iopub.status.idle":"2023-07-10T11:49:29.261795Z","shell.execute_reply.started":"2023-07-10T11:49:29.246781Z","shell.execute_reply":"2023-07-10T11:49:29.260882Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"class TokenEmbedding(layers.Layer):\n    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n        super().__init__()\n        self.num_hid = num_hid\n        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n        #self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n        '''\n        self.pos_emb = tf.math.divide(\n            self.positional_encoding(maxlen-1, num_hid),\n            tf.math.sqrt(tf.cast(num_hid, tf.float32)))\n        '''\n        self.pos_emb = self.positional_encoding(maxlen-1, num_hid)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        x = self.emb(x)\n        x = tf.math.multiply(x, tf.math.sqrt(tf.cast(self.num_hid, tf.float32)))\n        '''\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        return x + positions\n        '''\n        return x + self.pos_emb[:maxlen, :]\n    \n    def positional_encoding(self, maxlen, num_hid):\n        depth = num_hid/2\n        positions = tf.range(maxlen, dtype = tf.float32)[..., tf.newaxis]\n        depths = tf.range(depth, dtype = tf.float32)[np.newaxis, :]/depth\n        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n        angle_rads = tf.linalg.matmul(positions, angle_rates)\n        pos_encoding = tf.concat(\n          [tf.math.sin(angle_rads), tf.math.cos(angle_rads)],\n          axis=-1)\n        return pos_encoding\n\n\n","metadata":{}},{"cell_type":"code","source":"class LandmarkEmbedding(tf.keras.Model):\n    def __init__(self, num_hid=256, maxlen=100):\n        super(LandmarkEmbedding, self).__init__()\n        self.conv1 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn1 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu1 = tf.keras.layers.LeakyReLU()\n\n        self.conv2 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn2 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu2 = tf.keras.layers.LeakyReLU()\n        self.dropout2 = tf.keras.layers.Dropout(0.2)\n\n        self.conv3 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn3 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu3 = tf.keras.layers.LeakyReLU()\n        self.dropout3 = tf.keras.layers.Dropout(0.2)\n\n        self.conv4 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.bn4 = tf.keras.layers.BatchNormalization()\n        self.leaky_relu4 = tf.keras.layers.LeakyReLU()\n        self.dropout4 = tf.keras.layers.Dropout(0.2)\n\n        self.sigmoid = tf.keras.layers.Activation('sigmoid')\n        self.pos_emb = self.positional_encoding(maxlen, num_hid)\n        self.maxlen = maxlen\n        self.num_hid = num_hid\n\n    def call(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.leaky_relu1(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.leaky_relu2(x)\n        x = self.dropout2(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.leaky_relu3(x)\n        x = self.dropout3(x)\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.leaky_relu4(x)\n        x = self.dropout4(x)\n        x = tf.math.multiply(x, tf.math.sqrt(tf.cast(self.num_hid, tf.float32)))\n        x = x + self.pos_emb\n\n        return self.sigmoid(x)\n\n    def positional_encoding(self, maxlen, num_hid):\n        depth = num_hid/2\n        positions = tf.range(maxlen, dtype=tf.float32)[..., tf.newaxis]\n        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth\n        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n        angle_rads = tf.linalg.matmul(positions, angle_rates)\n        pos_encoding = tf.concat([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], axis=-1)\n        return pos_encoding\n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T11:49:29.263448Z","iopub.execute_input":"2023-07-10T11:49:29.263823Z","iopub.status.idle":"2023-07-10T11:49:29.282166Z","shell.execute_reply.started":"2023-07-10T11:49:29.263792Z","shell.execute_reply":"2023-07-10T11:49:29.281256Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class TransformerEncoder(keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T11:49:29.283757Z","iopub.execute_input":"2023-07-10T11:49:29.284076Z","iopub.status.idle":"2023-07-10T11:49:29.299451Z","shell.execute_reply.started":"2023-07-10T11:49:29.284046Z","shell.execute_reply":"2023-07-10T11:49:29.298549Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"#### Here I added the training flag to the TransformerDecoder's Dropout layers.","metadata":{}},{"cell_type":"code","source":"class TransformerDecoder(keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n        super().__init__()\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n        self.self_att = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.self_dropout = layers.Dropout(0.5)\n        self.enc_dropout = layers.Dropout(0.1)\n        self.ffn_dropout = layers.Dropout(0.1)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n\n    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n        \"\"\"Masks the upper half of the dot product matrix in self attention.\n\n        This prevents flow of information from future tokens to current token.\n        1's in the lower triangle, counting from the lower right corner.\n        \"\"\"\n        i = tf.range(n_dest)[:, None]\n        j = tf.range(n_src)\n        m = i >= j - n_src + n_dest\n        mask = tf.cast(m, dtype)\n        mask = tf.reshape(mask, [1, n_dest, n_src])\n        mult = tf.concat(\n            [batch_size[..., tf.newaxis], tf.constant([1, 1], dtype=tf.int32)], 0\n        )\n        return tf.tile(mask, mult)\n\n    def call(self, enc_out, target, training):\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n        target_att = self.self_att(target, target, attention_mask=causal_mask)\n        target_norm = self.layernorm1(target + self.self_dropout(target_att, training = training))\n        enc_out = self.enc_att(target_norm, enc_out)\n        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out, training = training) + target_norm)\n        ffn_out = self.ffn(enc_out_norm)\n        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out, training = training))\n        return ffn_out_norm","metadata":{"execution":{"iopub.status.busy":"2023-07-10T11:49:29.301031Z","iopub.execute_input":"2023-07-10T11:49:29.301374Z","iopub.status.idle":"2023-07-10T11:49:29.317612Z","shell.execute_reply.started":"2023-07-10T11:49:29.301318Z","shell.execute_reply":"2023-07-10T11:49:29.316533Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"#### Here I made the passing of the training flag explicit.","metadata":{}},{"cell_type":"code","source":"class Transformer(tf.keras.Model):\n    def __init__(\n        self,\n        num_hid=64,\n        num_head=2,\n        num_feed_forward=128,\n        source_maxlen=100,\n        target_maxlen=100,\n        num_layers_enc=4,\n        num_layers_dec=1,\n        num_classes=60,\n    ):\n        super().__init__()\n        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n        self.num_layers_enc = num_layers_enc\n        self.num_layers_dec = num_layers_dec\n        self.target_maxlen = target_maxlen\n        self.num_classes = num_classes\n\n        self.enc_input = LandmarkEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n        self.dec_input = TokenEmbedding(\n            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n        )\n\n        self.encoder = keras.Sequential(\n            [self.enc_input]\n            + [\n                TransformerEncoder(num_hid, num_head, num_feed_forward)\n                for _ in range(num_layers_enc)\n            ]\n        )\n\n        for i in range(num_layers_dec):\n            setattr(\n                self,\n                f\"dec_layer_{i}\",\n                TransformerDecoder(num_hid, num_head, num_feed_forward),\n            )\n\n        self.classifier = layers.Dense(num_classes)\n\n    def decode(self, enc_out, target, training):\n        y = self.dec_input(target)\n        for i in range(self.num_layers_dec):\n            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y, training)\n        return y\n\n    def call(self, inputs, training):\n        source = inputs[0]\n        target = inputs[1]\n        x = self.encoder(source, training)\n        y = self.decode(x, target, training)\n        return self.classifier(y)\n\n    @property\n    def metrics(self):\n        return [self.loss_metric]\n\n    def train_step(self, batch):\n        \"\"\"Processes one batch inside model.fit().\"\"\"\n        source = batch[0]\n        target = batch[1]\n\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        \n        dec_input = target[:, :-1]\n        dec_target = target[:, 1:]\n        with tf.GradientTape() as tape:\n            preds = self([source, dec_input])\n            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n            mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        self.loss_metric.update_state(loss)\n        return {\"loss\": self.loss_metric.result()}\n\n    def test_step(self, batch):        \n        source = batch[0]\n        target = batch[1]\n\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        \n        dec_input = target[:, :-1]\n        dec_target = target[:, 1:]\n        preds = self([source, dec_input])\n        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n        mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n        self.loss_metric.update_state(loss)\n        return {\"loss\": self.loss_metric.result()}\n\n    def generate(self, source, target_start_token_idx):\n        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n        bs = tf.shape(source)[0]\n        enc = self.encoder(source, training = False)\n        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n        dec_logits = []\n        for i in range(self.target_maxlen - 1):\n            dec_out = self.decode(enc, dec_input, training = False)\n            logits = self.classifier(dec_out)\n            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n            last_logit = logits[:, -1][..., tf.newaxis]\n            dec_logits.append(last_logit)\n            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n        return dec_input","metadata":{"execution":{"iopub.status.busy":"2023-07-10T11:49:29.318874Z","iopub.execute_input":"2023-07-10T11:49:29.319263Z","iopub.status.idle":"2023-07-10T11:49:29.341676Z","shell.execute_reply.started":"2023-07-10T11:49:29.319231Z","shell.execute_reply":"2023-07-10T11:49:29.340774Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# 正解率を計算するためのメトリクスを作成\ntrain_accuracy = tf.keras.metrics.CategoricalAccuracy()\nval_accuracy = tf.keras.metrics.CategoricalAccuracy()\n\n# 学習ループ内で正解率を更新するコールバックを定義\nclass AccuracyCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        train_acc = train_accuracy.result()\n        val_acc = val_accuracy.result()\n        print(f\"Epoch {epoch+1}: Train Accuracy = {train_acc}, Validation Accuracy = {val_acc}\")\n        # 正解率をリセット\n        train_accuracy.reset_states()\n        val_accuracy.reset_states()\n# val_lossが3回マイナスになった場合に学習を停止するコールバック\nclass EarlyStoppingCallback(tf.keras.callbacks.Callback):\n    def __init__(self, patience=7):\n        super(EarlyStoppingCallback, self).__init__()\n        self.patience = patience\n        self.min_val_loss = float('inf')\n        self.wait = 0\n\n    def on_epoch_end(self, epoch, logs=None):\n        val_loss = logs.get('val_loss')\n        if val_loss < self.min_val_loss:\n            self.min_val_loss = val_loss\n            self.wait = 0\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                self.model.stop_training = True\n                print(\"Training stopped due to early stopping.\")\n\nbatch = next(iter(val_dataset))\nidx_to_char = list(char_to_num.keys())\n\nmodel = Transformer(\n    num_hid=200,\n    num_head=4,\n    num_feed_forward=400,\n    source_maxlen = FRAME_LEN,\n    target_maxlen=64,\n    num_layers_enc=2,\n    num_layers_dec=1,\n    num_classes=62,\n)\n\n\n\nloss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1)\naccuracy_callback = AccuracyCallback()\noptimizer = keras.optimizers.Adam(0.0001)\n\n\n# モデルのコンパイル\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=[train_accuracy])","metadata":{"execution":{"iopub.status.busy":"2023-07-10T11:49:29.346168Z","iopub.execute_input":"2023-07-10T11:49:29.346497Z","iopub.status.idle":"2023-07-10T11:49:32.225482Z","shell.execute_reply.started":"2023-07-10T11:49:29.346465Z","shell.execute_reply":"2023-07-10T11:49:32.224537Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#modelアーキテクト\n#tf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True, show_layer_names=True, expand_nested=True, show_layer_activations=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T11:49:32.226990Z","iopub.execute_input":"2023-07-10T11:49:32.227331Z","iopub.status.idle":"2023-07-10T11:49:32.232364Z","shell.execute_reply.started":"2023-07-10T11:49:32.227299Z","shell.execute_reply":"2023-07-10T11:49:32.231285Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"%%time\n# EarlyStoppingCallbackをコールバックリストに追加して学習を行う\nhistory = model.fit(train_dataset, verbose=2, validation_data=val_dataset, epochs=100,\n                    callbacks=[AccuracyCallback(), EarlyStoppingCallback()])","metadata":{"execution":{"iopub.status.busy":"2023-07-10T11:49:32.234066Z","iopub.execute_input":"2023-07-10T11:49:32.234446Z","iopub.status.idle":"2023-07-10T13:25:18.739046Z","shell.execute_reply.started":"2023-07-10T11:49:32.234415Z","shell.execute_reply":"2023-07-10T13:25:18.738008Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/100\nEpoch 1: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 265s - loss: 0.7717 - val_loss: 0.6204 - 265s/epoch - 174ms/step\nEpoch 2/100\nEpoch 2: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 201s - loss: 0.5482 - val_loss: 0.5152 - 201s/epoch - 132ms/step\nEpoch 3/100\nEpoch 3: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 209s - loss: 0.4849 - val_loss: 0.4741 - 209s/epoch - 138ms/step\nEpoch 4/100\nEpoch 4: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 195s - loss: 0.4561 - val_loss: 0.4594 - 195s/epoch - 128ms/step\nEpoch 5/100\nEpoch 5: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 209s - loss: 0.4381 - val_loss: 0.4468 - 209s/epoch - 138ms/step\nEpoch 6/100\nEpoch 6: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 227s - loss: 0.4243 - val_loss: 0.4392 - 227s/epoch - 149ms/step\nEpoch 7/100\nEpoch 7: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 229s - loss: 0.4134 - val_loss: 0.4283 - 229s/epoch - 150ms/step\nEpoch 8/100\nEpoch 8: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 231s - loss: 0.4038 - val_loss: 0.4255 - 231s/epoch - 152ms/step\nEpoch 9/100\nEpoch 9: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 218s - loss: 0.3963 - val_loss: 0.4187 - 218s/epoch - 144ms/step\nEpoch 10/100\nEpoch 10: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 230s - loss: 0.3889 - val_loss: 0.4173 - 230s/epoch - 151ms/step\nEpoch 11/100\nEpoch 11: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 225s - loss: 0.3821 - val_loss: 0.4175 - 225s/epoch - 148ms/step\nEpoch 12/100\nEpoch 12: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 226s - loss: 0.3762 - val_loss: 0.4120 - 226s/epoch - 149ms/step\nEpoch 13/100\nEpoch 13: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 221s - loss: 0.3702 - val_loss: 0.4137 - 221s/epoch - 145ms/step\nEpoch 14/100\nEpoch 14: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 216s - loss: 0.3651 - val_loss: 0.4090 - 216s/epoch - 142ms/step\nEpoch 15/100\nEpoch 15: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 227s - loss: 0.3601 - val_loss: 0.4147 - 227s/epoch - 149ms/step\nEpoch 16/100\nEpoch 16: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 223s - loss: 0.3549 - val_loss: 0.4142 - 223s/epoch - 147ms/step\nEpoch 17/100\nEpoch 17: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 222s - loss: 0.3499 - val_loss: 0.4107 - 222s/epoch - 146ms/step\nEpoch 18/100\nEpoch 18: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 224s - loss: 0.3451 - val_loss: 0.4087 - 224s/epoch - 147ms/step\nEpoch 19/100\nEpoch 19: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 221s - loss: 0.3408 - val_loss: 0.4101 - 221s/epoch - 146ms/step\nEpoch 20/100\nEpoch 20: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 221s - loss: 0.3363 - val_loss: 0.4105 - 221s/epoch - 145ms/step\nEpoch 21/100\nEpoch 21: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 222s - loss: 0.3319 - val_loss: 0.4128 - 222s/epoch - 146ms/step\nEpoch 22/100\nEpoch 22: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 220s - loss: 0.3279 - val_loss: 0.4146 - 220s/epoch - 145ms/step\nEpoch 23/100\nEpoch 23: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 225s - loss: 0.3236 - val_loss: 0.4176 - 225s/epoch - 148ms/step\nEpoch 24/100\nEpoch 24: Train Accuracy = 0.0, Validation Accuracy = 0.0\n1520/1520 - 225s - loss: 0.3199 - val_loss: 0.4174 - 225s/epoch - 148ms/step\nEpoch 25/100\nEpoch 25: Train Accuracy = 0.0, Validation Accuracy = 0.0\nTraining stopped due to early stopping.\n1520/1520 - 226s - loss: 0.3159 - val_loss: 0.4183 - 226s/epoch - 149ms/step\nCPU times: user 2h 1min 48s, sys: 6min 36s, total: 2h 8min 24s\nWall time: 1h 35min 46s\n","output_type":"stream"}]},{"cell_type":"code","source":"print(model.summary())","metadata":{"execution":{"iopub.status.busy":"2023-07-10T14:50:57.477309Z","iopub.execute_input":"2023-07-10T14:50:57.477841Z","iopub.status.idle":"2023-07-10T14:50:57.549450Z","shell.execute_reply.started":"2023-07-10T14:50:57.477804Z","shell.execute_reply":"2023-07-10T14:50:57.548302Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Model: \"transformer\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n landmark_embedding (Landmar  (None, 128, 200)         1495600   \n kEmbedding)                                                     \n                                                                 \n token_embedding (TokenEmbed  multiple                 253600    \n ding)                                                           \n                                                                 \n sequential_5 (Sequential)   (None, 128, 200)          4711600   \n                                                                 \n transformer_decoder (Transf  multiple                 1447000   \n ormerDecoder)                                                   \n                                                                 \n transformer_decoder_1 (Tran  multiple                 1447000   \n sformerDecoder)                                                 \n                                                                 \n transformer_decoder_2 (Tran  multiple                 1447000   \n sformerDecoder)                                                 \n                                                                 \n dense_20 (Dense)            multiple                  12462     \n                                                                 \n=================================================================\nTotal params: 9,318,664\nTrainable params: 9,317,062\nNon-trainable params: 1,602\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])","metadata":{"execution":{"iopub.status.busy":"2023-07-10T14:51:02.401838Z","iopub.execute_input":"2023-07-10T14:51:02.402983Z","iopub.status.idle":"2023-07-10T14:51:02.699694Z","shell.execute_reply.started":"2023-07-10T14:51:02.402929Z","shell.execute_reply":"2023-07-10T14:51:02.698648Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7bae6c7a4340>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"code","source":"batches = [batch for batch in val_dataset]\n\npreds_list = []\nground_truth_list = []\n\nfor batch in batches[:1]:\n    source = batch[0]\n    target = batch[1].numpy()\n    bs = tf.shape(source)[0]\n    preds = model.generate(source, start_token_idx)\n    preds = preds.numpy()\n\n    for i in range(bs):\n        target_text = \"\".join([idx_to_char[_] for _ in target[i, :]])\n        ground_truth_list.append(target_text.replace('P', ''))\n        prediction = \"\"\n        for idx in preds[i, :]:\n            prediction += idx_to_char[idx]\n            if idx == end_token_idx:\n                break\n        preds_list.append(prediction)\n\nfor i in range(10):\n    print(ground_truth_list[i])\n    print(preds_list[i])\n    print('\\n~~~\\n')","metadata":{"execution":{"iopub.status.busy":"2023-07-10T14:51:08.736058Z","iopub.execute_input":"2023-07-10T14:51:08.736877Z","iopub.status.idle":"2023-07-10T14:51:19.404051Z","shell.execute_reply.started":"2023-07-10T14:51:08.736823Z","shell.execute_reply":"2023-07-10T14:51:19.402815Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"S3 creekhouseE\nS33 creek houseE\n\n~~~\n\nSscales/kuhaylahE\nSscales/thayalaE\n\n~~~\n\nS1383 william lanierE\nS138383 william lanierE\n\n~~~\n\nS988 franklin laneE\nS988 funnan laneE\n\n~~~\n\nS6920 northeast 661st roadE\nS6920 northeast 661st roadE\n\n~~~\n\nSwww.freem.ne.jpE\nSwww.freem.mee.jpE\n\n~~~\n\nShttps://jsi.is/hukuokaE\nShttps://jsitis/hkkuokaE\n\n~~~\n\nS239613 stolze streetE\nS2961 13r stolze streetE\n\n~~~\n\nS271097 bayshore boulevardE\nS271097 baytore boulevardE\n\n~~~\n\nSfederico pearsonE\nSfederico saronE\n\n~~~\n\n","output_type":"stream"}]},{"cell_type":"code","source":"ground_truth_processed = [ground_truth_list[i][1:-1] for i in range(len(ground_truth_list))]\npreds_list_processed = [preds_list[i][1:-1] for i in range(len(preds_list))]\nlev_dist = [lev.distance(ground_truth_processed[i], preds_list_processed[i])\n            for i in range(len(preds_list_processed))]\nN = [len(phrase) for phrase in ground_truth_processed]\n\nprint('Validation score: '+str((np.sum(N) - np.sum(lev_dist))/np.sum(N)))","metadata":{"execution":{"iopub.status.busy":"2023-07-10T14:51:49.790067Z","iopub.execute_input":"2023-07-10T14:51:49.790452Z","iopub.status.idle":"2023-07-10T14:51:49.803386Z","shell.execute_reply.started":"2023-07-10T14:51:49.790421Z","shell.execute_reply":"2023-07-10T14:51:49.802108Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Validation score: 0.8181818181818182\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Levenstein Distance Train","metadata":{}},{"cell_type":"code","source":"# Compute Levenstein Distances\ndef get_ld_train():\n    N = 100 if IS_INTERACTIVE else 1000\n    LD_TRAIN = []\n    for idx, (frames, phrase_true) in enumerate(zip(tqdm(X_train, total=N), y_train)):\n        # Predict Phrase and Convert to String\n        phrase_pred = predict_phrase(frames).numpy()\n        phrase_pred = outputs2phrase(phrase_pred)\n        # True Phrase Ordinal to String\n        phrase_true = outputs2phrase(phrase_true)\n        # Add Levenstein Distance\n        LD_TRAIN.append({\n            'phrase_true': phrase_true,\n            'phrase_pred': phrase_pred,\n            'levenshtein_distance': levenshtein(phrase_pred, phrase_true),\n        })\n        # Take subset in interactive mode\n        if idx == N:\n            break\n            \n    # Convert to DataFrame\n    LD_TRAIN_DF = pd.DataFrame(LD_TRAIN)\n    \n    return LD_TRAIN_DF","metadata":{"execution":{"iopub.status.busy":"2023-07-10T14:52:39.041415Z","iopub.execute_input":"2023-07-10T14:52:39.041894Z","iopub.status.idle":"2023-07-10T14:52:39.050328Z","shell.execute_reply.started":"2023-07-10T14:52:39.041862Z","shell.execute_reply":"2023-07-10T14:52:39.048829Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# TFLiteModel","metadata":{}},{"cell_type":"code","source":" class TFLiteModel(tf.Module):\n    def __init__(self, model):\n        super(TFLiteModel, self).__init__()\n        self.target_start_token_idx = start_token_idx\n        self.target_end_token_idx = end_token_idx\n        # Load the feature generation and main models\n        self.model = model\n\n    @tf.function(input_signature=[tf.TensorSpec(shape=[None, len(SEL_COLS)], dtype=tf.float32, name='inputs')])\n    def __call__(self, inputs, training=False):\n        # Preprocess Data\n        x = tf.cast(inputs, tf.float32)\n        x = x[None]\n        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, len(SEL_COLS))), lambda: tf.identity(x))\n        x = x[0]\n        x = pre_process(x)\n        x = x[None]\n        x = self.model.generate(x, self.target_start_token_idx)\n        x = x[0]\n        idx = tf.argmax(tf.cast(tf.equal(x, self.target_end_token_idx), tf.int32))\n        idx = tf.where(tf.math.less(idx, 1), tf.constant(2, dtype=tf.int64), idx)\n        x = x[1:idx]\n        x = tf.one_hot(x, 59)\n        return {'outputs': x}\n\ntflitemodel_base = TFLiteModel(model)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T14:53:01.034261Z","iopub.execute_input":"2023-07-10T14:53:01.035075Z","iopub.status.idle":"2023-07-10T14:53:01.049904Z","shell.execute_reply.started":"2023-07-10T14:53:01.035039Z","shell.execute_reply":"2023-07-10T14:53:01.048587Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"model.save_weights(\"model.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:26:04.103337Z","iopub.execute_input":"2023-07-10T13:26:04.103840Z","iopub.status.idle":"2023-07-10T13:26:04.402098Z","shell.execute_reply.started":"2023-07-10T13:26:04.103808Z","shell.execute_reply":"2023-07-10T13:26:04.401009Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\nkeras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]#, tf.lite.OpsSet.SELECT_TF_OPS]\ntflite_model = keras_model_converter.convert()\nwith open('/kaggle/working/model.tflite', 'wb') as f:\n    f.write(tflite_model)\n\ninfargs = {\"selected_columns\" : SEL_COLS}\n\nwith open('inference_args.json', \"w\") as json_file:\n    json.dump(infargs, json_file)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:26:04.403353Z","iopub.execute_input":"2023-07-10T13:26:04.403685Z","iopub.status.idle":"2023-07-10T13:28:55.088076Z","shell.execute_reply.started":"2023-07-10T13:26:04.403654Z","shell.execute_reply":"2023-07-10T13:28:55.087047Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Tensor(\"cond_2/Pad:0\", shape=(None, 26, 3), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"code","source":"!zip submission.zip  './model.tflite' './inference_args.json'","metadata":{"execution":{"iopub.status.busy":"2023-07-10T14:53:10.658096Z","iopub.execute_input":"2023-07-10T14:53:10.658526Z","iopub.status.idle":"2023-07-10T14:53:14.701936Z","shell.execute_reply.started":"2023-07-10T14:53:10.658491Z","shell.execute_reply":"2023-07-10T14:53:14.700303Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"updating: model.tflite (deflated 19%)\nupdating: inference_args.json (deflated 85%)\n","output_type":"stream"}]},{"cell_type":"code","source":"interpreter = tf.lite.Interpreter(\"model.tflite\")\n\nREQUIRED_SIGNATURE = \"serving_default\"\nREQUIRED_OUTPUT = \"outputs\"\n\nwith open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n    character_map = json.load(f)\nrev_character_map = {j:i for i,j in character_map.items()}\n\nfound_signatures = list(interpreter.get_signature_list().keys())\n\nif REQUIRED_SIGNATURE not in found_signatures:\n    raise KernelEvalException('Required input signature not found.')\n\nprediction_fn = interpreter.get_signature_runner(\"serving_default\")\noutput = prediction_fn(inputs=batch[0][0])\nprediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\nprint(prediction_str)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T14:53:33.090645Z","iopub.execute_input":"2023-07-10T14:53:33.091897Z","iopub.status.idle":"2023-07-10T14:53:34.648523Z","shell.execute_reply.started":"2023-07-10T14:53:33.091848Z","shell.execute_reply":"2023-07-10T14:53:34.647211Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"8 rounit north\n","output_type":"stream"}]}]}