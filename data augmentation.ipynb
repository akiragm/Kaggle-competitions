{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "history_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ColabPreprocess  --START\n",
        "置換<br>\n",
        "\n",
        "/kaggle/input/            <br>\n",
        "/content/"
      ],
      "metadata": {
        "id": "uIY3gyjVDW16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "9D2OX6U0CaH6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "! curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "! apt update\n",
        "! apt install gcsfuse"
      ],
      "metadata": {
        "outputId": "c7e26258-a546-4633-cb1d-9a00e5cddb84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6qpeTkECaH7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2659  100  2659    0     0  80575      0 --:--:-- --:--:-- --:--:-- 83093\n",
            "OK\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Get:5 http://packages.cloud.google.com/apt gcsfuse-bionic InRelease [5,004 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:7 http://packages.cloud.google.com/apt gcsfuse-bionic/main amd64 Packages [2,356 B]\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Get:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease [18.1 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,360 kB]\n",
            "Hit:14 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 Packages [29.5 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,299 kB]\n",
            "Fetched 5,054 kB in 3s (1,686 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "13 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  gcsfuse\n",
            "0 upgraded, 1 newly installed, 0 to remove and 13 not upgraded.\n",
            "Need to get 14.0 MB of archives.\n",
            "After this operation, 31.2 MB of additional disk space will be used.\n",
            "Get:1 http://packages.cloud.google.com/apt gcsfuse-bionic/main amd64 gcsfuse amd64 1.0.0 [14.0 MB]\n",
            "Fetched 14.0 MB in 1s (26.6 MB/s)\n",
            "Selecting previously unselected package gcsfuse.\n",
            "(Reading database ... 123069 files and directories currently installed.)\n",
            "Preparing to unpack .../gcsfuse_1.0.0_amd64.deb ...\n",
            "Unpacking gcsfuse (1.0.0) ...\n",
            "Setting up gcsfuse (1.0.0) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5f_lFB9TCaH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#kaggle get_gcspath\n",
        "from kaggle_datasets import KaggleDatasets\n",
        "print(KaggleDatasets().get_gcs_path())"
      ],
      "metadata": {
        "id": "1GmDR8Lh1fku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj3Q0v8LEtnE",
        "outputId": "40f584c4-157a-4795-a9ad-f29e18327031"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.13)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.5.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.26.16)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir -p asl-fingerspelling\n",
        "! gcsfuse  --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 kds-060e8f96f45da8817e298f5151de7d204c3aa2ebfe6436b68e8d87e2 asl-fingerspelling"
      ],
      "metadata": {
        "id": "olczO1_pC2TX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66579d13-ea71-4fd6-e553-115e5e060cf3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I0626 14:46:48.727639 2023/06/26 14:46:48.727604 Start gcsfuse/1.0.0 (Go version go1.20.4) for app \"\" using mount point: /content/asl-fingerspelling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir -p aslfr-parquets-to-tfrecords-cleaned\n",
        "! gcsfuse  --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 kds-1865224784ea3c877e01e6064beb4b6fd74d08462427498e98ddc3df aslfr-parquets-to-tfrecords-cleaned"
      ],
      "metadata": {
        "outputId": "721d6f3a-f526-4a82-f8b4-32f0c6d7789c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2KWdEtVCaH9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I0626 14:46:49.444254 2023/06/26 14:46:49.444226 Start gcsfuse/1.0.0 (Go version go1.20.4) for app \"\" using mount point: /content/aslfr-parquets-to-tfrecords-cleaned\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Levenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Esy-KMuR8ivw",
        "outputId": "59427fae-4e48-4925-e8fc-16aad990e8c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Levenshtein\n",
            "  Downloading Levenshtein-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=2.3.0 (from Levenshtein)\n",
            "  Downloading rapidfuzz-3.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.21.1 rapidfuzz-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8L77lmNJEQYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --END  ColabPreprocess"
      ],
      "metadata": {
        "id": "ZDYAPQiVEATw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. I used two transformer layer in the encoder instead of four.\n",
        "2. I used four attention heads instead of two.\n",
        "3. I used new tokens for SOS, EOS, and padding (very minor since Rohith used rare tokens for these purposes, but still- more 'correct').\n",
        "2. I fixed a bug (probably?) in the decoder's dropout layers, which did not have the training flag, resulting in dropout during inference. This change gave a nice bump in the score.\n",
        "3. I made the passing of the training flag explicit. I know it can be implicit since it is a kwarg, but explicit passing makes the whole thing more straightforward and maybe fix another one or two training-flag-related bugs along the way.\n",
        "4. I changed the positional encoding in the decoder from tf.keras.layers.Embedding to proper positional embeddings (i.e., the usual sines and cosines usually used for this purpose). This had a significant impact.\n",
        "5. I added positional embedding to the encoder. This, too, had a significant impact.\n"
      ],
      "metadata": {
        "id": "ls6jZUmODp5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import Levenshtein as lev\n",
        "import os\n",
        "import gc"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T03:43:39.647352Z",
          "iopub.execute_input": "2023-06-26T03:43:39.647946Z",
          "iopub.status.idle": "2023-06-26T03:43:39.654535Z",
          "shell.execute_reply.started": "2023-06-26T03:43:39.647910Z",
          "shell.execute_reply": "2023-06-26T03:43:39.653364Z"
        },
        "trusted": true,
        "id": "hJnLDjLzDp53"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "B3hcEfbDDp55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "inpdir = \"/content/asl-fingerspelling\"\n",
        "df = pd.read_csv(f'{inpdir}/train.csv')\n",
        "df[\"phrase_bytes\"] = df[\"phrase\"].map(lambda x: x.encode(\"utf-8\"))\n",
        "display(df.head())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-21T12:27:34.921833Z",
          "iopub.execute_input": "2023-06-21T12:27:34.922562Z",
          "iopub.status.idle": "2023-06-21T12:27:35.134638Z",
          "shell.execute_reply.started": "2023-06-21T12:27:34.922525Z",
          "shell.execute_reply": "2023-06-21T12:27:35.133695Z"
        },
        "id": "0Ni7noahDp55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_landmarks = pd.read_parquet('/content/asl-fingerspelling/train_landmarks/1019715464.parquet')\n",
        "keys = train_landmarks.keys()[1:]\n",
        "train_landmarks.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-21T12:27:35.136172Z",
          "iopub.execute_input": "2023-06-21T12:27:35.136526Z",
          "iopub.status.idle": "2023-06-21T12:27:52.128059Z",
          "shell.execute_reply.started": "2023-06-21T12:27:35.136494Z",
          "shell.execute_reply": "2023-06-21T12:27:52.122672Z"
        },
        "id": "F1hbBsMFDp55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TFRecord"
      ],
      "metadata": {
        "id": "t_Zq2IIvDp56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LPOSE = [13, 15, 17, 19, 21]\n",
        "RPOSE = [14, 16, 18, 20, 22]\n",
        "POSE = LPOSE + RPOSE\n",
        "\n",
        "RHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\n",
        "LHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\n",
        "POSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n",
        "\n",
        "X = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\n",
        "Y = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\n",
        "Z = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n",
        "\n",
        "SEL_COLS = X + Y + Z\n",
        "FRAME_LEN = 128\n",
        "\n",
        "X_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\n",
        "Y_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\n",
        "Z_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n",
        "\n",
        "RHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\n",
        "LHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\n",
        "RPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\n",
        "LPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n",
        "\n",
        "print('SEL_COLS size:' + str(len(SEL_COLS)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-21T12:27:52.129279Z",
          "iopub.status.idle": "2023-06-21T12:27:52.130399Z",
          "shell.execute_reply.started": "2023-06-21T12:27:52.130158Z",
          "shell.execute_reply": "2023-06-21T12:27:52.13018Z"
        },
        "id": "jycbHpy_Dp56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def load_relevant_data_subset(pq_path):\n",
        "    return pd.read_parquet(pq_path, columns=SEL_COLS)\n",
        "\n",
        "counter = 0\n",
        "for file_id in tqdm(df.file_id.unique()):\n",
        "    \n",
        "    print(counter)\n",
        "    counter+=1\n",
        "    \n",
        "    pqfile = f\"{inpdir}/train_landmarks/{file_id}.parquet\"\n",
        "    if not os.path.isdir(\"tfds\"): os.mkdir(\"tfds\")\n",
        "    tffile = f\"tfds/{file_id}.tfrecord\"\n",
        "    seq_refs = df.loc[df.file_id == file_id]\n",
        "    seqs = load_relevant_data_subset(pqfile)\n",
        "    seqs_numpy = seqs.to_numpy()\n",
        "    with tf.io.TFRecordWriter(tffile) as file_writer:\n",
        "        for seq_id, phrase in zip(seq_refs.sequence_id, seq_refs.phrase_bytes):\n",
        "            frames = seqs_numpy[seqs.index == seq_id]\n",
        "            \n",
        "            r_nonan = np.sum(np.sum(np.isnan(frames[:, RHAND_IDX]), axis = 1) == 0)\n",
        "            l_nonan = np.sum(np.sum(np.isnan(frames[:, LHAND_IDX]), axis = 1) == 0)\n",
        "            no_nan = max(r_nonan, l_nonan)\n",
        "            \n",
        "            if 2*len(phrase)<no_nan:\n",
        "                features = {SEL_COLS[i]: tf.train.Feature(\n",
        "                    float_list=tf.train.FloatList(value=frames[:, i])) for i in range(len(SEL_COLS))}\n",
        "                features[\"phrase\"] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[phrase]))\n",
        "                record_bytes = tf.train.Example(features=tf.train.Features(feature=features)).SerializeToString()\n",
        "                file_writer.write(record_bytes)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-21T12:27:52.131662Z",
          "iopub.status.idle": "2023-06-21T12:27:52.132528Z",
          "shell.execute_reply.started": "2023-06-21T12:27:52.132225Z",
          "shell.execute_reply": "2023-06-21T12:27:52.132253Z"
        },
        "id": "Hmzq1uMZDp56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading"
      ],
      "metadata": {
        "id": "anVm5CKGDp57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Here I use new tokens for padding, start and end of sentences. (Capitals are good since the original phrases have only lower case letters, besides numbers and various signs)."
      ],
      "metadata": {
        "id": "NaASHGkwDp57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pad_token = 'P'\n",
        "start_token = 'S'\n",
        "end_token = 'E'\n",
        "pad_token_idx = 59\n",
        "start_token_idx = 60\n",
        "end_token_idx = 61"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T03:43:50.038588Z",
          "iopub.execute_input": "2023-06-26T03:43:50.039294Z",
          "iopub.status.idle": "2023-06-26T03:43:50.043979Z",
          "shell.execute_reply.started": "2023-06-26T03:43:50.039260Z",
          "shell.execute_reply": "2023-06-26T03:43:50.043067Z"
        },
        "trusted": true,
        "id": "zGEhbPRUDp58"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open (\"/content/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
        "    char_to_num = json.load(f)\n",
        "\n",
        "\n",
        "char_to_num[pad_token] = pad_token_idx\n",
        "char_to_num[start_token] = start_token_idx\n",
        "char_to_num[end_token] = end_token_idx\n",
        "\n",
        "num_to_char = {j:i for i,j in char_to_num.items()}\n",
        "\n",
        "\n",
        "inpdir = \"/content/asl-fingerspelling\"\n",
        "df = pd.read_csv(f'{inpdir}/train.csv')\n",
        "\n",
        "LPOSE = [13, 15, 17, 19, 21]\n",
        "RPOSE = [14, 16, 18, 20, 22]\n",
        "POSE = LPOSE + RPOSE\n",
        "\n",
        "RHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\n",
        "LHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\n",
        "POSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n",
        "\n",
        "X = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\n",
        "Y = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\n",
        "Z = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n",
        "\n",
        "SEL_COLS = X + Y + Z\n",
        "FRAME_LEN = 128\n",
        "\n",
        "X_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\n",
        "Y_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\n",
        "Z_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n",
        "\n",
        "RHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\n",
        "LHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\n",
        "RPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\n",
        "LPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n",
        "\n",
        "print(RPOSE_IDX)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T03:43:51.279454Z",
          "iopub.execute_input": "2023-06-26T03:43:51.279841Z",
          "iopub.status.idle": "2023-06-26T03:43:51.388163Z",
          "shell.execute_reply.started": "2023-06-26T03:43:51.279809Z",
          "shell.execute_reply": "2023-06-26T03:43:51.387128Z"
        },
        "trusted": true,
        "id": "SdTNf47NDp58",
        "outputId": "3da0b227-39f7-458d-d27c-a588c73aae86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[47, 48, 49, 50, 51, 99, 100, 101, 102, 103, 151, 152, 153, 154, 155]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{0: ' ', 1: '!', 2: '#', 3: '$', 4: '%', 5: '&', 6: \"'\", 7: '(', 8: ')', 9: '*', 10: '+', 11: ',', 12: '-', 13: '.', 14: '/', 15: '0', 16: '1', 17: '2', 18: '3', 19: '4', 20: '5', 21: '6', 22: '7', 23: '8', 24: '9', 25: ':', 26: ';', 27: '=', 28: '?', 29: '@', 30: '[', 31: '_', 32: 'a', 33: 'b', 34: 'c', 35: 'd', 36: 'e', 37: 'f', 38: 'g', 39: 'h', 40: 'i', 41: 'j', 42: 'k', 43: 'l', 44: 'm', 45: 'n', 46: 'o', 47: 'p', 48: 'q', 49: 'r', 50: 's', 51: 't', 52: 'u', 53: 'v', 54: 'w', 55: 'x', 56: 'y', 57: 'z', 58: '~', 59: 'P', 60: 'S', 61: 'E'}\n",
        "add Codeadd Markdown"
      ],
      "metadata": {
        "id": "7l2otrH4Dp59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_pad(x):\n",
        "    if tf.shape(x)[0] < FRAME_LEN:\n",
        "        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0], [0, 0]]))\n",
        "        print(x)\n",
        "    else:\n",
        "        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n",
        "    return x\n",
        "\n",
        "def translate_landmarks(landmarks, max_translation):\n",
        "    translation = tf.random.uniform(shape=tf.shape(landmarks), minval=-max_translation, maxval=max_translation)\n",
        "    translated_landmarks = landmarks + translation\n",
        "    return translated_landmarks\n",
        "\n",
        "def scale_landmarks(landmarks, min_scale, max_scale):\n",
        "    scale_factor = tf.random.uniform(shape=tf.shape(landmarks), minval=min_scale, maxval=max_scale)\n",
        "    scaled_landmarks = landmarks * scale_factor\n",
        "    return scaled_landmarks\n",
        "\n",
        "def pre_process(x):\n",
        "\n",
        "    rhand = tf.gather(x, RHAND_IDX, axis=1)\n",
        "    lhand = tf.gather(x, LHAND_IDX, axis=1)\n",
        "    rpose = tf.gather(x, RPOSE_IDX, axis=1)\n",
        "    lpose = tf.gather(x, LPOSE_IDX, axis=1)\n",
        "\n",
        "    rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)\n",
        "    lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)\n",
        "\n",
        "    rnans = tf.math.count_nonzero(rnan_idx)\n",
        "    lnans = tf.math.count_nonzero(lnan_idx)\n",
        "\n",
        "    # For dominant hand\n",
        "    if rnans > lnans:\n",
        "        hand = lhand\n",
        "        pose = lpose\n",
        "\n",
        "        hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n",
        "        hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n",
        "        hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n",
        "        hand = tf.concat([1-hand_x, hand_y, hand_z], axis=1)\n",
        "\n",
        "        pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n",
        "        pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n",
        "        pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n",
        "        pose = tf.concat([1-pose_x, pose_y, pose_z], axis=1)\n",
        "    else:\n",
        "        hand = rhand\n",
        "        pose = rpose\n",
        "\n",
        "    hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n",
        "    hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n",
        "    hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n",
        "    hand = tf.concat([hand_x[..., tf.newaxis], hand_y[..., tf.newaxis], hand_z[..., tf.newaxis]], axis=-1)\n",
        "\n",
        "    mean = tf.math.reduce_mean(hand, axis=1)[:, tf.newaxis, :]\n",
        "    std = tf.math.reduce_std(hand, axis=1)[:, tf.newaxis, :]\n",
        "    hand = (hand - mean) / std\n",
        "\n",
        "    pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n",
        "    pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n",
        "    pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n",
        "    pose = tf.concat([pose_x[..., tf.newaxis], pose_y[..., tf.newaxis], pose_z[..., tf.newaxis]], axis=-1)\n",
        "\n",
        "    x = tf.concat([hand, pose], axis=1)\n",
        "    x = resize_pad(x)\n",
        "\n",
        "    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n",
        "    x = tf.reshape(x, (FRAME_LEN, len(LHAND_IDX) + len(LPOSE_IDX)))\n",
        "    return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T03:43:56.251215Z",
          "iopub.execute_input": "2023-06-26T03:43:56.251603Z",
          "iopub.status.idle": "2023-06-26T03:43:56.275318Z",
          "shell.execute_reply.started": "2023-06-26T03:43:56.251574Z",
          "shell.execute_reply": "2023-06-26T03:43:56.274277Z"
        },
        "trusted": true,
        "id": "Qk5pTdDzDp5-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = tf.lookup.StaticHashTable(\n",
        "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=list(char_to_num.keys()),\n",
        "        values=list(char_to_num.values()),\n",
        "    ),\n",
        "    default_value=tf.constant(-1),\n",
        "    name=\"class_weight\"\n",
        ")\n",
        "\n",
        "def preprocess_fn(landmarks, phrase):\n",
        "    phrase = start_token + phrase + end_token\n",
        "    phrase = tf.strings.bytes_split(phrase)\n",
        "    phrase = table.lookup(phrase)\n",
        "    phrase = tf.pad(phrase, paddings=[[0, 64 - tf.shape(phrase)[0]]], mode = 'CONSTANT',\n",
        "                    constant_values = pad_token_idx)\n",
        "\n",
        "    # landmarksを前処理する\n",
        "    translated_landmarks = translate_landmarks(landmarks, max_translation=10)\n",
        "    scaled_landmarks = scale_landmarks(landmarks, min_scale=0.8, max_scale=1.2)\n",
        "\n",
        "    # 前処理済みのlandmarksを結合する\n",
        "    combined_landmarks = tf.concat([landmarks, translated_landmarks, scaled_landmarks], axis=1)\n",
        "\n",
        "    return pre_process(combined_landmarks), phrase\n",
        "\n",
        "def decode_fn(record_bytes):\n",
        "    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in SEL_COLS}\n",
        "    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n",
        "    features = tf.io.parse_single_example(record_bytes, schema)\n",
        "    phrase = features[\"phrase\"]\n",
        "    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in SEL_COLS])\n",
        "    landmarks = tf.transpose(landmarks)\n",
        "\n",
        "    return landmarks, phrase\n",
        "\n",
        "inpdir = \"/content/aslfr-parquets-to-tfrecords-cleaned\"\n",
        "tffiles = df.file_id.map(lambda x: f'{inpdir}/tfds/{x}.tfrecord').unique()\n",
        "\n",
        "batch_size = 32\n",
        "val_len = int(0.05 * len(tffiles))\n",
        "\n",
        "train_dataset = tf.data.TFRecordDataset(tffiles[val_len:]).map(decode_fn).map(preprocess_fn).shuffle(30000, reshuffle_each_iteration=True).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_dataset = tf.data.TFRecordDataset(tffiles[:val_len]).map(decode_fn).map(preprocess_fn).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T03:43:57.838702Z",
          "iopub.execute_input": "2023-06-26T03:43:57.839078Z",
          "iopub.status.idle": "2023-06-26T03:43:59.156543Z",
          "shell.execute_reply.started": "2023-06-26T03:43:57.839049Z",
          "shell.execute_reply": "2023-06-26T03:43:59.155472Z"
        },
        "trusted": true,
        "id": "GK-wOj8CDp5_",
        "outputId": "1cc5acf0-25e4-40eb-862b-c4a75b08bfe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"cond_1/Pad:0\", shape=(None, 26, 3), dtype=float32)\n",
            "Tensor(\"cond_1/Pad:0\", shape=(None, 26, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The model"
      ],
      "metadata": {
        "id": "PufJODDjDp5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:e865f0c6-0e14-4387-9779-537f8b3d065d.png)\n",
        "![image.png](attachment:39ec8854-9e5f-4617-aa6a-69098b605134.png)\n",
        "![image.png](attachment:8b28cc87-2b4a-467f-a818-44b7e1f8dc8f.png)\n",
        "![image.png](attachment:243f34f9-3f88-4452-aac7-ee1ec0a3d065.png)"
      ],
      "metadata": {
        "id": "35kTT5MRDp5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Here I implemented proper positional embeddings for both the encoder and the decoder."
      ],
      "metadata": {
        "id": "su-6MWcSDp6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPBlock(layers.Layer):\n",
        "    def __init__(self, num_hid=64, num_layers=5):\n",
        "        super().__init__()\n",
        "        self.mlp = tf.keras.Sequential()\n",
        "        for _ in range(num_layers):\n",
        "            self.mlp.add(tf.keras.layers.Dense(num_hid, activation=tf.nn.gelu))\n",
        "        self.mlp.add(tf.keras.layers.Dense(num_hid))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.mlp(inputs)\n",
        "\n",
        "\n",
        "class TokenEmbedding(layers.Layer):\n",
        "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64, mlp_num_layers=2):\n",
        "        super().__init__()\n",
        "        self.num_hid = num_hid\n",
        "        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n",
        "        self.pos_emb = self.positional_encoding(maxlen - 1, num_hid)\n",
        "        self.mlp_block = MLPBlock(num_hid, num_layers=mlp_num_layers)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        x = self.emb(x) * tf.math.sqrt(tf.cast(self.num_hid, tf.float32))\n",
        "        x = x + self.pos_emb[:maxlen, :]\n",
        "        x = self.mlp_block(x)\n",
        "        return x\n",
        "\n",
        "    def positional_encoding(self, maxlen, num_hid):\n",
        "        positions = tf.range(maxlen, dtype=tf.float32)[..., tf.newaxis]\n",
        "        depth = num_hid // 2\n",
        "        angles = positions / tf.pow(10000, tf.range(0, depth, 1, dtype=tf.float32) / num_hid)  # depthのインクリメントを修正\n",
        "        pos_encoding = tf.concat([tf.sin(angles), tf.cos(angles)], axis=-1)\n",
        "        return pos_encoding\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T03:44:00.656849Z",
          "iopub.execute_input": "2023-06-26T03:44:00.659000Z",
          "iopub.status.idle": "2023-06-26T03:44:00.672227Z",
          "shell.execute_reply.started": "2023-06-26T03:44:00.658953Z",
          "shell.execute_reply": "2023-06-26T03:44:00.671099Z"
        },
        "trusted": true,
        "id": "9QJUkdQeDp6A"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "class TokenEmbedding(layers.Layer):\n",
        "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n",
        "        super().__init__()\n",
        "        self.num_hid = num_hid\n",
        "        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n",
        "        #self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
        "        '''\n",
        "        self.pos_emb = tf.math.divide(\n",
        "            self.positional_encoding(maxlen-1, num_hid),\n",
        "            tf.math.sqrt(tf.cast(num_hid, tf.float32)))\n",
        "        '''\n",
        "        self.pos_emb = self.positional_encoding(maxlen-1, num_hid)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        x = self.emb(x)\n",
        "        x = tf.math.multiply(x, tf.math.sqrt(tf.cast(self.num_hid, tf.float32)))\n",
        "        '''\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        return x + positions\n",
        "        '''\n",
        "        return x + self.pos_emb[:maxlen, :]\n",
        "    \n",
        "    def positional_encoding(self, maxlen, num_hid):\n",
        "        depth = num_hid/2\n",
        "        positions = tf.range(maxlen, dtype = tf.float32)[..., tf.newaxis]\n",
        "        depths = tf.range(depth, dtype = tf.float32)[np.newaxis, :]/depth\n",
        "        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n",
        "        angle_rads = tf.linalg.matmul(positions, angle_rates)\n",
        "        pos_encoding = tf.concat(\n",
        "          [tf.math.sin(angle_rads), tf.math.cos(angle_rads)],\n",
        "          axis=-1)\n",
        "        return pos_encoding\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-22T01:27:59.514789Z",
          "iopub.execute_input": "2023-06-22T01:27:59.517058Z",
          "iopub.status.idle": "2023-06-22T01:27:59.530952Z",
          "shell.execute_reply.started": "2023-06-22T01:27:59.517017Z",
          "shell.execute_reply": "2023-06-22T01:27:59.529987Z"
        },
        "id": "0K45t4ttDp6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LandmarkEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_hid=64, maxlen=100):\n",
        "        super(LandmarkEmbedding, self).__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.relu1 = tf.keras.layers.ReLU()\n",
        "\n",
        "        self.conv2 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
        "        self.relu2 = tf.keras.layers.ReLU()\n",
        "        self.dropout2 = tf.keras.layers.Dropout(0.2)\n",
        "\n",
        "        self.conv3 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
        "        self.relu3 = tf.keras.layers.ReLU()\n",
        "        self.dropout3 = tf.keras.layers.Dropout(0.2)\n",
        "\n",
        "        self.conv4 = tf.keras.layers.Conv1D(num_hid, 11, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "        self.bn4 = tf.keras.layers.BatchNormalization()\n",
        "        self.relu4 = tf.keras.layers.ReLU()\n",
        "        self.dropout4 = tf.keras.layers.Dropout(0.2)\n",
        "\n",
        "        self.sigmoid = tf.keras.layers.Activation('sigmoid')\n",
        "        self.pos_emb = self.positional_encoding(maxlen, num_hid)\n",
        "        self.maxlen = maxlen\n",
        "        self.num_hid = num_hid\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.dropout4(x)\n",
        "\n",
        "        x = tf.math.multiply(x, tf.math.sqrt(tf.cast(self.num_hid, tf.float32)))\n",
        "        x = x + self.pos_emb\n",
        "\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "    def positional_encoding(self, maxlen, num_hid):\n",
        "        depth = num_hid/2\n",
        "        positions = tf.range(maxlen, dtype=tf.float32)[..., tf.newaxis]\n",
        "        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth\n",
        "        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n",
        "        angle_rads = tf.linalg.matmul(positions, angle_rates)\n",
        "        pos_encoding = tf.concat([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], axis=-1)\n",
        "        return pos_encoding\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T03:44:02.337798Z",
          "iopub.execute_input": "2023-06-26T03:44:02.338456Z",
          "iopub.status.idle": "2023-06-26T03:44:02.358498Z",
          "shell.execute_reply.started": "2023-06-26T03:44:02.338424Z",
          "shell.execute_reply": "2023-06-26T03:44:02.357547Z"
        },
        "trusted": true,
        "id": "yhso_UunDp6A"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T03:44:04.520721Z",
          "iopub.execute_input": "2023-06-26T03:44:04.521113Z",
          "iopub.status.idle": "2023-06-26T03:44:04.530882Z",
          "shell.execute_reply.started": "2023-06-26T03:44:04.521080Z",
          "shell.execute_reply": "2023-06-26T03:44:04.529251Z"
        },
        "trusted": true,
        "id": "N8Lugl97Dp6B"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# TFRecordファイルのパス\n",
        "tfrecord_file = \"/kaggle/working/tfds/128822441.tfrecord\"\n",
        "\n",
        "# TFRecordデータセットの作成\n",
        "dataset = tf.data.TFRecordDataset([tfrecord_file])\n",
        "print(dataset)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T03:44:07.559213Z",
          "iopub.execute_input": "2023-06-26T03:44:07.560180Z",
          "iopub.status.idle": "2023-06-26T03:44:07.577877Z",
          "shell.execute_reply.started": "2023-06-26T03:44:07.560136Z",
          "shell.execute_reply": "2023-06-26T03:44:07.576727Z"
        },
        "trusted": true,
        "id": "PryRJHouDp6B",
        "outputId": "6a18ef4a-a618-4df0-ed80-587f193f29c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Here I added the training flag to the TransformerDecoder's Dropout layers."
      ],
      "metadata": {
        "id": "6Ul8pZtNDp6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.self_att = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.self_dropout = layers.Dropout(0.5)\n",
        "        self.enc_dropout = layers.Dropout(0.1)\n",
        "        self.ffn_dropout = layers.Dropout(0.1)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
        "        \"\"\"Masks the upper half of the dot product matrix in self attention.\n",
        "\n",
        "        This prevents flow of information from future tokens to current token.\n",
        "        1's in the lower triangle, counting from the lower right corner.\n",
        "        \"\"\"\n",
        "        i = tf.range(n_dest)[:, None]\n",
        "        j = tf.range(n_src)\n",
        "        m = i >= j - n_src + n_dest\n",
        "        mask = tf.cast(m, dtype)\n",
        "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "        mult = tf.concat(\n",
        "            [batch_size[..., tf.newaxis], tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, enc_out, target, training):\n",
        "        input_shape = tf.shape(target)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        target_att = self.self_att(target, target, attention_mask=causal_mask)\n",
        "        target_norm = self.layernorm1(target + self.self_dropout(target_att, training = training))\n",
        "        enc_out = self.enc_att(target_norm, enc_out)\n",
        "        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out, training = training) + target_norm)\n",
        "        ffn_out = self.ffn(enc_out_norm)\n",
        "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out, training = training))\n",
        "        return ffn_out_norm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T03:44:08.748287Z",
          "iopub.execute_input": "2023-06-26T03:44:08.748649Z",
          "iopub.status.idle": "2023-06-26T03:44:08.764270Z",
          "shell.execute_reply.started": "2023-06-26T03:44:08.748619Z",
          "shell.execute_reply": "2023-06-26T03:44:08.762681Z"
        },
        "trusted": true,
        "id": "lbmu0lqMDp6C"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Here I made the passing of the training flag explicit."
      ],
      "metadata": {
        "id": "IxjuUK90Dp6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_hid=64,\n",
        "        num_head=2,\n",
        "        num_feed_forward=128,\n",
        "        source_maxlen=100,\n",
        "        target_maxlen=100,\n",
        "        num_layers_enc=4,\n",
        "        num_layers_dec=1,\n",
        "        num_classes=60,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n",
        "        self.num_layers_enc = num_layers_enc\n",
        "        self.num_layers_dec = num_layers_dec\n",
        "        self.target_maxlen = target_maxlen\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.enc_input = LandmarkEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
        "        self.dec_input = TokenEmbedding(\n",
        "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
        "        )\n",
        "\n",
        "        self.encoder = keras.Sequential(\n",
        "            [self.enc_input]\n",
        "            + [\n",
        "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
        "                for _ in range(num_layers_enc)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        for i in range(num_layers_dec):\n",
        "            setattr(\n",
        "                self,\n",
        "                f\"dec_layer_{i}\",\n",
        "                TransformerDecoder(num_hid, num_head, num_feed_forward),\n",
        "            )\n",
        "\n",
        "        self.classifier = layers.Dense(num_classes)\n",
        "\n",
        "    def decode(self, enc_out, target, training):\n",
        "        y = self.dec_input(target)\n",
        "        for i in range(self.num_layers_dec):\n",
        "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y, training)\n",
        "        return y\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        source = inputs[0]\n",
        "        target = inputs[1]\n",
        "        x = self.encoder(source, training)\n",
        "        y = self.decode(x, target, training)\n",
        "        return self.classifier(y)\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_metric]\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        \"\"\"Processes one batch inside model.fit().\"\"\"\n",
        "        source = batch[0]\n",
        "        target = batch[1]\n",
        "\n",
        "        input_shape = tf.shape(target)\n",
        "        batch_size = input_shape[0]\n",
        "\n",
        "        dec_input = target[:, :-1]\n",
        "        dec_target = target[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            preds = self([source, dec_input])\n",
        "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "            mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n",
        "            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\"loss\": self.loss_metric.result()}\n",
        "\n",
        "    def test_step(self, batch):\n",
        "        source = batch[0]\n",
        "        target = batch[1]\n",
        "\n",
        "        input_shape = tf.shape(target)\n",
        "        batch_size = input_shape[0]\n",
        "\n",
        "        dec_input = target[:, :-1]\n",
        "        dec_target = target[:, 1:]\n",
        "        preds = self([source, dec_input])\n",
        "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "        mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n",
        "        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\"loss\": self.loss_metric.result()}\n",
        "\n",
        "    def generate(self, source, target_start_token_idx):\n",
        "        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n",
        "        bs = tf.shape(source)[0]\n",
        "        enc = self.encoder(source, training = False)\n",
        "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
        "        dec_logits = []\n",
        "        for i in range(self.target_maxlen - 1):\n",
        "            dec_out = self.decode(enc, dec_input, training = False)\n",
        "            logits = self.classifier(dec_out)\n",
        "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "            last_logit = logits[:, -1][..., tf.newaxis]\n",
        "            dec_logits.append(last_logit)\n",
        "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
        "        return dec_input"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T03:44:11.821637Z",
          "iopub.execute_input": "2023-06-26T03:44:11.822020Z",
          "iopub.status.idle": "2023-06-26T03:44:11.845701Z",
          "shell.execute_reply.started": "2023-06-26T03:44:11.821989Z",
          "shell.execute_reply": "2023-06-26T03:44:11.844544Z"
        },
        "trusted": true,
        "id": "FMzXwlaNDp6D"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 正解率を計算するためのメトリクスを作成\n",
        "train_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "val_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "# 学習ループ内で正解率を更新するコールバックを定義\n",
        "class AccuracyCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        train_acc = train_accuracy.result()\n",
        "        val_acc = val_accuracy.result()\n",
        "        print(f\"Epoch {epoch+1}: Train Accuracy = {train_acc}, Validation Accuracy = {val_acc}\")\n",
        "        # 正解率をリセット\n",
        "        train_accuracy.reset_states()\n",
        "        val_accuracy.reset_states()\n",
        "# val_lossが連続3回マイナスになった場合に学習を停止するコールバック\n",
        "class EarlyStoppingCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, patience=3):\n",
        "        super(EarlyStoppingCallback, self).__init__()\n",
        "        self.patience = patience\n",
        "        self.min_val_loss = float('inf')\n",
        "        self.wait = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_loss = logs.get('val_loss')\n",
        "        if val_loss < self.min_val_loss:\n",
        "            self.min_val_loss = val_loss\n",
        "            self.wait = 0\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.model.stop_training = True\n",
        "                print(\"Training stopped due to early stopping.\")\n",
        "\n",
        "batch = next(iter(val_dataset))\n",
        "idx_to_char = list(char_to_num.keys())\n",
        "\n",
        "model = Transformer(\n",
        "    num_hid=200,\n",
        "    num_head=4,\n",
        "    num_feed_forward=400,\n",
        "    source_maxlen = FRAME_LEN,\n",
        "    target_maxlen=64,\n",
        "    num_layers_enc=2,\n",
        "    num_layers_dec=1,\n",
        "    num_classes=62,\n",
        ")\n",
        "\n",
        "\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1)\n",
        "accuracy_callback = AccuracyCallback()\n",
        "optimizer = keras.optimizers.Adam(0.0001)\n",
        "\n",
        "\n",
        "# モデルのコンパイル\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=[train_accuracy])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T03:45:35.856212Z",
          "iopub.execute_input": "2023-06-26T03:45:35.856580Z",
          "iopub.status.idle": "2023-06-26T03:45:36.221753Z",
          "shell.execute_reply.started": "2023-06-26T03:45:35.856550Z",
          "shell.execute_reply": "2023-06-26T03:45:36.220648Z"
        },
        "trusted": true,
        "id": "qZRLTg89Dp6D"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# EarlyStoppingCallbackをコールバックリストに追加して学習を行う\n",
        "history = model.fit(train_dataset, verbose=2, validation_data=val_dataset, epochs=100,\n",
        "                    callbacks=[AccuracyCallback(), EarlyStoppingCallback()])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T03:45:40.633802Z",
          "iopub.execute_input": "2023-06-26T03:45:40.634201Z",
          "iopub.status.idle": "2023-06-26T04:23:16.771818Z",
          "shell.execute_reply.started": "2023-06-26T03:45:40.634167Z",
          "shell.execute_reply": "2023-06-26T04:23:16.769586Z"
        },
        "trusted": true,
        "id": "rLPhxPFzDp6E",
        "outputId": "412224c2-03fc-4818-a352-28ad54db6ded",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Epoch 1: Train Accuracy = 0.0, Validation Accuracy = 0.0\n",
            "1520/1520 - 541s - loss: 0.8489 - val_loss: 0.7635 - 541s/epoch - 356ms/step\n",
            "Epoch 2/100\n",
            "Epoch 2: Train Accuracy = 0.0, Validation Accuracy = 0.0\n",
            "1520/1520 - 143s - loss: 0.6654 - val_loss: 0.5850 - 143s/epoch - 94ms/step\n",
            "Epoch 3/100\n",
            "Epoch 3: Train Accuracy = 0.0, Validation Accuracy = 0.0\n",
            "1520/1520 - 143s - loss: 0.5456 - val_loss: 0.5312 - 143s/epoch - 94ms/step\n",
            "Epoch 4/100\n",
            "Epoch 4: Train Accuracy = 0.0, Validation Accuracy = 0.0\n",
            "1520/1520 - 141s - loss: 0.5008 - val_loss: 0.4975 - 141s/epoch - 93ms/step\n",
            "Epoch 5/100\n",
            "Epoch 5: Train Accuracy = 0.0, Validation Accuracy = 0.0\n",
            "1520/1520 - 140s - loss: 0.4758 - val_loss: 0.4841 - 140s/epoch - 92ms/step\n",
            "Epoch 6/100\n",
            "Epoch 6: Train Accuracy = 0.0, Validation Accuracy = 0.0\n",
            "1520/1520 - 143s - loss: 0.4584 - val_loss: 0.4708 - 143s/epoch - 94ms/step\n",
            "Epoch 7/100\n",
            "Epoch 7: Train Accuracy = 0.0, Validation Accuracy = 0.0\n",
            "1520/1520 - 142s - loss: 0.4446 - val_loss: 0.4611 - 142s/epoch - 93ms/step\n",
            "Epoch 8/100\n",
            "Epoch 8: Train Accuracy = 0.0, Validation Accuracy = 0.0\n",
            "1520/1520 - 142s - loss: 0.4336 - val_loss: 0.4576 - 142s/epoch - 94ms/step\n",
            "Epoch 9/100\n",
            "Epoch 9: Train Accuracy = 0.0, Validation Accuracy = 0.0\n",
            "1520/1520 - 142s - loss: 0.4238 - val_loss: 0.4529 - 142s/epoch - 93ms/step\n",
            "Epoch 10/100\n",
            "Epoch 10: Train Accuracy = 0.0, Validation Accuracy = 0.0\n",
            "1520/1520 - 142s - loss: 0.4149 - val_loss: 0.4501 - 142s/epoch - 93ms/step\n",
            "Epoch 11/100\n",
            "Epoch 11: Train Accuracy = 0.0, Validation Accuracy = 0.0\n",
            "1520/1520 - 141s - loss: 0.4072 - val_loss: 0.4443 - 141s/epoch - 93ms/step\n",
            "Epoch 12/100\n",
            "Epoch 12: Train Accuracy = 0.0, Validation Accuracy = 0.0\n",
            "1520/1520 - 141s - loss: 0.4000 - val_loss: 0.4426 - 141s/epoch - 93ms/step\n",
            "Epoch 13/100\n",
            "Epoch 13: Train Accuracy = 0.0, Validation Accuracy = 0.0\n",
            "1520/1520 - 141s - loss: 0.3937 - val_loss: 0.4414 - 141s/epoch - 93ms/step\n",
            "Epoch 14/100\n",
            "Epoch 14: Train Accuracy = 0.0, Validation Accuracy = 0.0\n",
            "1520/1520 - 142s - loss: 0.3874 - val_loss: 0.4435 - 142s/epoch - 94ms/step\n",
            "Epoch 15/100\n",
            "Epoch 15: Train Accuracy = 0.0, Validation Accuracy = 0.0\n",
            "1520/1520 - 143s - loss: 0.3815 - val_loss: 0.4406 - 143s/epoch - 94ms/step\n",
            "Epoch 16/100\n",
            "Epoch 16: Train Accuracy = 0.0, Validation Accuracy = 0.0\n",
            "1520/1520 - 144s - loss: 0.3759 - val_loss: 0.4454 - 144s/epoch - 95ms/step\n",
            "Epoch 17/100\n",
            "Epoch 17: Train Accuracy = 0.0, Validation Accuracy = 0.0\n",
            "1520/1520 - 142s - loss: 0.3708 - val_loss: 0.4423 - 142s/epoch - 93ms/step\n",
            "Epoch 18/100\n",
            "Epoch 18: Train Accuracy = 0.0, Validation Accuracy = 0.0\n",
            "Training stopped due to early stopping.\n",
            "1520/1520 - 144s - loss: 0.3660 - val_loss: 0.4445 - 144s/epoch - 95ms/step\n",
            "CPU times: user 1h 14min 48s, sys: 3min 23s, total: 1h 18min 12s\n",
            "Wall time: 52min 16s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T04:23:22.467839Z",
          "iopub.execute_input": "2023-06-26T04:23:22.468542Z",
          "iopub.status.idle": "2023-06-26T04:23:22.523050Z",
          "shell.execute_reply.started": "2023-06-26T04:23:22.468503Z",
          "shell.execute_reply": "2023-06-26T04:23:22.522121Z"
        },
        "trusted": true,
        "id": "m0PK7-zbDp6E",
        "outputId": "c0327d4c-6e0d-4559-9a72-693f2c8c0895",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " landmark_embedding (Landmar  (None, 128, 200)         1495600   \n",
            " kEmbedding)                                                     \n",
            "                                                                 \n",
            " token_embedding (TokenEmbed  multiple                 133000    \n",
            " ding)                                                           \n",
            "                                                                 \n",
            " sequential_3 (Sequential)   (None, 128, 200)          3103600   \n",
            "                                                                 \n",
            " transformer_decoder (Transf  multiple                 1447000   \n",
            " ormerDecoder)                                                   \n",
            "                                                                 \n",
            " dense_9 (Dense)             multiple                  12462     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,696,064\n",
            "Trainable params: 4,694,462\n",
            "Non-trainable params: 1,602\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T04:23:25.874327Z",
          "iopub.execute_input": "2023-06-26T04:23:25.874717Z",
          "iopub.status.idle": "2023-06-26T04:23:26.232494Z",
          "shell.execute_reply.started": "2023-06-26T04:23:25.874667Z",
          "shell.execute_reply": "2023-06-26T04:23:26.231586Z"
        },
        "trusted": true,
        "id": "BHA4qBuLDp6E",
        "outputId": "94c92f22-b38a-4fe8-8ea7-af4cf4775658",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0583176860>]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ],
      "metadata": {
        "id": "SywRcwtGDp6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batches = [batch for batch in val_dataset]\n",
        "\n",
        "preds_list = []\n",
        "ground_truth_list = []\n",
        "\n",
        "for batch in batches[:1]:\n",
        "    source = batch[0]\n",
        "    target = batch[1].numpy()\n",
        "    bs = tf.shape(source)[0]\n",
        "    preds = model.generate(source, start_token_idx)\n",
        "    preds = preds.numpy()\n",
        "\n",
        "    for i in range(bs):\n",
        "        target_text = \"\".join([idx_to_char[_] for _ in target[i, :]])\n",
        "        ground_truth_list.append(target_text.replace('P', ''))\n",
        "        prediction = \"\"\n",
        "        for idx in preds[i, :]:\n",
        "            prediction += idx_to_char[idx]\n",
        "            if idx == end_token_idx:\n",
        "                break\n",
        "        preds_list.append(prediction)\n",
        "\n",
        "for i in range(10):\n",
        "    print(ground_truth_list[i])\n",
        "    print(preds_list[i])\n",
        "    print('\\n~~~\\n')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T04:23:33.831503Z",
          "iopub.execute_input": "2023-06-26T04:23:33.832760Z",
          "iopub.status.idle": "2023-06-26T04:23:40.926369Z",
          "shell.execute_reply.started": "2023-06-26T04:23:33.832713Z",
          "shell.execute_reply": "2023-06-26T04:23:40.925390Z"
        },
        "trusted": true,
        "id": "CTn5dZ8ADp6F",
        "outputId": "71f572f0-4271-407e-c301-4ff0a9d856ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S3 creekhouseE\n",
            "S33 creek houseE\n",
            "\n",
            "~~~\n",
            "\n",
            "Sscales/kuhaylahE\n",
            "Sscales co pogoush aya payE\n",
            "\n",
            "~~~\n",
            "\n",
            "S1383 william lanierE\n",
            "S1385 william lanierE\n",
            "\n",
            "~~~\n",
            "\n",
            "S988 franklin laneE\n",
            "S987 franken lakeE\n",
            "\n",
            "~~~\n",
            "\n",
            "S6920 northeast 661st roadE\n",
            "S6920 northeast 66th roadE\n",
            "\n",
            "~~~\n",
            "\n",
            "Swww.freem.ne.jpE\n",
            "Swww.frem.itE\n",
            "\n",
            "~~~\n",
            "\n",
            "Shttps://jsi.is/hukuokaE\n",
            "Shttps://jsi.isi.is/hkurokaE\n",
            "\n",
            "~~~\n",
            "\n",
            "S239613 stolze streetE\n",
            "S2961 stol stold eastE\n",
            "\n",
            "~~~\n",
            "\n",
            "S271097 bayshore boulevardE\n",
            "S2710 97bay caia boulevardE\n",
            "\n",
            "~~~\n",
            "\n",
            "Sfederico pearsonE\n",
            "S9 ederico pearonE\n",
            "\n",
            "~~~\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth_processed = [ground_truth_list[i][1:-1] for i in range(len(ground_truth_list))]\n",
        "preds_list_processed = [preds_list[i][1:-1] for i in range(len(preds_list))]\n",
        "lev_dist = [lev.distance(ground_truth_processed[i], preds_list_processed[i])\n",
        "            for i in range(len(preds_list_processed))]\n",
        "N = [len(phrase) for phrase in ground_truth_processed]\n",
        "\n",
        "print('Validation score: '+str((np.sum(N) - np.sum(lev_dist))/np.sum(N)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T04:23:45.163446Z",
          "iopub.execute_input": "2023-06-26T04:23:45.163846Z",
          "iopub.status.idle": "2023-06-26T04:23:45.171850Z",
          "shell.execute_reply.started": "2023-06-26T04:23:45.163812Z",
          "shell.execute_reply": "2023-06-26T04:23:45.170820Z"
        },
        "trusted": true,
        "id": "naN88UcIDp6F",
        "outputId": "124f8314-38a0-46a5-e29a-823e00749133",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 0.7084048027444254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TFLiteModel"
      ],
      "metadata": {
        "id": "ucgKzMf4Dp6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " class TFLiteModel(tf.Module):\n",
        "    def __init__(self, model):\n",
        "        super(TFLiteModel, self).__init__()\n",
        "        self.target_start_token_idx = start_token_idx\n",
        "        self.target_end_token_idx = end_token_idx\n",
        "        # Load the feature generation and main models\n",
        "        self.model = model\n",
        "\n",
        "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, len(SEL_COLS)], dtype=tf.float32, name='inputs')])\n",
        "    def __call__(self, inputs, training=False):\n",
        "        # Preprocess Data\n",
        "        x = tf.cast(inputs, tf.float32)\n",
        "        x = x[None]\n",
        "        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, len(SEL_COLS))), lambda: tf.identity(x))\n",
        "        x = x[0]\n",
        "        x = pre_process(x)\n",
        "        x = x[None]\n",
        "        x = self.model.generate(x, self.target_start_token_idx)\n",
        "        x = x[0]\n",
        "        idx = tf.argmax(tf.cast(tf.equal(x, self.target_end_token_idx), tf.int32))\n",
        "        idx = tf.where(tf.math.less(idx, 1), tf.constant(2, dtype=tf.int64), idx)\n",
        "        x = x[1:idx]\n",
        "        x = tf.one_hot(x, 59)\n",
        "        return {'outputs': x}\n",
        "\n",
        "tflitemodel_base = TFLiteModel(model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T04:23:47.474886Z",
          "iopub.execute_input": "2023-06-26T04:23:47.476016Z",
          "iopub.status.idle": "2023-06-26T04:23:47.488141Z",
          "shell.execute_reply.started": "2023-06-26T04:23:47.475971Z",
          "shell.execute_reply": "2023-06-26T04:23:47.487131Z"
        },
        "trusted": true,
        "id": "tEcGTaJ2Dp6G"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(\"model.h5\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T04:23:50.183480Z",
          "iopub.execute_input": "2023-06-26T04:23:50.184185Z",
          "iopub.status.idle": "2023-06-26T04:23:50.376400Z",
          "shell.execute_reply.started": "2023-06-26T04:23:50.184149Z",
          "shell.execute_reply": "2023-06-26T04:23:50.375374Z"
        },
        "trusted": true,
        "id": "yC-8CLlHDp6G"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\n",
        "keras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]#, tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "tflite_model = keras_model_converter.convert()\n",
        "with open('/kaggle/working/model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "infargs = {\"selected_columns\" : SEL_COLS}\n",
        "\n",
        "with open('inference_args.json', \"w\") as json_file:\n",
        "    json.dump(infargs, json_file)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T04:23:53.080752Z",
          "iopub.execute_input": "2023-06-26T04:23:53.081962Z",
          "iopub.status.idle": "2023-06-26T04:25:04.842986Z",
          "shell.execute_reply.started": "2023-06-26T04:23:53.081917Z",
          "shell.execute_reply": "2023-06-26T04:25:04.841993Z"
        },
        "trusted": true,
        "id": "0PxmgsFQDp6G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "78a2f303-4104-4a8d-fbd1-6160fa89820e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"cond_2/Pad:0\", shape=(None, 26, 3), dtype=float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, conv1d_layer_call_fn, conv1d_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, re_lu_layer_call_fn while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-139d7c4a996e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkeras_model_converter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupported_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpsSet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLITE_BUILTINS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#, tf.lite.OpsSet.SELECT_TF_OPS]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras_model_converter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working/model.tflite'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtflite_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/model.tflite'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip submission.zip  './model.tflite' './inference_args.json'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T04:25:07.995108Z",
          "iopub.execute_input": "2023-06-26T04:25:07.995950Z",
          "iopub.status.idle": "2023-06-26T04:25:10.415570Z",
          "shell.execute_reply.started": "2023-06-26T04:25:07.995914Z",
          "shell.execute_reply": "2023-06-26T04:25:10.414236Z"
        },
        "trusted": true,
        "id": "S4U791C_Dp6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(\"model.tflite\")\n",
        "\n",
        "REQUIRED_SIGNATURE = \"serving_default\"\n",
        "REQUIRED_OUTPUT = \"outputs\"\n",
        "\n",
        "with open (\"/content/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
        "    character_map = json.load(f)\n",
        "rev_character_map = {j:i for i,j in character_map.items()}\n",
        "\n",
        "found_signatures = list(interpreter.get_signature_list().keys())\n",
        "\n",
        "if REQUIRED_SIGNATURE not in found_signatures:\n",
        "    raise KernelEvalException('Required input signature not found.')\n",
        "\n",
        "prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
        "output = prediction_fn(inputs=batch[0][0])\n",
        "prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\n",
        "print(prediction_str)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T04:25:14.037030Z",
          "iopub.execute_input": "2023-06-26T04:25:14.037455Z",
          "iopub.status.idle": "2023-06-26T04:25:14.458201Z",
          "shell.execute_reply.started": "2023-06-26T04:25:14.037420Z",
          "shell.execute_reply": "2023-06-26T04:25:14.457173Z"
        },
        "trusted": true,
        "id": "YpI1MlRqDp6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0KxaL1qFDp6O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}