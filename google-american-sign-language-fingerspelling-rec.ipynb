{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"HowItWorks\n1.Connect\nIngestdatafromavarietyofdatasetsincludingHadoopHDFS,AmazonS3,andmore.\n\n2.Transform\nAutomaticallyvisualizeandaddressdataqualityissueswithadvancedfeatureengineeringthattransformsyourdataintoanoptimalmodelingdataset.\n\n3.Build\nQuicklycreateandtesthighlyaccurateandrobustmodelswithstate-of-the-artautomatedmachinelearningthatspanstheentiredatasciencelifecycleandcanprocessavarietyofdatatypeswithinasingledataset.\n\n4.Validate\nAssessmodelrobustnessandmitigaterisksinproductionbyobtainingaholisticviewofthemodelsandpreventingfailuresonnewdata.\n\n5.Explain\nEasilyunderstandthe‘why’behindmodelpredictionstobuildbettermodelsandprovideexplanationsofmodeloutputatagloballevel(acrossasetofpredictions)oratalocallevel(foranindividualprediction).","metadata":{}},{"cell_type":"markdown","source":"##MODEL<br>\n教師あり　　\nラベル：各英単語　a~z,特殊文字,数字　<br>\n対象ファイル:/kaggle/input/asl-fingerspelling/character_to_prediction_index.json<br>\nインスタンス:","metadata":{"_uuid":"6451424b-51c2-4b1c-84ed-fcf05e53a5e6","_cell_guid":"07c08a76-fd59-4faf-9e9d-c14fbb6110c1","_kg_hide-input":false,"_kg_hide-output":true,"trusted":true}},{"cell_type":"code","source":"import pandas as pd\n\ndf=pd.read_parquet('/kaggle/input/asl-fingerspelling/train_landmarks/1019715464.parquet')\n\n##表示オプションを変更してすべての行と列を表示する\n#pd.set_option('display.max_columns',None)\n#pd.set_option('display.max_rows',None)\n##表示オプションを元に戻す\n#pd.reset_option('display.max_columns')\n#pd.reset_option('display.max_rows')\n\n#DataFrameを表示する\nprint(df.query('sequence_id==1975433633'))","metadata":{"_uuid":"1a694b26-b045-41f2-ac00-9a10036dc9fd","_cell_guid":"48e8eda8-73dc-4682-a0e3-5bee33ed6435","execution":{"iopub.status.busy":"2023-06-08T00:57:52.342383Z","iopub.execute_input":"2023-06-08T00:57:52.342758Z","iopub.status.idle":"2023-06-08T00:58:05.880662Z","shell.execute_reply.started":"2023-06-08T00:57:52.342726Z","shell.execute_reply":"2023-06-08T00:58:05.879206Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/asl-fingerspelling/train_landmarks/1019715464.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# # 表示オプションを変更してすべての行と列を表示する\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#pd.set_option('display.max_columns', None)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#pd.set_option('display.max_rows', None)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# DataFrameを表示する\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence_id == 1975433633\u001b[39m\u001b[38;5;124m'\u001b[39m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parquet.py:503\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03mLoad a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;124;03mDataFrame\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    501\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parquet.py:251\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m path_or_handle, handles, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    245\u001b[0m     path,\n\u001b[1;32m    246\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    247\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    248\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    249\u001b[0m )\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    255\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m_as_manager(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyarrow/parquet/__init__.py:2827\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, memory_map, read_dictionary, filesystem, filters, buffer_size, partitioning, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[1;32m   2816\u001b[0m         \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[1;32m   2817\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m ParquetFile(\n\u001b[1;32m   2818\u001b[0m             source, metadata\u001b[38;5;241m=\u001b[39mmetadata, read_dictionary\u001b[38;5;241m=\u001b[39mread_dictionary,\n\u001b[1;32m   2819\u001b[0m             memory_map\u001b[38;5;241m=\u001b[39mmemory_map, buffer_size\u001b[38;5;241m=\u001b[39mbuffer_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2824\u001b[0m             thrift_container_size_limit\u001b[38;5;241m=\u001b[39mthrift_container_size_limit,\n\u001b[1;32m   2825\u001b[0m         )\n\u001b[0;32m-> 2827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2828\u001b[0m \u001b[43m                        \u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2830\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2831\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_dataset=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to get the legacy behaviour is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2832\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated as of pyarrow 8.0.0, and the legacy implementation will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2833\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2834\u001b[0m     \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   2836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_prefixes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyarrow/parquet/__init__.py:2473\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.read\u001b[0;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[1;32m   2465\u001b[0m         index_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   2466\u001b[0m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[1;32m   2467\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m   2468\u001b[0m         ]\n\u001b[1;32m   2469\u001b[0m         columns \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2470\u001b[0m             \u001b[38;5;28mlist\u001b[39m(columns) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[1;32m   2471\u001b[0m         )\n\u001b[0;32m-> 2473\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_expression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2475\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\n\u001b[1;32m   2476\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2478\u001b[0m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"## train_landmarks/1019715464.parquet\n|sequence_id|frame|x_face_0|x_face_1|x_face_2|x_face_3|x_face_4|\n|:-|:-|:-|:-|:-|:-|:-|\n|1975433633|0|0.578892|0.578482|0.582906|0.572686|0.579030|\n|1975433633|1|0.577563|0.578528|0.582916|0.572760|0.579090|\n|1975433633|2|0.576181|0.576949|0.581346|0.572293|0.577725|\n|1975433633|…|0.576181|0.576949|0.581346|0.572293|0.577725|\n|1975433633|126|0.576181|0.576949|0.581346|0.572293|0.577725|\n\n## train.csv\n|path\t\t\t\t\t\t|\t\tfile_id\t|\tsequence_id\t|\tparticipant_id|\tphrase\t|\n|:-|:-|:-|:-|:-|\n|train_landmarks/1019715464.parquet|1019715464\t|\t1975433633\t|\t21\t\t|\t\t+95-335-395|\n|train_landmarks/1019715464.parquet|1019715464\t|\t1975473601\t|\t93\t\t\t|\tbugs.launchpad.net|\n|train_landmarks/1019715464.parquet|\t1019715464\t|\t1975502450\t|\t93\t\t|\t\tmirandasphysiosteps.com|\n|train_landmarks/1019715464.parquet|\t1019715464\t|\t1975521182\t|\t73\t\t|\t\t5endeavorcircle|\n|train_landmarks/1019715464.parquet|\t1019715464\t|\t1975541698\t|\t161\t\t|\t\t+44-4560-846|","metadata":{"_uuid":"55f9b628-ec66-4efc-84fe-75164d3172f7","_cell_guid":"5c56fc2d-6b6f-4123-850f-ee24cfbe8c84","_kg_hide-input":true,"trusted":true}},{"cell_type":"markdown","source":"##CompetitionsAgenda\nThiscompetionscreatepredictMachinelearningwhatASL(AmericanSignLanguage)toCharacter.<br>\nand,AlredyASMdatawasconvertedLandMarkDATA.<br>\n\n\nMyideaisuseingsupervisedlearningmodel.<br>\nandknowedaswesupervisedlearningnesseryneedlabel.<br>\nThiscompetitionisprepared　filethatcharacter_to_prediction_index.json<br>\nInstacneDataFilenameis.parquet<br>\n\n","metadata":{"_uuid":"4c5f4794-2bc3-4f1d-bd3e-fb05533f5724","_cell_guid":"7c62d944-6109-4c88-8ebc-924c6d9bb3a3","trusted":true}},{"cell_type":"markdown","source":"##やりたいこと\nこのコンペティションの目標は、アメリカンサインランゲージ（ASL）の指文字を検出し、テキストに変換することです。<br>\nしたがって、ASLの指文字を検出および翻訳するためのモデルを構築する必要があります。<br>\n<br>\n与えられたデータには、以下のようなファイルが含まれています。<br>\n<br>\n[train/supplemental_metadata].csv:メタデータが含まれているCSVファイルです。<br>\nこのファイルには、ランドマークファイルへのパス、データファイルの一意の識別子（file_id）、データ貢献者の一意の識別子（participant_id）、<br>\nランドマークシーケンスの一意の識別子（sequence_id）、およびランドマークシーケンスのラベル（phrase）が含まれています。<br>\n<br>\ncharacter_to_prediction_index.json:文字と予測インデックスの対応が記載されているJSONファイルです。<br>\n<br>\n[train/supplemental]_landmarks/:ランドマークデータが含まれているディレクトリです。<br>\nここには、MediaPipeのホリスティックモデルを使用して抽出されたランドマークが含まれています。<br>\n各データファイルには複数のシーケンスが含まれる場合があります。各シーケンスはフレームごとにランドマークの座標が含まれるため、テキストに変換するための情報を利用できます。<br><br>\n\nこれらの情報を考慮して、ASLの指文字を検出および翻訳するためのモデルを構築する方法について説明します。<br><br>\n\n##1\nランドマークデータを準備する:[train/supplemental]_landmarks/ディレクトリ内のファイルから、各フレームのランドマーク座標データを読み込みます。\nこのデータは、MediaPipeホリスティックモデルによって抽出されたものです。各ランドマークにはx、y、z座標が含まれており、手の形状や位置を表しています。\n##2\nラベルデータを準備する:[train/supplemental_metadata].csvファイルから、各シーケンスのラベルデータ（指文字のテキスト）を読み込みます。\nこれは、ランドマークデータに対応する正解ラベルとなります。\n##3\nモデルのトレーニング:TensorFlowや他のフレームワークを使用して、ASLの指文字を検出および翻訳するためのモデルをトレーニングします。\n入力としては、各フレームのランドマーク座標データを使用し、出力としては各フレームの指文字のテキストを予測するモデルを構築します。\nトレーニングには、適切なネットワークアーキテクチャ（たとえば、畳み込みニューラルネットワークやリカレントニューラルネットワーク）や損失関数、最適化アルゴリズムなどを選択します。\nまた、ランドマークデータと対応する正解ラベルを使用してモデルをトレーニングします。\n##4\nモデルの変換:トレーニングされたモデルをTensorFlowLiteモデルに変換します。\nこれにより、モデルを軽量化し、モバイルデバイスやエッジデバイスで効率的に実行できるようになります。\n変換には、TensorFlowLiteの変換ツールやAPIを使用します。\n##5\n提出準備:変換されたTensorFlowLiteモデルを提出するために準備します。\n提出には、コンペティションの要件に従って、モデルのチェックポイントをtflite形式に変換する必要があります。\n詳細な手順は評価ページで提供されるはずです。\n##6\nモデルの評価とチューニング:提出したモデルは評価され、性能が評価されるでしょう。\nコンペティションの評価基準に従って、モデルの精度や効率性が評価されます。\nもし必要ならば、モデルを改善するためにトレーニングやハイパーパラメータの調整を行うことができます。<br><br>\n\n以上の手順を経て、ASLの指文字を検出および翻訳するためのモデルを構築することができます。","metadata":{"_uuid":"1755e9b2-6018-4f97-bebc-dc8488cab007","_cell_guid":"b21f14ec-ebdb-4f25-b9ff-054f901440ea","_kg_hide-output":false,"_kg_hide-input":false,"trusted":true}},{"cell_type":"code","source":"importpandasaspd\n\n#1\ndf_landmark=pd.read_parquet('/kaggle/input/asl-fingerspelling/train_landmarks/1019715464.parquet')\ndf_landmark","metadata":{"_uuid":"be9ed6ed-44e2-4070-a42d-a2c6619bc506","_cell_guid":"2c8ab29d-cd12-4c5e-b76e-0e44fd0cd96d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-07T06:52:58.023477Z","iopub.execute_input":"2023-06-07T06:52:58.023824Z","iopub.status.idle":"2023-06-07T06:52:58.365205Z","shell.execute_reply.started":"2023-06-07T06:52:58.023798Z","shell.execute_reply":"2023-06-07T06:52:58.364076Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_landmark\u001b[49m\n","\u001b[0;31mNameError\u001b[0m: name 'df_landmark' is not defined"],"ename":"NameError","evalue":"name 'df_landmark' is not defined","output_type":"error"}]},{"cell_type":"code","source":"importpandasaspd\n\n#1\n#df_landmark=pd.read_parquet('/kaggle/input/asl-fingerspelling/train_landmarks/1019715464.parquet')\n\n#2\ndf_metadata=pd.read_csv('/kaggle/input/asl-fingerspelling/supplemental_metadata.csv')\ndf_metadata","metadata":{"_uuid":"daa7efa7-fe7a-4951-bd7a-f465422e07c0","_cell_guid":"70dda5c7-f2f8-4365-ac64-959a53f34f09","collapsed":false,"execution":{"iopub.status.busy":"2023-06-07T02:21:56.033722Z","iopub.execute_input":"2023-06-07T02:21:56.034220Z","iopub.status.idle":"2023-06-07T02:21:56.135839Z","shell.execute_reply.started":"2023-06-07T02:21:56.034184Z","shell.execute_reply":"2023-06-07T02:21:56.134666Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importpandasaspd\n\n#1\n#df_landmark=pd.read_parquet('/kaggle/input/asl-fingerspelling/train_landmarks/1019715464.parquet')\n\n#2\ndf_train=pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')\ndf_train","metadata":{"_uuid":"717e25ac-7435-4647-ab5e-4d30848d1a0c","_cell_guid":"06e7f23a-55bb-436e-b201-38c9a8c84995","collapsed":false,"execution":{"iopub.status.busy":"2023-06-07T03:18:31.891827Z","iopub.execute_input":"2023-06-07T03:18:31.892897Z","iopub.status.idle":"2023-06-07T03:18:41.738331Z","shell.execute_reply.started":"2023-06-07T03:18:31.892854Z","shell.execute_reply":"2023-06-07T03:18:41.737209Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##MODEL","metadata":{"_uuid":"60ba64f4-4b2c-4144-9562-18ea9ecb4fa5","_cell_guid":"a46fc3a9-43d1-4f21-bc67-c4d2c778f665","trusted":true}},{"cell_type":"code","source":"##keras\n#sequence_idとphraseが対応し、ラベルがphrase\n#126frame\n#x,Y,Z\n\n\nimporttensorflowastf\nfromtensorflow.keras.modelsimportSequential\nfromtensorflow.keras.layersimportDense\n\ndefcreate_model(input_size,hidden_size,num_classes):\n#モデルの定義\nmodel=Sequential()\nmodel.add(Dense(hidden_size,activation='relu',input_shape=(input_size,)))\n\n#隠れ層の追加\nmodel.add(Dense(hidden_size,activation='relu'))\nmodel.add(Dense(hidden_size,activation='relu'))\nmodel.add(Dense(hidden_size,activation='relu'))\n\n#出力層の追加\nmodel.add(Dense(num_classes,activation='softmax'))\n\nreturnmodel","metadata":{"_uuid":"10627127-e2ce-4691-b4bd-8b799c5ee6fb","_cell_guid":"25da28c6-144b-4356-8ea9-ecf70b187202","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#LightBGM\nimportlightgbmaslgb\nfromsklearn.datasetsimportload_breast_cancer\nfromsklearn.model_selectionimporttrain_test_split\nfromsklearn.metricsimportaccuracy_score\n\n#データの読み込み\ndata=load_breast_cancer()\nX_train,X_test,y_train,y_test=train_test_split(data.data,data.target,test_size=0.2,random_state=42)\n\n#LightGBMのモデルを定義\nmodel=lgb.LGBMClassifier()\n\n#モデルのトレーニング\nmodel.fit(X_train,y_train)\n\n#テストデータで予測を実行\ny_pred=model.predict(X_test)\n\n#精度の評価\naccuracy=accuracy_score(y_test,y_pred)\nprint(\"Accuracy:\",accuracy)\n\n","metadata":{"_uuid":"c433fbfa-fe1d-4d9b-821d-7a1d975f0e08","_cell_guid":"81427a9d-78e3-4295-b26e-efb2a57d0b8b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##表形式データ\nLightGBM\nXGBoost\n##ベンチマーク\nAutoML\nDriverlessAI\n##ModelDevelopmenttool\nKeras","metadata":{}},{"cell_type":"markdown","source":"##KerasモデルをTensorFlowLiteモデルに変換","metadata":{"_uuid":"ebdd341b-ae8d-4197-8efd-07c08e7abf5a","_cell_guid":"ee4e35dc-1028-480d-b087-15ead5ff62e5","trusted":true}},{"cell_type":"code","source":"##KerasモデルをTensorFlowLiteモデルに変換\n\n#TensorFlowLiteモデルへの変換\nconverter=tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model=converter.convert()\n\n#TensorFlowLiteモデルの保存\nwithopen('model.tflite','wb')asf:\nf.write(tflite_model)","metadata":{"_uuid":"f4455e3b-01bc-4083-87bf-ceab0647348a","_cell_guid":"e78fd37e-f601-4db1-a8fe-a269885d82f5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##TensorFlowLiteモデルを使用して推論\n#TensorFlowLiteモデルのロード\ninterpreter=tf.lite.Interpreter(model_path='model.tflite')\ninterpreter.allocate_tensors()\n\n#入力データの準備\ninput_data=...\n\n#入力テンソルの取得とセットアップ\ninput_details=interpreter.get_input_details()\ninterpreter.set_tensor(input_details[0]['index'],input_data)\n\n#推論の実行\ninterpreter.invoke()\n\n#出力テンソルの取得と解釈\noutput_details=interpreter.get_output_details()\noutput_data=interpreter.get_tensor(output_details[0]['index'])\n\n#推論結果の利用\n...","metadata":{"_uuid":"d660ff75-8784-4da0-85f3-1f8fd3ebaff1","_cell_guid":"ff3d973b-a7a4-40d7-94f5-353e33b44925","trusted":true},"execution_count":null,"outputs":[]}]}